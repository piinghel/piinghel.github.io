<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-03-03T14:45:52+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Pieter-Jan</title><subtitle>Systematic trading and data science things.
</subtitle><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><entry><title type="html">Unleashing The Beast: LightGBM</title><link href="http://localhost:4000/quants/2025/02/20/beast.html" rel="alternate" type="text/html" title="Unleashing The Beast: LightGBM" /><published>2025-02-20T00:00:00+01:00</published><updated>2025-02-20T00:00:00+01:00</updated><id>http://localhost:4000/quants/2025/02/20/beast</id><content type="html" xml:base="http://localhost:4000/quants/2025/02/20/beast.html"><![CDATA[<h1 id="roadmap">Roadmap</h1>
<ul>
  <li>The idea to show performance of Lighgbm first using the same approach as the last article and then also compare it to the low vol and linear model and compare performance and show the impressive added value you get from including new linearities</li>
</ul>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[Roadmap The idea to show performance of Lighgbm first using the same approach as the last article and then also compare it to the low vol and linear model and compare performance and show the impressive added value you get from including new linearities]]></summary></entry><entry><title type="html">Ridge regression</title><link href="http://localhost:4000/quants/2025/02/09/ridge.html" rel="alternate" type="text/html" title="Ridge regression" /><published>2025-02-09T00:00:00+01:00</published><updated>2025-02-09T00:00:00+01:00</updated><id>http://localhost:4000/quants/2025/02/09/ridge</id><content type="html" xml:base="http://localhost:4000/quants/2025/02/09/ridge.html"><![CDATA[<p><a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">In the last article</a>, by scaling returns based on volatility, we improved performance with minimal complexity. But what if we could do better?</p>

<p>This time, we’ll predict stock rankings using a linear regression model that combines multiple features—like price changes, trading volume, and risk levels—and learns the optimal ranking based on a target label. A target label is simply the outcome we want the model to predict, such as stock returns or Sharpe ratio. By learning to predict this label, the model will rank the stocks accordingly.</p>

<p>Predicting stock rankings isn’t as simple as it seems. We’re dealing with features on different scales—price changes, trading volume, risk—and if we don’t normalize them, bigger features like market cap will overshadow smaller ones, like short-term returns. Additionally, the signal-to-noise ratio is quite low, meaning there’s a lot of random fluctuation in the data, which makes it harder for the model to find meaningful patterns. My intuition tells me normalizing these features will help the model learn a better ranking. I’m not totally sure yet, but that’s exactly what I’m testing today. If we get the normalization right, the model might perform better—but we’ll see.</p>

<p>The dataset stays the same as the one used in <a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">the previous article</a>: daily price, volume, and market capitalization data for all Russell 1000 (RIY) constituents, covering about 3,300 stocks historically. We use point-in-time constituents of the RIY and apply basic liquidity filters, excluding stocks priced below $5 to ensure realistic implementation.</p>

<h2 id="feature-engineering">Feature Engineering</h2>

<p>Coming from a statistical and computer science background, it makes sense to let the data figure out how the relationships should be learned, instead of imposing too many assumptions. This contrasts with the typical approach in multifactor portfolios, where the modeler decides how features should be combined. The advantage of using a regression model is that it learns the weights for how features should be combined. We’re sticking with linear regression for now, assuming the relationships are linear and no interaction terms. It’s a simple, data-driven approach that focuses on finding those direct, linear connections between features and the target.</p>

<p>Obviously, we need to define our predictive features and choose a target variable. We focus on price, volume, market capitalization, and market-derived features. Below is a breakdown of the main feature groups, computed daily for all 3,300 stocks in our universe.</p>

<ol>
  <li><strong>Momentum Features</strong>
    <ul>
      <li>Capture trend-following behavior.</li>
      <li>Examples:
        <ul>
          <li>Lagged returns over 1 to 10 days.</li>
          <li>Rolling cumulative returns over 21 to 252 days.</li>
          <li>Moving Average Convergence Divergence (MACD) to detect shifts in momentum.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Volatility Features</strong>
    <ul>
      <li>Measure risk.</li>
      <li>Examples:
        <ul>
          <li>Rolling historical volatility over 21, 63, or 126 days.</li>
          <li>Separate downside and upside volatility.</li>
          <li>Average True Range (ATR) to normalize price fluctuations.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Liquidity Features</strong>
    <ul>
      <li>Assess trading activity.</li>
      <li>Examples:
        <ul>
          <li>Rolling mean and standard deviation of trading volume.</li>
          <li>Ratio of current volume to its rolling maximum to highlight unusual trading activity.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Size Features</strong>
    <ul>
      <li>Measure company scale.</li>
      <li>Examples:
        <ul>
          <li>Rolling mean and minimum of market cap.</li>
          <li>Distinguishes small-cap from large-cap stocks.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Short Mean Reversion Features</strong>
    <ul>
      <li>Identify when prices revert to historical norms.</li>
      <li>Examples:
        <ul>
          <li>Price deviation from its rolling moving average.</li>
          <li>Position relative to rolling minimum and maximum values.</li>
          <li>Bollinger Bands to spot overbought or oversold conditions.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Correlation with the Market</strong>
    <ul>
      <li>Capture systematic risk.</li>
      <li>Examples:
        <ul>
          <li>Rolling correlation with the Russell 1000 over 63-day windows.</li>
          <li>Helps separate defensive stocks from high-beta names.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p>That leaves us with around 150 predictive features, some of which are obviously correlated.</p>

<h3 id="target-variable">Target Variable</h3>
<p>Now, what exactly are we trying to predict? We’ll focus on two targets:</p>

<ul>
  <li>Return over the next 20 days</li>
  <li>Sharpe ratio over the next 20 days</li>
</ul>

<p>Could we explore other horizons? Sure. But to keep things simple, we’ll stick to 20 days for now.</p>

<h2 id="preprocessing-cross-sectional-normalization">Preprocessing: Cross-Sectional Normalization</h2>

<p>Cross-sectional normalization adjusts each feature relative to all other stocks on the same day, ensuring the model focuses on relative differences rather than absolute values. This transformation also improves interpretation—when features are normalized consistently, it’s easier to recognize high or low values across time.</p>

<p>By applying this normalization, we ensure that stocks are evaluated in a comparable way at each point in time, which helps stabilize model inputs and prevents certain features from disproportionately influencing rankings.</p>

<h2 id="mathematical-formulation">Mathematical Formulation</h2>

<p>For a given feature $X^p$, the normalized value for stock $i$ at time $t$ is:</p>

\[X_{i,t}^{p,\text{norm}} = f\left(X_{i,t}^{p}, X_{1:N,t}^{p}\right)\]

<p>where:</p>

<ul>
  <li>$X^p_{i,t}$ is the raw feature value for stock $i$ at time $t$.</li>
  <li>$X^p_{1:N,t}$ is the set of values for all stocks at time $t$ for feature $p$.</li>
  <li>$f(\cdot)$ is the chosen normalization method.</li>
</ul>

<p>Different methods can be used to transform the raw values, each with its own strengths and tradeoffs.</p>

<h2 id="z-scoring">Z-Scoring</h2>

<p>One common approach is z-scoring, which standardizes features by centering them around zero and scaling them to have a standard deviation of one:</p>

\[X_{i,t}^{p,\text{norm}} = \frac{X_{i,t}^{p} - \hat{\mu}^p_t}{\hat{\sigma}^p_t}\]

<p>where:</p>

<ul>
  <li>
    <p>$\hat{\mu}^p_t$ is the estimated mean across all stocks at time $t$ for feature $p$:</p>

\[\hat{\mu}^p_t = \frac{1}{N} \sum_{i=1}^{N} X_{i,t}^{p}\]
  </li>
  <li>
    <p>$\hat{\sigma}^p_t$ is the estimated standard deviation:</p>

\[\hat{\sigma}^p_t = \sqrt{\frac{1}{N} \sum_{i=1}^{N} \left( X_{i,t}^{p} - \hat{\mu}^p_t \right)^2}\]
  </li>
</ul>

<p>Z-scoring retains the relative magnitudes of the original values, allowing the model to distinguish between small and large variations. However, it is sensitive to extreme outliers, so values beyond ±5 standard deviations are clipped.</p>

<h2 id="ranking-normalization">Ranking Normalization</h2>

<p>Another approach is ranking normalization, sometimes referred to as uniform transformation, which transforms feature values into ranks and scales them between 0 and 1:</p>

\[R_{i,t}^{p} = \frac{r_{i,t}^{p}}{N}\]

<p>where:</p>

<ul>
  <li>$r^p_{i,t}$ is the rank of stock $i$ at time $t$ based on feature $p$ (0 for the lowest value, $N$ for the highest).</li>
  <li>$R^p_{i,t}$ is the normalized rank.</li>
</ul>

<p>Unlike z-scoring, ranking ensures that the distribution remains stable over time. This makes it robust to extreme values but removes magnitude information—only relative positioning is preserved.</p>

<h2 id="visualizing-the-effect-of-normalization">Visualizing the Effect of Normalization</h2>

<p>Below, in Figure 1, a summary of the different normalization methods are applied to a single feature (20-day return). From left to right: the original distribution, z-scored, and ranked.</p>

<p><img src="/assets/ridge/example_normalization.png" alt="Figure 1" /></p>

<p><strong>Figure 1</strong>: Effect of normalization on 20-day return distribution. Left: Original data, Middle: Z-scored, Right: Ranked between 0 and 1.</p>

<h2 id="choosing-the-right-normalization-method">Choosing the Right Normalization Method</h2>

<p>The choice between z-scoring and ranking depends on the specific objectives of the model.</p>

<ul>
  <li><strong>Z-scoring</strong> preserves magnitude differences but is subject to distribution shifts over time.</li>
  <li><strong>Ranking</strong> is stable and eliminates outliers but may lose information about the strength of differences between stocks.</li>
</ul>

<p>Both methods allow the model to process stock features in a comparable way on any given day, but they emphasize different aspects of the data.</p>

<h2 id="evaluating-the-impact-of-normalization">Evaluating the Impact of Normalization</h2>

<p>To test whether normalization improves rankings, three approaches are compared:</p>

<ol>
  <li>No normalization (raw features)</li>
  <li>Z-scoring across all stocks</li>
  <li>Ranking across all stocks</li>
</ol>

<p>If normalization leads to better performance, the next step is to refine how it is applied—particularly to the target label.</p>

<h2 id="should-the-target-label-be-normalized-by-sector">Should the Target Label Be Normalized by Sector?</h2>

<p>While normalizing input features ensures consistency across time, the same consideration applies to the target label (e.g., Return 20 or Sharpe Ratio 20).</p>

<p>The question is whether it should be normalized across all stocks or within each sector:</p>

<ul>
  <li><strong>Global normalization</strong> applies normalization to the target label across the full stock universe.</li>
  <li><strong>Sector-specific normalization</strong> applies normalization within each sector while keeping all other features globally normalized.</li>
</ul>

<p>Sector-specific normalization prevents stocks in different industries from being compared directly in terms of their returns, which can be useful if sector dynamics influence performance. The goal is to determine whether this adjustment improves ranking performance.</p>

<h3 id="handling-missing-data">Handling Missing Data</h3>

<h2 id="handling-missing-data-1">Handling Missing Data</h2>

<p>Stock data isn’t always perfect—missing values happen. While the model can still run, leaving gaps in the data isn’t ideal. The approach here is straightforward:</p>

<ul>
  <li>Forward fill: If a stock has prior data, we use the last known value.</li>
  <li>Cross-sectional mean imputation: If no past data exists, we replace the missing value with the sector average for that day.</li>
  <li>Default values: When neither of the above applies:
    <ul>
      <li>For z-scoring, missing values are set to 0.</li>
      <li>For ranking, missing values are set to 0.5 (midpoint of the ranking scale).</li>
    </ul>
  </li>
</ul>

<p>There are more advanced ways to handle missing data, from statistical methods to ML-based imputations. But for now, this simple, reliable approach gets the job done. A deeper dive into missing data strategies is something for another time.</p>

<h2 id="modeling-the-cross-sectional-normalized-score">Modeling the Cross-Sectional Normalized Score</h2>

<p>At the core of this strategy, I’m building a model to predict a stock’s cross-sectional normalized score, which could be its Sharpe ratio, return, or another performance measure. I think of this as a function mapping available information at time $t$ to an expected score at $t+1$. To ensure comparability across stocks, the score is normalized in the cross-section before modeling.</p>

<p>Mathematically, I assume that there exists a function $g(\cdot)$ such that:</p>

\[s_{i,t+1} = g(\mathbf{z}_{i,t}) + \epsilon_{i,t+1}\]

<p>where:</p>
<ul>
  <li>$s_{i,t+1}$ is the true cross-sectional normalized score for stock $i$ at time $t+1$.</li>
  <li>$z_{i,t}$ is a vector of predictor variables for stock $i$ at time $t$.</li>
  <li>$\epsilon_{i,t+1}$ is the error term, representing what the model cannot predict.</li>
</ul>

<p>The objective is to approximate $g(\cdot)$ using historical data. This function follows two key principles:</p>

<ul>
  <li>it leverage the entire panel of stocks, meaning the same functional form applies universally.</li>
  <li>It depends only on stock-specific features at time $t$. While some features contain past information (such as return over the past 20 days), these are explicitly engineered rather than dynamically learned. In addition, the model does not learn interactions between different stocks.</li>
</ul>

<h3 id="ridge-regression-as-a-baseline">Ridge Regression as a Baseline</h3>

<p>To estimate $g(\cdot)$, I use Ridge Regression, a simple yet effective baseline, particularly when predictors are highly correlated. It solves the following optimization problem:</p>

\[\underset{\boldsymbol{\beta}}{\min} \frac{1}{n} \sum_{i=1}^n (s_{i,t+1} - \mathbf{x}_i^\top \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \beta_j^2\]

<p>where the second term, $\lambda \sum_{j=1}^p \beta_j^2$, regularizes the coefficients to prevent instability.</p>

<p>Ridge is a reasonable choice here because:</p>
<ul>
  <li>Stocks with similar characteristics often exhibit collinearity, and Ridge helps stabilize coefficient estimates.</li>
  <li>The regularization term shrinks extreme values, reducing sensitivity to noise.</li>
  <li>It provides a simple reference point before exploring more complex models.</li>
</ul>

<p>The model is estimated using historical data, and to assess its effectiveness, I apply an expanding walkforward validation which I explain just below in a bit more detail.</p>

<h2 id="expanding-walkforward-validation">Expanding Walkforward Validation</h2>

<p>To see how well the model holds up over time, I use an expanding walkforward validation. The idea is simple:</p>

<ol>
  <li>Start with a 3-year burn-in period – The model isn’t tested yet; it just learns from the data.</li>
  <li>Update the model every 2 years – Each time, I add the latest data and refit the model.</li>
  <li>Keep expanding the dataset – Older data stays in, and new data gets added.</li>
</ol>

<p>With stock data, I’ve always found that the more historical data, the better. The signal-to-noise ratio is low, so keeping as much information as possible helps the model the find the signal in all the noise.</p>

<p>A rolling validation window could be an alternative, but it discards older data that might still be valuable. In my experience, an expanding window works better because it allows the model to pick up long-term relationships, leading to more stable predictions.</p>

<p>For hyperparameter tuning, one option is to split the training set into separate train and validation sets. But honestly, I’ve never found this to be worth the extra time. Optimizing hyperparameters can take a while, and in most cases, default values that make sense are already a very good starting point.</p>

<p>Below is a schematic of the expanding walkforward approach:</p>

<p><img src="/assets/ridge/walk-forward.png" alt="Figure 2" /></p>

<p><strong>Figure 2</strong>: Expanding walkforward validation process.</p>

<h2 id="portfolio-construction">Portfolio Construction</h2>

<p>Once I have stock rankings, I build a long-short portfolio:</p>

<ul>
  <li>I go long on the 75 stocks with the highest scores.</li>
  <li>I short the 75 stocks with the lowest scores.</li>
</ul>

<p>The approach is robust across different portfolio sizes, whether using 50, 100, or 150 stocks.</p>

<p>To keep risk under control, I use volatility targeting:</p>
<ul>
  <li>Higher-volatility stocks get smaller weights.</li>
  <li>Lower-volatility stocks get larger weights.</li>
</ul>

<p>This ensures that the portfolio maintains a stable risk profile instead of being dominated by a few volatile names.</p>

<p>For a deeper dive into my portfolio construction process, check out my <a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">previous article</a> where I go into more detail.</p>

<h2 id="results">Results</h2>
<p>To evaluate different modeling choices, I tested 10 model variations, combining:</p>
<ul>
  <li>5 normalization methods: Raw, Z-score (global &amp; sector), Ranking (global &amp; sector).</li>
  <li>2 target labels: Sharpe Ratio (SR 20) and Return (Return 20).</li>
  <li>A combined strategy (“Combo”), which equally weights all strategies.</li>
</ul>

<p>To ensure a fair comparison visually, all strategies are scaled to 10% volatility. The goal is to understand how normalization, sector adjustments, and target labels affect performance. Figure 3 visualizes cumulative returns across all strategies—without labels, adding a bit of suspense.</p>

<p>While all models deliver positive returns, there is a clear differences in performance.</p>

<p><img src="/assets/ridge/all_lines.png" alt="Figure 3" /></p>

<p><strong>Figure 3</strong>: Cumulative returns of all strategies, scaled to 10% volatility.</p>

<p>Figure 4 provides a summary of how normalization method, sector adjustment, and target label choice affected Sharpe Ratios across all models. It highlights the performance differences between Z-scoring, ranking, and raw features, as well as the impact of normalizing within sectors versus globally. The results also show how using Sharpe Ratios as a target label compares to using raw returns.</p>

<p>One of the strongest patterns in the results is the impact of normalization. Models using Z-scoring performed best, followed by ranking, while raw features consistently underperform. This was surprising—I initially expected ranking to work better due to its stabilizing effect. However, it seems that ranking may also remove valuable signals, which could explain why Z-scoring outperformed. As expected, raw features introduced too much noise, making them the weakest option.</p>

<p>Beyond normalization type, sector adjustments had an even greater effect. Normalizing the target label within sectors significantly improved Sharpe Ratios, reinforcing the idea that comparing stock performance within their peer group leads to better rankings. Each sector has its own dynamics, and normalizing within them helps separate true signals from sector-wide noise. This adjustment ensures that comparisons remain meaningful, making rankings more robust.</p>

<p>Target label choice also played a key role. Models trained on Sharpe Ratios outperformed those trained on raw returns, highlighting the advantage of using a risk-adjusted measure. This makes sense—predicting volatility is generally easier than predicting raw returns, which may explain why Sharpe Ratio models performed better across different setups.</p>

<p><img src="/assets/ridge/summary_barplot.png" alt="Figure 4" /></p>

<p><strong>Figure 4</strong>: Sharpe Ratio performance across key modeling choices.</p>

<h3 id="normalization-effects-depend-on-the-target-label">Normalization Effects Depend on the Target Label</h3>

<p>Looking at Figure 5, the effect of normalization depends on the target label.</p>

<p>For Return 20 models, normalization had a smaller effect, but ranking and Z-scoring still outperformed raw features. Interestingly, Z-scoring has regained popularity in recent years.</p>

<p>For Sharpe Ratio models, the impact of normalization was stronger. Z-scoring was clearly the best performer, followed by ranking, then raw features.</p>

<p><img src="/assets/ridge/normalization_target.png" alt="Figure 5" /></p>

<p><strong>Figure 5</strong>: Cumulative return of different normalization methods, conditioned on the target label. Volatility is set at 10% for all strategies.</p>

<h3 id="key-takeaways">Key Takeaways</h3>
<ul>
  <li>Normalization improves signal stability, helping models generalize better.</li>
  <li>Sector-based adjustments on the target label refine comparisons, preventing large sector-specific biases.</li>
  <li>Target label choice affects robustness, with Sharpe Ratio-based models performing better.</li>
</ul>

<h2 id="the-combo-strategy-holds-up-well">The “Combo” Strategy Holds Up Well</h2>

<p>One surprising result: the “Combo” strategy performed exceptionally well (see table 1). Instead of picking a single best model, it equally weights all strategies—yet still ranked second overall in Sharpe Ratio.</p>

<p>Even without fine-tuning, blending multiple signals smoothed performance and improved stability. This suggests that diversification across models can outperform an individual “best” model.</p>

<h2 id="full-performance-breakdown">Full Performance Breakdown</h2>

<p>To quantify these findings, here’s the performance breakdown across models:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Return (ann. %)</th>
      <th>Volatility (ann. %)</th>
      <th>Sharpe Ratio (ann.)</th>
      <th>Max. Drawdown (%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SR Z-Score By Sector</td>
      <td>13.36</td>
      <td>7.87</td>
      <td>1.70</td>
      <td>11.58</td>
    </tr>
    <tr>
      <td>Combo</td>
      <td>9.98</td>
      <td>6.71</td>
      <td>1.49</td>
      <td>10.90</td>
    </tr>
    <tr>
      <td>SR Ranking By Sector</td>
      <td>10.68</td>
      <td>7.90</td>
      <td>1.35</td>
      <td>11.82</td>
    </tr>
    <tr>
      <td>Return Z-Score By Sector</td>
      <td>9.67</td>
      <td>7.38</td>
      <td>1.31</td>
      <td>15.37</td>
    </tr>
    <tr>
      <td>SR Z-Score Global</td>
      <td>10.68</td>
      <td>8.30</td>
      <td>1.29</td>
      <td>18.39</td>
    </tr>
    <tr>
      <td>SR Ranking Global</td>
      <td>10.40</td>
      <td>8.41</td>
      <td>1.24</td>
      <td>15.93</td>
    </tr>
    <tr>
      <td>Return Ranking By Sector</td>
      <td>9.31</td>
      <td>7.64</td>
      <td>1.22</td>
      <td>10.63</td>
    </tr>
    <tr>
      <td>SR Raw Global</td>
      <td>9.98</td>
      <td>8.39</td>
      <td>1.19</td>
      <td>15.49</td>
    </tr>
    <tr>
      <td>Return Raw Global</td>
      <td>8.42</td>
      <td>7.28</td>
      <td>1.16</td>
      <td>10.96</td>
    </tr>
    <tr>
      <td>Return Ranking Global</td>
      <td>8.54</td>
      <td>7.63</td>
      <td>1.12</td>
      <td>13.62</td>
    </tr>
    <tr>
      <td>Return Z-Score Global</td>
      <td>7.95</td>
      <td>7.45</td>
      <td>1.07</td>
      <td>21.23</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1</strong>: Performance metrics across different modeling choices, ranked by Sharpe Ratio in descending order. Results exclude transaction costs and slippage.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>Sector normalization (target label), normalization method, and target label choice all had a meaningful impact on performance:</p>
<ul>
  <li>Sector normalization was a game-changer—comparing stocks within their sector led to major improvements.</li>
  <li>Normalization method mattered more than expected—Z-scoring outperformed ranking, contradicting my initial intuition.</li>
  <li>Sharpe Ratio models consistently outperformed return-based models, reinforcing the importance of risk-adjusted metrics.</li>
</ul>

<p>Instead of searching for a single best model, it may be smarter to combine perspectives. The “Combo” strategy showed that diversification across models stabilizes results—even without fine-tuning.</p>

<h2 id="todo">TODO:</h2>
<ul>
  <li>refer to combo in tabel</li>
  <li>interpration a bit better</li>
  <li>also say that things are scaled to all the same vol</li>
  <li>make it a bit more personalized</li>
  <li>go a bit more in depth about zscoring and ranking</li>
  <li>not happy about sector should the target label be done by sector or gloablly. The transition it weird</li>
  <li>Discuss the effects of ranking and zscoring a bit more</li>
  <li>Add some numbers in the text</li>
  <li>Go a little bit deeper on why things works: Sharpe ratio because of the volatility, sector comparisoin. A lot of it is just a tradeoff between removing noise, adding simple making it simpler for the model</li>
  <li>Write in present tense</li>
</ul>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[In the last article, by scaling returns based on volatility, we improved performance with minimal complexity. But what if we could do better?]]></summary></entry><entry><title type="html">The Low Volatility Factor: A Boring Way to Make Money?</title><link href="http://localhost:4000/quant/2024/12/15/low-volatility-factor.html" rel="alternate" type="text/html" title="The Low Volatility Factor: A Boring Way to Make Money?" /><published>2024-12-15T00:00:00+01:00</published><updated>2024-12-15T00:00:00+01:00</updated><id>http://localhost:4000/quant/2024/12/15/low-volatility-factor</id><content type="html" xml:base="http://localhost:4000/quant/2024/12/15/low-volatility-factor.html"><![CDATA[<h2 id="the-low-volatility-factor-a-steady-approach">The Low-Volatility Factor: A Steady Approach</h2>

<p>The low-volatility factor is a well-known concept in quantitative investing. It’s based on a simple observation: stocks that fluctuate less tend to have better risk-adjusted returns than those with more extreme price swings. This pheneoman also exist in other asset classes. We will be focusing on stocks.</p>

<p>This post is part of a series on ranking stocks. I’ll start with a single-factor approach and gradually build up—first by combining multiple factors using linear regression, then testing more advanced design choices, and finally exploring interactions and non-linearities with LightGBM. At the end, I’ll compare all three approaches to see whether complexity actually adds value. For now, let’s focus on constructing a long-short portfolio using the low-volatility factor in the Russell 1000.</p>

<h2 id="tradeable-universe">Tradeable Universe</h2>

<p>The dataset covers the Russell 1000 (RIY), which tracks the largest U.S. stocks. To keep it realistic, I filter out stocks priced under $5. The sample runs from 1995 to 2024, covering around 3,300 stocks as companies enter and exit the index. At any given time, about 1,000 stocks are tradeable. Since it uses point-in-time constituents, there’s no survivorship bias. Figures 1 visualizes the number of tradeabel stocks over time.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/nr_stocks.svg" alt="Figure 1" /><br />
Figure 1: Number of tradeable stocks over time.</p>

<h2 id="measuring-volatility">Measuring Volatility</h2>

<p>The low-volatility factor identifies stocks with more stable price movements. To measure this, I compute the standard deviation of daily returns over three short-term rolling windows:</p>

<ul>
  <li>5-day</li>
  <li>10-day</li>
  <li>21-day</li>
</ul>

<p>Shorter windows help capture shifts in volatility faster, while longer ones smooth things out. A mix of these gives a balance between stability and adaptability.</p>

<p>Across the dataset, the average volatility is 33%, with most stocks falling between 18% and 39%. The median is 26%. Some stocks exhibit extreme price swings, so I winsorized the data at 5% and 200% to prevent outliers from distorting the results.</p>

<p>The distribution is skewed to the right, meaning most stocks cluster around moderate volatility, but a few experience significantly higher fluctuations.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/distribution_volatilities.svg" alt="Figure 2" /></p>

<p>Figure 2: Annualized volatility across all stocks in the dataset.</p>

<h2 id="does-low-volatility-matter">Does Low Volatility Matter?</h2>

<p>I was testing if low-volatility stocks actually perform differently, so I checked a couple of things:</p>

<ol>
  <li>How they did in the next 10 days</li>
  <li>Their Sharpe ratio also for the next 10 days</li>
</ol>

<p>For returns, the Pearson correlation was only 0.03, so higher-volatility stocks showed a tiny edge in raw returns. But when I switched to Spearman, it pretty much flattened out to zero (0.00). I prefer Spearman here because it works with ranks, not raw values, which helps avoid the noise from outliers.</p>

<p>When I looked at Sharpe ratios, the pattern flipped. Pearson showed a small negative correlation (-0.035), and Spearman bumped it up to -0.04. This suggests that more volatile stocks tend to have lower Sharpe ratios.</p>

<p>The numbers are small, but that’s to be expected—there’s always a lot of noise in finance. Even weak signals can matter if you apply them consistently over a large universe of stocks.</p>

<h2 id="sorting-stocks-into-portfolios">Sorting Stocks into Portfolios</h2>

<p>To turn this into a tradeable strategy, I rank stocks by volatility at each point in time and sort them into five portfolios. This ensures that portfolio assignments are always relative to the current market.</p>

<p>Here’s how it works:</p>
<ol>
  <li>Compute rolling volatility for each stock.</li>
  <li>Rank stocks by volatility within the universe.</li>
  <li>Normalize ranks to a 0-1 scale.</li>
  <li>Assign stocks to one of five portfolios based on percentile rank.</li>
</ol>

<p>Let $r_{i,t}$ be the cross-sectional rank of stock $i$ at time $t$, and $N$ be the number of stocks. The normalized rank is:</p>

\[\frac{r_{i,t}}{N}\]

<p>Stocks are then grouped into these buckets:</p>

<ul>
  <li>Portfolio 1: Lowest 10% of stocks ($0 \leq \text{Rank Score} &lt; 0.1$) → Low volatility</li>
  <li>Portfolio 2: 10% to 20% of stocks ($0.1 \leq \text{Rank Score} &lt; 0.2$)</li>
  <li>Portfolio 3: Middle 60% ($0.2 \leq \text{Rank Score} &lt; 0.8$)</li>
  <li>Portfolio 4: 80% to 90% of stocks ($0.8 \leq \text{Rank Score} &lt; 0.9$)</li>
  <li>Portfolio 5: Highest 10% ($0.9 \leq \text{Rank Score} \leq 1.0$) → High volatility</li>
</ul>

<p>This way, every stock’s classification is determined relative to the cross-sectional volatility of the market at that time.</p>

<h2 id="constructing-a-long-only-and-long-short-portfolio">Constructing a Long-Only and Long-Short Portfolio</h2>

<p>Once the stocks are bucketed, I create two types of portfolios: equal-weighted and volatility-targeted.</p>

<h3 id="equal-weighted-portfolio">Equal-Weighted Portfolio</h3>

<p>In the equal-weighted portfolio, all stocks are given equal weight, and I remain fully invested at all times. However, this creates a mismatch between the volatilities of the long and short positions, which I’ll explain further below. To construct the long-short portfolio, I simply take the difference between the low-volatility (P1) and high-volatility portfolios (P5).</p>

<h3 id="volatility-targeted-portfolio">Volatility-Targeted Portfolio</h3>

<p>Volatility targeting adjusts stock weights based on their volatility to create a more stable portfolio. Here’s how it works:</p>

<ol>
  <li>Compute the Volatility Scaling Factor: For each stock, calculate:<br />
\(\text{vol_ratio} = \frac{\sigma_{target}}{\hat{\sigma}_{i,t}}\)<br />
where:
    <ul>
      <li>$\sigma_{target} = 20\%$ is an arbitrary target, chosen because it’s close to the average stock volatility. This ensures that the overall portfolio volatility remains around 8%.</li>
      <li>${\hat{\sigma}_{i,t}}$ represents the stock’s estimated future volatility, typically calculated using a rolling 60-day standard deviation. More complex models could be used, but we’ll keep it simple for now.</li>
    </ul>
  </li>
  <li>
    <p>Adjust Equal Weights: Multiply the equal weight of each stock by its $\text{vol_ratio}$:<br />
\(w_i = \text{equal_weight} \times \text{vol_ratio}\)</p>
  </li>
  <li>
    <p>Cap Individual Weights: Ensure no stock weight exceeds 4% to prevent excessive concentration. This cap is somewhat arbitrary and depends on the number of stocks in the portfolio and possibly other factors, such as sector diversification or liquidity constraints.</p>
  </li>
  <li>Constrain Portfolio Exposure: Ensure that the total portfolio weight does not exceed $1$ (i.e., fully invested). During periods of high volatility, the total weight may decrease to limit risk. While this may resemble market timing, I see it as dynamically adjusting risk—a common technique in trend-following strategies.</li>
</ol>

<p>The resulting portfolio balances the long and short legs by dynamically adjusting stock weights to achieve the target volatility. The portfolios are rebalanced weekly to reflect changes in volatility over time. For simplicity, I haven’t factored in transaction costs in this analysis.</p>

<h2 id="performance-analysis">Performance Analysis</h2>

<h3 id="equal-weighted-portfolio-1">Equal-Weighted Portfolio</h3>

<p>Let’s look at the performance of the equal-weighted long-short portfolio. The main metrics here are return, volatility, and Sharpe ratio—annualized, of course.</p>

<p>What stands out is that the low-volatility portfolio delivers much higher returns than the high-volatility one. As expected, the low-volatility stocks show the lowest total volatility, while the high-volatility stocks show the highest. This results in the low-volatility portfolio performing better in terms of risk-adjusted returns.</p>

<p>But there’s a problem. Since the long portfolio (low-volatility stocks) is much less volatile than the short portfolio (high-volatility stocks), the long-short portfolio becomes unbalanced, which hurts its performance.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/barplot_metrics_ew.png" alt="Figure 3" /></p>

<p><strong>Figure 3</strong>: Geometric Return, Volatility, and Sharpe Ratio for equal-weighted portfolios (before volatility targeting).</p>

<h3 id="improving-performance-with-volatility-targeting">Improving Performance with Volatility Targeting</h3>

<p>Volatility targeting helps adjust for the imbalance between long and short positions by scaling portfolio weights based on volatility. This adjustment leads to more stable performance and a higher Sharpe ratio for both the long-only and long-short portfolios.</p>

<p>Figure 4 shows how this works—volatility is more aligned across the five portfolios, reducing the mismatch. As a result, the long-short portfolio improves, benefiting from more balanced risk.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/barplot_metrics_ew_vt.png" alt="Figure 4" /></p>

<p><strong>Figure 4</strong>: Geometric Return, Volatility, and Sharpe Ratio after volatility targeting.</p>

<h3 id="metrics-after-volatility-targeting">Metrics After Volatility Targeting</h3>

<p><img src="/assets/2024-12-15-low-volatility-factor/perf_backtest_ew_vt.png" alt="Figure 5" /></p>

<p><strong>Figure 5</strong>: Net Asset Value of the volatility-targeted portfolio.</p>

<h2 id="volatility-targeting-comparison">Volatility Targeting Comparison</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Long (No VT)</th>
      <th>Long (VT)</th>
      <th>Short (No VT)</th>
      <th>Short (VT)</th>
      <th>L/S (No VT)</th>
      <th>L/S (VT)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Geom. Return (%)</td>
      <td>11.8</td>
      <td>11.1</td>
      <td>4.7</td>
      <td>2.9</td>
      <td>-5.3</td>
      <td>7.7</td>
    </tr>
    <tr>
      <td>Volatility (%)</td>
      <td>12.2</td>
      <td>9.7</td>
      <td>38.6</td>
      <td>9.7</td>
      <td>32.6</td>
      <td>8.3</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>1.0</td>
      <td>1.1</td>
      <td>0.1</td>
      <td>0.3</td>
      <td>-0.2</td>
      <td>0.9</td>
    </tr>
    <tr>
      <td>Max Drawdown (%)</td>
      <td>40.2</td>
      <td>29.5</td>
      <td>89.6</td>
      <td>36.7</td>
      <td>91.6</td>
      <td>33.6</td>
    </tr>
    <tr>
      <td>Max TUW (Days)</td>
      <td>863</td>
      <td>612</td>
      <td>4802</td>
      <td>1647</td>
      <td>5539</td>
      <td>944</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1</strong>: Performance metrics for long, short, and long-short portfolios before and after volatility targeting (VT). “No VT” indicates no volatility targeting, while “VT” applies it.</p>

<p>Figure 6 shows the portfolio weights for both the long (low-volatility) and short (high-volatility) portfolios after volatility targeting.</p>

<p>As shown, the short portfolio (high-volatility stocks) now receives a much lower allocation, fluctuating between 0.2 and 0.6. Meanwhile, the low-volatility portfolio stays almost fully invested. This makes sense—high-volatility stocks naturally experience larger price swings, so less capital is allocated to them to keep the risk balanced.</p>

<p>During extreme market events like the dot-com crash (2000), the financial crisis (2009), and COVID (2020), both portfolios temporarily reduce exposure, reflecting overall higher volatility.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/portfolio_weights_long_short_vol_target.svg" alt="Figure 6" /></p>

<p><strong>Figure 6</strong>: Portfolio weights for Long and Short portfolios after volatility targeting.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>The low-volatility factor delivers strong risk-adjusted returns. Lower-volatility stocks tend to outperform, which contradicts the idea that higher risk always leads to higher returns.</li>
  <li>Equal weighting in long-short portfolios creates a risk imbalance. Since low-volatility stocks are naturally less risky, shorting high-volatility stocks at equal weight leads to uneven exposure and weaker performance.</li>
  <li>Volatility targeting balances risk and improves performance. Adjusting weights based on volatility evens out risk between long and short positions, leading to a more stable strategy.</li>
  <li>Dynamic risk adjustment smooths returns. Volatility-targeted portfolios adapt to market shifts, reducing drawdowns and improving long-term stability.</li>
</ol>

<p>A simple volatility-targeting adjustment makes a long-short portfolio more stable and effective. In the next post, I’ll explore how combining multiple factors can further enhance results.</p>

<h2 id="todo">TODO</h2>
<ul>
  <li>measuring volatitily is not so well described</li>
  <li>Tradeable universe; check this section</li>
  <li>use some numbers in the results</li>
  <li>Add columns before and after vol scaling in Performance tables</li>
</ul>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quant" /><summary type="html"><![CDATA[The Low-Volatility Factor: A Steady Approach]]></summary></entry></feed>