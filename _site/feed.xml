<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-05-12T13:54:27+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Pieter-Jan</title><subtitle>Systematic trading and data science things.
</subtitle><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><entry><title type="html">Rebalancing Luck</title><link href="http://localhost:4000/quants/2025/05/10/rebalancing-luck.html" rel="alternate" type="text/html" title="Rebalancing Luck" /><published>2025-05-10T00:00:00+02:00</published><updated>2025-05-10T00:00:00+02:00</updated><id>http://localhost:4000/quants/2025/05/10/rebalancing%20luck</id><content type="html" xml:base="http://localhost:4000/quants/2025/05/10/rebalancing-luck.html"><![CDATA[<h1 id="todo">Todo</h1>
<ul>
  <li>Can lower turnover and transaction costs but don’t fully understand how</li>
  <li>Also refer to the the original article of concretum research</li>
  <li>Make writing style slightly more personal. Write form the I and sometimes give your own opinion.</li>
</ul>

<h1 id="rebalancing-schedules-tranching-and-cleaner-portfolio-paths">Rebalancing Schedules, Tranching, and Cleaner Portfolio Paths</h1>

<p>In most backtests, the rebalancing schedule is hardcoded — for example, every three weeks on a Friday — and rarely questioned. But that choice can have a real impact on performance. Shifting the execution day by just a few days can change inputs, alter positions, and affect returns, volatility, and drawdowns.</p>

<p>What also matters is how much of the portfolio you rebalance at once. Rebalancing the full book in one shot can amplify path dependency. A more gradual approach — rebalancing just one-third each week — might lead to smoother results without changing the model at all.</p>

<p>In this post, I look at both: how different rebalancing schedules affect performance, and how tranching improves stability across the board.</p>

<h2 id="setup">Setup</h2>

<p>The strategy is a long-short equity model that rebalances every three weeks, always on a single weekday. Since there are three possible offset weeks and five weekdays, that gives 15 potential rebalancing schedules.</p>

<p>I simulate all 15 variants. The only difference is the day on which the portfolio is rebalanced:</p>

<ul>
  <li>3 offset weeks × 5 weekdays = 15 combinations</li>
  <li>Each run uses the same trained model, applied using the predictions available on that day</li>
  <li>The signals, logic, and asset universe remain unchanged</li>
</ul>

<p>In the base case, I rebalance the entire portfolio once every 3 weeks — for example, always on Friday of offset week 0.</p>

<p>Then I compare it to a tranching setup: the portfolio is split into three parts, and one-third is rebalanced each week. Over a three-week cycle, the full portfolio is still updated — just more gradually. This smoother execution path helps reduce sensitivity to any one day’s noise.</p>

<h2 id="rebalancing-day-does-matter">Rebalancing Day Does Matter</h2>

<p>Here’s the cumulative return of the 15 full-rebalance variants:</p>

<p><img src="/assets/tranching/all_perf_plots.png" alt="Figure 1" /></p>

<p><strong>Figure 1:</strong> Cumulative returns for each rebalancing schedule (15 variants).</p>

<p>And the table of results:</p>

<table>
  <thead>
    <tr>
      <th>Tranche</th>
      <th>Geometric Return</th>
      <th>Volatility</th>
      <th>Modified Sharpe</th>
      <th>Max Drawdown</th>
      <th>Time Underwater</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>offset=0, day=1</td>
      <td>11.67%</td>
      <td>6.56%</td>
      <td>1.78</td>
      <td>9.79%</td>
      <td>375</td>
    </tr>
    <tr>
      <td>offset=0, day=2</td>
      <td>11.72%</td>
      <td>6.61%</td>
      <td>1.77</td>
      <td>12.08%</td>
      <td>314</td>
    </tr>
    <tr>
      <td>offset=0, day=3</td>
      <td>11.25%</td>
      <td>6.56%</td>
      <td>1.71</td>
      <td>13.27%</td>
      <td>253</td>
    </tr>
    <tr>
      <td>offset=0, day=4</td>
      <td>12.73%</td>
      <td>6.63%</td>
      <td>1.92</td>
      <td>11.98%</td>
      <td>236</td>
    </tr>
    <tr>
      <td>offset=0, day=5</td>
      <td>12.79%</td>
      <td>6.76%</td>
      <td>1.89</td>
      <td>15.54%</td>
      <td>264</td>
    </tr>
    <tr>
      <td>offset=1, day=1</td>
      <td>12.52%</td>
      <td>6.71%</td>
      <td>1.86</td>
      <td>14.75%</td>
      <td>569</td>
    </tr>
    <tr>
      <td>offset=1, day=2</td>
      <td>12.83%</td>
      <td>6.74%</td>
      <td>1.90</td>
      <td>13.23%</td>
      <td>269</td>
    </tr>
    <tr>
      <td>offset=1, day=3</td>
      <td>12.18%</td>
      <td>6.65%</td>
      <td>1.83</td>
      <td>11.92%</td>
      <td>251</td>
    </tr>
    <tr>
      <td>offset=1, day=4</td>
      <td>11.16%</td>
      <td>6.57%</td>
      <td>1.70</td>
      <td>10.80%</td>
      <td>330</td>
    </tr>
    <tr>
      <td>offset=1, day=5</td>
      <td>12.95%</td>
      <td>6.53%</td>
      <td>1.98</td>
      <td>10.89%</td>
      <td>236</td>
    </tr>
    <tr>
      <td>offset=2, day=1</td>
      <td>12.29%</td>
      <td>6.60%</td>
      <td>1.88</td>
      <td>10.17%</td>
      <td>297</td>
    </tr>
    <tr>
      <td>offset=2, day=2</td>
      <td>11.43%</td>
      <td>6.47%</td>
      <td>1.77</td>
      <td>11.42%</td>
      <td>266</td>
    </tr>
    <tr>
      <td>offset=2, day=3</td>
      <td>11.34%</td>
      <td>6.61%</td>
      <td>1.72</td>
      <td>10.70%</td>
      <td>578</td>
    </tr>
    <tr>
      <td>offset=2, day=4</td>
      <td>11.31%</td>
      <td>6.54%</td>
      <td>1.73</td>
      <td>10.14%</td>
      <td>341</td>
    </tr>
    <tr>
      <td>offset=2, day=5</td>
      <td>11.28%</td>
      <td>6.57%</td>
      <td>1.72</td>
      <td>14.28%</td>
      <td>608</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Performance metrics for each full-rebalance variant.</p>

<p>I found that returns were fairly stable across rebalancing schedules, which honestly surprised me a bit. I expected more divergence. That said, there’s still a meaningful spread — modified Sharpe ratios range from 1.71 to 1.98, and some variants spent quite a bit longer in drawdown than others. It’s not huge, but it’s enough to notice. And none of it comes from the model itself — it’s just small shifts in execution timing that end up compounding over time.</p>

<h2 id="tranching-a-simple-effective-fix">Tranching: A Simple, Effective Fix</h2>

<p>Instead of picking one rebalancing day, we can average across them.</p>

<p>In the tranching setup, one-third of the portfolio is rebalanced each week, rotating through all three offsets. The full portfolio still turns over every 3 weeks, but in a staggered way.</p>

<p>Then, for interpretability, I group results by weekday, averaging across the three offsets.</p>

<p><img src="/assets/tranching/tranched_perf_plots.png" alt="Figure 2" /> 
<strong>Figure 2:</strong> Cumulative returns with tranching by weekday (averaged over offsets).</p>

<table>
  <thead>
    <tr>
      <th>Weekday</th>
      <th>Geometric Return</th>
      <th>Volatility</th>
      <th>Modified Sharpe</th>
      <th>Max Drawdown</th>
      <th>Time Underwater</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1 (Mon)</td>
      <td>12.18%</td>
      <td>6.06%</td>
      <td>2.01</td>
      <td>11.30%</td>
      <td>388</td>
    </tr>
    <tr>
      <td>2 (Tue)</td>
      <td>12.02%</td>
      <td>6.06%</td>
      <td>1.98</td>
      <td>12.23%</td>
      <td>300</td>
    </tr>
    <tr>
      <td>3 (Wed)</td>
      <td>11.62%</td>
      <td>6.07%</td>
      <td>1.91</td>
      <td>11.59%</td>
      <td>262</td>
    </tr>
    <tr>
      <td>4 (Thu)</td>
      <td>11.73%</td>
      <td>6.05%</td>
      <td>1.94</td>
      <td>10.82%</td>
      <td>265</td>
    </tr>
    <tr>
      <td>5 (Fri)</td>
      <td>12.34%</td>
      <td>6.06%</td>
      <td>2.04</td>
      <td>12.80%</td>
      <td>364</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 2:</strong> Tranche-averaged performance by weekday.</p>

<p>The improvements are not just cosmetic. Modified Sharpe ratios move up and stabilize around 1.91, 2.04. Volatility compresses, especially compared to the 6.5–6.9% range seen earlier. Drawdowns get shallower and spend less time underwater.</p>

<p>This doesn’t happen because we’re adding return. It happens because we’re executing more evenly, avoiding sharp jumps.</p>

<h2 id="why-it-works">Why It Works</h2>

<p>Tranching reduces path dependency. A single full-book rebalance can reflect one-off shocks, price dislocations, or liquidity gaps — all of which inject noise into a strategy that’s otherwise stable.</p>

<p>By spreading rebalancing over time, you soften those shocks. You’re still following the model, just in a way that filters out randomness from any specific day. The full portfolio is still turned over every three weeks, so turnover doesn’t increase — but transitions are smoother and less vulnerable to execution timing.</p>

<p>There’s also an effect on the signal itself. In the standard setup, you only express the model’s predictions once every three weeks. With tranching, you use the most recent predictions each week — even though you’re only rebalancing part of the capital. That means the portfolio reflects more prediction points, more frequently, and benefits from a kind of temporal diversification across model outputs. That added reactivity may also contribute to the improvement in Sharpe ratios.</p>

<h2 id="a-few-trade-offs-and-extra-benefits">A Few Trade-Offs and Extra Benefits</h2>

<p>One thing to watch out for is execution complexity. Rebalancing every week means running the pipeline and sending orders more often. It’s not just the code — it’s the extra monitoring, mental overhead, and operational stress that comes with doing something three times instead of once. And if some positions are small, the trades might not even be worth it. Whether that matters depends on the signal structure and execution setup — but it’s worth keeping in mind.</p>

<p>That said, I’ve found tranching to be surprisingly helpful in research. By smoothing out the noise from timing, it gives a cleaner view of the strategy’s true performance. That makes it easier to evaluate whether a new feature or model change actually adds value — instead of just getting lucky on a rebalance date.</p>

<p>There’s also a small benefit I didn’t expect at first. While we still rebalance each slice only once every three weeks, we use the most recent model predictions each week. So we’re injecting new signal into the portfolio every week — just on a fraction of the AUM. That adds a bit of temporal diversification and keeps the portfolio more responsive without needing to go all-in.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>This started as a quick curiosity around rebalancing schedules, but it turned into a clear case for using tranching by default.</p>

<p>It makes the strategy more stable and less path-dependent. The signal stays the same. The logic stays the same. But the outcome gets cleaner.</p>

<p>For research purposes, that clarity really helps. When adding a new feature or modifying a signal, it becomes easier to see if we actually improved things.</p>

<p>I’ll be using this setup going forward for any model that rebalances in fixed cycles. It’s simple, and it works.</p>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[Todo Can lower turnover and transaction costs but don’t fully understand how Also refer to the the original article of concretum research Make writing style slightly more personal. Write form the I and sometimes give your own opinion.]]></summary></entry><entry><title type="html">Dope Articles</title><link href="http://localhost:4000/quants/2025/04/20/dope-articles.html" rel="alternate" type="text/html" title="Dope Articles" /><published>2025-04-20T00:00:00+02:00</published><updated>2025-04-20T00:00:00+02:00</updated><id>http://localhost:4000/quants/2025/04/20/dope%20articles</id><content type="html" xml:base="http://localhost:4000/quants/2025/04/20/dope-articles.html"><![CDATA[<h2 id="roadmap">Roadmap</h2>
<ul>
  <li>just give an overview of all relevant articles that are relevant.</li>
  <li>Might add a quick summary what I liked about these</li>
  <li>Also update from time to time</li>
  <li>
    <p>Focus on Cross sectional asset pricing</p>
  </li>
  <li>Learning to rank</li>
  <li>couple of articles of robeco</li>
  <li>Article of Pictet</li>
</ul>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[Roadmap just give an overview of all relevant articles that are relevant. Might add a quick summary what I liked about these Also update from time to time Focus on Cross sectional asset pricing]]></summary></entry><entry><title type="html">LightGBM</title><link href="http://localhost:4000/quants/2025/02/20/lgbm.html" rel="alternate" type="text/html" title="LightGBM" /><published>2025-02-20T00:00:00+01:00</published><updated>2025-02-20T00:00:00+01:00</updated><id>http://localhost:4000/quants/2025/02/20/lgbm</id><content type="html" xml:base="http://localhost:4000/quants/2025/02/20/lgbm.html"><![CDATA[<h1 id="roadmap">Roadmap</h1>
<ul>
  <li>
    <p>The idea to show performance of LightGBM first using the same approach as the last article and then also compare it to the low vol and linear model and compare performance and show the impressive added value you get from including non-linearities</p>
  </li>
  <li>introduce LightGBM very shortly: check</li>
  <li>explain again a bit the process but a very brief recap. Refer to previous articles: check</li>
  <li>The idea is to show the added value of LightGBM compared to ridge compared to the low vol factor: check</li>
  <li>I’m thinking of also introducing the singal performance and metrics: TODO
      - add also low vol factor here
      - add summary statisitcs here as well; mean, min max, std, sr</li>
  <li>Compare performance returns: low vol, linear, lightgbm: check</li>
  <li>Should be a relatively short article</li>
  <li>introduce transaction costs: ok will say</li>
  <li>show turnover: nope</li>
  <li>Quickly tell about the diminsihing performance over the last 10 years</li>
  <li>Mentioned the neural networks approach to model weigths</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>In my last two articles, I explored different ways to rank stocks and build long-short portfolios.</p>

<p>In the first article, I used a simple one-factor model based on volatility — ranking stocks by their recent volatility and betting on the fact that low-volatility stocks tend to outperform high-volatility ones.</p>

<p>In the second article, I moved to a more flexible approach by combining multiple features — like momentum, volatility, size, and liquidity — using Ridge regression, a linear model that learns how to weigh these features. I also examined some important design choices, like whether to use ranking or z-scoring for feature normalization, and whether to normalize the target label by sector.</p>

<p>Now I want to take this a step further and see what happens when I switch to a non-linear model, LightGBM. The idea is to keep everything else the same — same dataset, same features, same target label, same allocation model — and only change the model. This way, I can isolate the impact of using a non-linear approach that can capture more complex patterns in the data.</p>

<p>In the end, I’ll compare LightGBM with both Ridge and the low-volatility factor, to clearly see how much is gained (or not) when moving from a simple factor to a linear model and then to a non-linear one.</p>

<h2 id="recap-of-the-process">Recap of the process</h2>

<p>Before diving into LightGBM, let me briefly recap the setup I’ve been using, which stays unchanged for this analysis.</p>

<p>I’m working with the Russell 1000 universe, using point-in-time data and filtering out stocks priced below $5 to avoid illiquid names. The feature set is broad, covering about 150 features related to momentum, volatility, liquidity, size, mean reversion, and correlation with the market.</p>

<p>For the target label, I focus on the Sharpe ratio over the next 20 days. I’ve chosen this because it provides a more stable and risk-adjusted measure of performance compared to raw returns, which are often extremely noisy.</p>

<p>All features are normalized using cross-sectional ranking — turning each feature into a percentile rank between 0 and 1 on each day. Ranking has become my default because it makes features comparable across stocks and over time, and it avoids the impact of extreme outliers. I know z-scoring could sometimes be a better choice, and that’s something I want to revisit in the future, but for now I’m sticking with ranking.</p>

<p>Once I have model scores, I use a volatility-targeted allocation model to build long-short portfolios. Specifically, I go long the top 75 and short the bottom 75 stocks, adjusting each position based on volatility to avoid concentrating risk in a few names. This makes the portfolio more stable over time.</p>

<p>Finally, portfolios are rebalanced every three weeks. I find this strikes a good balance between adapting to new information and keeping turnover under control.</p>

<h2 id="why-move-to-lightgbm">Why move to LightGBM</h2>

<p>So far, the Ridge model combined multiple features in a linear way — it learned fixed weights for each feature, and that was it. But markets are rarely linear, and I suspect there are more complex patterns that Ridge simply can’t capture. For example, a feature like volatility might only matter when combined with a momentum signal, or extreme values of a feature might behave differently than moderate ones.</p>

<p>To address this, I’m moving to gradient boosting, a method that builds strong predictive models by combining many small decision trees. Gradient boosting works iteratively — each new tree tries to fix the mistakes of the previous ones — making it well suited to capture non-linear effects and feature interactions.</p>

<p>There are several well-known implementations of gradient boosting, like XGBoost, CatBoost, and LightGBM. I’m using LightGBM because it’s fast and efficient, especially when working with large datasets like mine. In practice, I don’t expect a big difference between these libraries, but LightGBM tends to be much quicker to train and predict, so it’s the obvious choice for this kind of project.</p>

<p>Another reason to choose gradient boosting is that these models are often among the best-performing approaches for tabular datasets — including many winning solutions in Kaggle competitions. So it makes sense to see whether this kind of model can push performance beyond what Ridge and the low-volatility factor can achieve.</p>

<p>With that in mind, I applied LightGBM to the same setup as before: ranking stocks in the Russell 1000, going long the top 75, short the bottom 75, and using volatility-based position sizing. This keeps everything consistent, so we can isolate the impact of the model itself.</p>

<h2>Performance</h2>

<p>Let’s take a look at how LightGBM performs when applied to the same setup as before — and the results are quite strong.</p>

<div style="display: block; text-align: left;">
  <img src="/assets/lightgbm/perf_lgbm.png" width="600" style="display: block; margin: 0;" />
  <p><strong>Figure 1:</strong> Performance of the LightGBM strategy, before and after transaction costs.</p>
</div>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Short</th>
      <th>Long</th>
      <th>L/S (No Fees)</th>
      <th>L/S (Fees)</th>
      <th>Russell 1000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Return (Ann. %)</td>
      <td>1.46</td>
      <td>12.48</td>
      <td>12.33</td>
      <td>10.69</td>
      <td>7.29</td>
    </tr>
    <tr>
      <td>Volatility (Ann. %)</td>
      <td>9.95</td>
      <td>10.54</td>
      <td>6.67</td>
      <td>6.69</td>
      <td>19.58</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.15</td>
      <td>1.18</td>
      <td>1.85</td>
      <td>1.60</td>
      <td>0.37</td>
    </tr>
    <tr>
      <td>Maximum Drawdown (%)</td>
      <td>30.78</td>
      <td>31.10</td>
      <td>13.79</td>
      <td>13.97</td>
      <td>56.88</td>
    </tr>
    <tr>
      <td>Max Time Under Water (days)</td>
      <td>1619</td>
      <td>599</td>
      <td>234</td>
      <td>265</td>
      <td>1666</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Performance statistics of the LightGBM strategy, with and without transaction costs (5 basis points per trade).</p>

<p>After accounting for costs, the strategy posts a 10.7% annualized return with volatility just under 7%, leading to a Sharpe ratio of 1.60. Drawdowns stay around 14%, and the strategy tends to bounce back relatively quickly after periods of underperformance.</p>

<p>What’s nice to see is that both sides of the book are doing their job. The long leg delivers a solid 12.5% return — well above the Russell 1000’s 7.3%. The short leg returns just 1.5%, meaning those bottom-ranked stocks are underperforming the market, as intended. That’s exactly the kind of clean separation you want in a ranking strategy.</p>

<p>That said, it’s worth noting the Russell 1000 isn’t a proper benchmark here — it’s long-only and far more volatile — but it’s still helpful as a rough point of reference since we’re trading within that universe.</p>

<p>Because both legs are volatility-targeted, the difference in returns translates cleanly into long-short performance, without relying on one side to carry all the weight. The result is a fairly smooth and balanced profile</p>

<h2 id="model-performance">Model Performance</h2>

<h3 id="1-statistical-model-performance">1. Statistical Model Performance</h3>

<p>Before looking at portfolio returns, it’s useful to evaluate how well each model ranks stocks based on expected future performance — independently of any portfolio construction.</p>

<p>Each day, the models generate a prediction for every stock $i$ in the universe. For Ridge and LightGBM, these predictions are trained to estimate the Sharpe ratio over the next 20 trading days, which we compute and rank within each sector to account for industry effects.</p>

<p>This gives us, on each day $t$, a full cross-section of predicted scores and a corresponding cross-section of true future Sharpe ratios — both across all stocks.</p>

<p>To evaluate signal quality, we compute the Spearman rank correlation between the predicted ranking and the true ranking:</p>

\[\rho_t = \text{Spearman} \left( \{ \hat{y}_{i,t} \}, \{ y_{i,t} \} \right)\]

<p>where:</p>

<ul>
  <li>$\hat{y}_{i,t}$ is the model-predicted score for stock $i$ at time $t$</li>
  <li>$y_{i,t}$ is the realized Sharpe ratio over the next 20 days for stock $i$, cross-sectionally rank-normalized by sector</li>
</ul>

<p>This correlation is computed across all stocks on each day $t$, giving us a time series ${ \rho_t }$ of daily values.</p>

<p>From this series, we compute:</p>

<ul>
  <li>Mean correlation — average predictive strength</li>
  <li>Standard deviation — signal stability</li>
  <li>Sharpe ratio — consistency over time (mean divided by standard deviation)</li>
</ul>

<p><img src="/assets/lightgbm/signal.png" width="600" /></p>
<p><strong>Figure 2:</strong> Cumulative Spearman correlation between model signals and future Sharpe ratio rankings, computed daily.</p>
<p><br /></p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Low Volatility</th>
      <th>Ridge Regression</th>
      <th>LightGBM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Mean (%)</td>
      <td>4.96</td>
      <td>5.14</td>
      <td>5.43</td>
    </tr>
    <tr>
      <td>Standard Deviation (%)</td>
      <td>14.39</td>
      <td>8.70</td>
      <td>7.03</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.34</td>
      <td>0.59</td>
      <td>0.77</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 2:</strong> Daily signal quality metrics. Models are evaluated based on how well their predicted rankings align with future 20-day Sharpe ratio rankings (normalized by sector), measured using Spearman correlation across all stocks each day.</p>

<p>Figure 2 plots the cumulative Spearman correlation between each model’s daily predictions and realized outcomes — giving us a running view of how consistently each signal aligns with future Sharpe ratio rankings. A steadily rising line indicates stable predictive power over time, while flatter or more erratic curves suggest a noisier, less reliable signal.</p>

<p>The summary statistics in Table 2 make this more concrete. While the average correlation values are all in the 5% range, LightGBM stands out by delivering its signal more consistently — with the lowest day-to-day variability and the highest signal-level Sharpe ratio.</p>

<p>This might sound modest — after all, Spearman correlation ranges from -1 to 1 (or -100% to 100%), and we’re hovering around just 5%. But this is exactly what’s meant by “low signal-to-noise” in finance: even a small but stable signal can lead to meaningful results. And that’s what we’re seeing here — LightGBM offers a modest edge in correlation that translates into significantly better downstream performance.
The signal-level results are encouraging — LightGBM produces a stronger and more stable ranking than both Ridge regression and the low-volatility factor. But how does this translate into actual financial performance?</p>

<h3 id="2-financial-performance">2. Financial Performance</h3>

<p><img src="/assets/lightgbm/perf_return_comp.png" width="600" /></p>
<p><strong>Figure 3:</strong> Performance over the full sample (1997–2024).</p>

<p><img src="/assets/lightgbm/perf_return_comp_last_10yr.png" width="600" /></p>
<p><strong>Figure 4:</strong> Performance over the last decade (2015–2024).</p>
<p><br /></p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Low Vol (Full)</th>
      <th>LR (Full)</th>
      <th>LGBM (Full)</th>
      <th>Low Vol (10Y)</th>
      <th>LR (10Y)</th>
      <th>LGBM (10Y)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Return (Ann. %)</td>
      <td>5.62</td>
      <td>8.76</td>
      <td>11.68</td>
      <td>7.94</td>
      <td>8.91</td>
      <td>9.33</td>
    </tr>
    <tr>
      <td>Volatility (Ann. %)</td>
      <td>8.67</td>
      <td>7.98</td>
      <td>7.09</td>
      <td>9.76</td>
      <td>8.91</td>
      <td>8.17</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.67</td>
      <td>1.09</td>
      <td>1.59</td>
      <td>0.83</td>
      <td>1.00</td>
      <td>1.13</td>
    </tr>
    <tr>
      <td>Max. Drawdown (%)</td>
      <td>35.27</td>
      <td>20.18</td>
      <td>13.71</td>
      <td>12.11</td>
      <td>17.07</td>
      <td>11.69</td>
    </tr>
    <tr>
      <td>Max. Time Under Water (Days)</td>
      <td>844</td>
      <td>862</td>
      <td>297</td>
      <td>350</td>
      <td>307</td>
      <td>297</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 3:</strong> Strategy performance over the full sample (1997–2024) and the last decade (2015–2024).</p>

<p>Figures 3 and 4, along with Table 3, show a clear pattern: LightGBM outperforms both Ridge and Low Vol across nearly every metric. It delivers higher returns, lower volatility, and stronger Sharpe ratios — with significantly shallower drawdowns and faster recovery times. Over the full sample, the Sharpe ratio reaches 1.59 — a meaningful improvement over Ridge (1.09) and Low Vol (0.67).</p>

<p>That said, it’s worth pointing out that performance in the last decade is lower across the board. LightGBM still leads, but the margin has narrowed. Returns are flatter, volatility is slightly higher, and Sharpe ratios have declined compared to the earlier years.</p>

<p>Why is that? Is the market becoming more efficient? Are features becoming less predictive? Is it overfitting to a regime that no longer holds? These are important questions — ones I won’t tackle here, but that I plan to explore in a follow-up post.</p>

<p>For now, the takeaway is simple: LightGBM shows consistent improvements over simpler models, but like all strategies, its performance evolves — and understanding <em>why</em> is just as important as measuring <em>how much</em>.</p>

<h2 id="whats-next">What’s Next</h2>

<p>There’s clearly room to improve. As we’ve seen, performance over the past decade has declined. It’s tempting to jump to conclusions about why — changing market dynamics, feature decay, increased competition — but the truth is, it’s hard to know. What we do know is that results have weakened, and that’s reason enough to dig deeper.</p>

<p>Here are a few directions I’m thinking about:</p>

<ul>
  <li><strong>Portfolio management</strong> 
Reading the book advanced portfolio managements and considering buying elements of portfolio management<br />
How much of the return is driven by known risk factors or sector tilts? Are we unknowingly taking on systematic exposures that explain most of the performance? Decomposing the P&amp;L and analyzing factor loadings would help shed light here. There are some great books and papers that go deep into this, and I plan to study those more closely.
    <ul>
      <li>understanding my pnl</li>
      <li>managing my factor risk</li>
      <li>build better allocation methods</li>
    </ul>
  </li>
  <li>
    <p><strong>Expanding the feature set</strong><br />
Right now, the model only sees price, market cap, and volume — a relatively narrow view. There’s likely signal in other types of data, especially when the current ones start to fade. Also, we arbitrarily chose to predict the Sharpe ratio over the next 20 days. But why not 10? Or 30? Or 60? Maybe even combine models trained on different horizons — this could help smooth out noise and pick up different return profiles.</p>
  </li>
  <li>
    <p><strong>Rethinking the modeling objective</strong><br />
One thing that’s been bothering me is the two-step setup: first, we model a score or ranking; then we convert that into positions using a separate allocation rule. But what if we skipped the middle step and directly optimized for portfolio weights? There’s some interesting work out there on end-to-end portfolio optimization that might be worth exploring.</p>

    <p>A direction that’s particularly exciting is inspired by <em>Artificial Asset Pricing</em> by Kelly, Mazarei, and Xiu (2023). Instead of predicting returns or Sharpe ratios and then ranking stocks, they use a transformer architecture to learn the stochastic discount factor directly. This approach sidesteps traditional factor modeling entirely, allowing the network to learn pricing kernels from raw panel data. It’s an ambitious setup, but one that might help close the gap between predictive modeling and true portfolio optimization.</p>
  </li>
</ul>

<p>All of this opens the door to a broader research direction — and I’ll dig into these ideas in future posts.</p>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[Roadmap The idea to show performance of LightGBM first using the same approach as the last article and then also compare it to the low vol and linear model and compare performance and show the impressive added value you get from including non-linearities]]></summary></entry><entry><title type="html">Ridge regression</title><link href="http://localhost:4000/quants/2025/02/09/ridge.html" rel="alternate" type="text/html" title="Ridge regression" /><published>2025-02-09T00:00:00+01:00</published><updated>2025-02-09T00:00:00+01:00</updated><id>http://localhost:4000/quants/2025/02/09/ridge</id><content type="html" xml:base="http://localhost:4000/quants/2025/02/09/ridge.html"><![CDATA[<h2 id="todo">TODO:</h2>
<ul>
  <li>refer to combo in tabel</li>
  <li>interpration a bit better</li>
  <li>also say that things are scaled to all the same vol</li>
  <li>make it a bit more personalized</li>
  <li>Add some numbers in the text</li>
  <li>Go a little bit deeper on why things works: Sharpe ratio because of the volatility, sector comparisoin. A lot of it is just a tradeoff between removing noise, adding simple making it simpler for the model
se</li>
</ul>

<p><a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">In the last article</a>, I showed how scaling returns by volatility helped improve performance with minimal complexity. This time, I want to take a more data-driven approach—one where the model learns how to combine different features instead of me setting the rules manually.</p>

<p>Stock prediction is hard. Instead of trying to guess exact returns, I’m focusing on something more practical: figuring out whether stock A will perform better than stock B. I don’t need to know by how much—just which one is likely to do better.</p>

<p>To do this, I’m using a multiple linear regression model that takes in many features—like price changes, trading volume, and risk levels—and learns how to weigh them optimally. The model is trained on a target label, which is just the outcome I want it to predict, like future returns or the Sharpe ratio.</p>

<p>But there are a few challenges. Features behave differently over time—their distributions shift, patterns that worked before can break down, and some signals that seem useful in one market environment might completely disappear in another. On top of that, the signal-to-noise ratio is low, making it tough to extract meaningful insights.</p>

<p>The dataset stays the same as the one used in <a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">the previous article</a>: daily price, volume, and market capitalization data for all Russell 1000 (RIY) constituents, covering about 3,300 stocks historically. We use point-in-time constituents of the RIY and apply basic liquidity filters, excluding stocks priced below $5 to ensure realistic implementation.</p>

<h2 id="feature-engineering">Feature Engineering</h2>

<p>Coming from a statistical and computer science background, I naturally lean toward letting the data determine relationships rather than imposing rigid assumptions. This contrasts with traditional multifactor portfolio approaches, where the modeler manually decides how to combine features (factors). Instead, I use a regression model that learns the optimal feature weights directly from the data.</p>

<p>For now, I’m keeping it simple with linear regression, assuming the relationships are linear and avoiding interaction terms. It’s a straightforward, data-driven approach that focuses on identifying direct, linear connections between features and the target variable.</p>

<h2 id="choosing-predictive-features">Choosing Predictive Features</h2>
<p>Before the model can learn anything useful, I define the right features and select a target variable. I focus on price, volume, market capitalization, and market-derived indicators, computing them daily for all 3,300 stocks in my universe. Here’s a breakdown of the key feature groups:</p>

<p><strong>1. Momentum Features</strong><br />
Capture trend-following behavior.</p>
<ul>
  <li>Lagged returns over 1 to 10 days.</li>
  <li>Rolling cumulative returns over 21 to 252 days.</li>
  <li>MACD to detect shifts in momentum.</li>
</ul>

<p><strong>2. Volatility Features</strong><br />
Measure risk.</p>
<ul>
  <li>Rolling historical volatility over 21, 63, or 126 days.</li>
  <li>Average True Range (ATR) to normalize price fluctuations.</li>
</ul>

<p><strong>3. Liquidity Features</strong><br />
Assess trading activity.</p>
<ul>
  <li>Rolling mean and standard deviation of trading volume.</li>
  <li>Ratio of current volume to its rolling maximum to highlight unusual trading activity.</li>
</ul>

<p><strong>4. Size Features</strong><br />
Measure company scale.</p>
<ul>
  <li>Rolling mean and minimum of market cap.</li>
  <li>Distinguishes small-cap from large-cap stocks.</li>
</ul>

<p><strong>5. Short Mean Reversion Features</strong><br />
Identify when prices revert to historical norms.</p>
<ul>
  <li>Price deviation from its rolling moving average.</li>
  <li>Position relative to rolling minimum and maximum values.</li>
  <li>Bollinger Bands to spot overbought or oversold conditions.</li>
</ul>

<p><strong>6. Correlation with the Market</strong><br />
Capture systematic risk.</p>
<ul>
  <li>Rolling correlation with the Russell 1000 over 63-day windows.</li>
  <li>Helps separate defensive stocks from high-beta names.</li>
</ul>

<p>In total, I work with around 150 predictive features, obviously many of which are correlated.</p>

<h2 id="target-variable">Target Variable</h2>
<p>The model is trained to predict return over the next 20 days and Sharpe ratio over the next 20 days. While other time horizons could be explored, I’m keeping it simple and focusing on 20 days for now.</p>

<h2 id="preprocessing-cross-sectional-normalization">Preprocessing: Cross-Sectional Normalization</h2>

<p>Cross-sectional normalization adjusts each feature relative to all other stocks on the same day, ensuring the model focuses on relative differences rather than absolute values. This transformation also makes interpretation easier—when features are normalized consistently, it becomes simpler to identify high or low values over time.</p>

<p>By applying this normalization, I make sure stocks are evaluated on a comparable basis at each point in time. This should help the model focus on learning the relative order of stocks rather than absolute levels, while also preventing certain features from disproportionately influencing the predictions.</p>

<h2 id="mathematical-formulation">Mathematical Formulation</h2>

<p>For a given feature $X^p$, the normalized value for stock $i$ at time $t$ is:</p>

\[X_{i,t}^{p,\text{norm}} = f\left(X_{i,t}^{p}, X_{1:N,t}^{p}\right)\]

<p>where:</p>

<ul>
  <li>$X^p_{i,t}$ is the raw feature value for stock $i$ at time $t$.</li>
  <li>$X^p_{1:N,t}$ is the set of values for all stocks at time $t$ for feature $p$.</li>
  <li>$f(\cdot)$ is the chosen normalization method.</li>
</ul>

<p>Different methods can be used to transform the raw values, each with its own strengths and tradeoffs. I’ll be comparing Z-scoring and ranking sometimes also called uniformization.</p>

<h2 id="z-scoring">Z-Scoring</h2>

<p>One common approach is z-scoring, which standardizes features by centering them around zero and scaling them to have a standard deviation of one:</p>

\[X_{i,t}^{p,\text{norm}} = \frac{X_{i,t}^{p} - \hat{\mu}^p_t}{\hat{\sigma}^p_t}\]

<p>where:</p>

<ul>
  <li>
    <p>$\hat{\mu}^p_t$ is the estimated mean across all stocks at time $t$ for feature $p$:</p>

\[\hat{\mu}^p_t = \frac{1}{N} \sum_{i=1}^{N} X_{i,t}^{p}\]
  </li>
  <li>
    <p>$\hat{\sigma}^p_t$ is the estimated standard deviation:</p>

\[\hat{\sigma}^p_t = \sqrt{\frac{1}{N} \sum_{i=1}^{N} \left( X_{i,t}^{p} - \hat{\mu}^p_t \right)^2}\]
  </li>
</ul>

<p>Z-scoring retains the relative magnitudes of the original values, allowing the model to distinguish between small and large variations. However, it is sensitive to extreme outliers, so values beyond ±5 standard deviations are clipped.</p>

<h2 id="ranking-normalization">Ranking Normalization</h2>

<p>Another approach is ranking normalization which transforms feature values into ranks and scales them between 0 and 1:</p>

\[R_{i,t}^{p} = \frac{r_{i,t}^{p}}{N}\]

<p>where:</p>

<ul>
  <li>$r^p_{i,t}$ is the rank of stock $i$ at time $t$ based on feature $p$ (0 for the lowest value, $N$ for the highest).</li>
  <li>$R^p_{i,t}$ is the normalized rank.</li>
</ul>

<p>Unlike z-scoring, ranking ensures that the distribution remains the same over time. This makes it robust to extreme values but removes magnitude information—only relative positioning is preserved.</p>

<h2 id="visualizing-the-effect-of-normalization">Visualizing the Effect of Normalization</h2>

<p>Below, in Figure 1, I summarize the different normalization methods applied to a single feature (20-day return). From left to right: the original distribution, z-scored, and ranked.</p>

<p><img src="/assets/ridge/example_normalization.png" alt="Figure 1" /></p>

<p><strong>Figure 1</strong>: Effect of normalization on 20-day return distribution. Left: Original data, Middle: Z-scored, Right: Ranked between 0 and 1.</p>

<h2 id="choosing-the-right-normalization-method">Choosing the Right Normalization Method</h2>

<p>How I normalize features has a big impact on how the model interprets stock differences. The choice between z-scoring and ranking depends on what I want the model to focus on.</p>

<ul>
  <li><strong>Z-scoring</strong> keeps magnitude differences intact, which helps when the strength of a signal matters. But it more sensitive to distribution shifts over time.</li>
  <li><strong>Ranking</strong> is more stable and removes extreme outliers since values are always mapped to a uniform distribution. However, this process also discards information about the magnitude of differences between stocks</li>
</ul>

<p>Both methods ensure that stocks are processed in a comparable way on any given day, but they emphasize different aspects of the data.</p>

<h2 id="evaluating-the-impact-of-normalization">Evaluating the Impact of Normalization</h2>

<p>To see if normalization improves results, I compare three approaches:</p>

<ol>
  <li>Using raw, unnormalized features</li>
  <li>Applying z-scoring across all stocks</li>
  <li>Using ranking across all stocks</li>
</ol>

<p>If normalization improves performance, the next step is to refine how I apply it—especially to the target label.</p>

<h2 id="should-the-target-label-be-normalized-by-sector">Should the Target Label Be Normalized by Sector?</h2>

<p>Normalizing features ensures consistency over time, but what about the target label? Instead of normalizing returns across all stocks, I test whether normalizing them within each sector improves results while keeping all other features globally normalized.</p>

<ul>
  <li><strong>Global normalization</strong> applies the same normalization across the full stock universe.</li>
  <li><strong>Sector-specific normalization</strong> adjusts returns within each sector while keeping all other features globally normalized.</li>
</ul>

<p>My hypothesis is that sector-normalizing the target label might help by preventing cross-sector differences from distorting the model’s learning process. Stocks in different industries often have structurally different return profiles, so this adjustment could make return comparisons more meaningful. Whether this actually improves performance is something I aim to find out.</p>

<h2 id="handling-missing-data">Handling Missing Data</h2>

<p>Some models, like decision trees, handle missing data automatically, but others don’t. To keep things simple, I use:</p>

<ul>
  <li><strong>Forward fill:</strong> Use the last known value if past data exists.</li>
  <li><strong>Cross-sectional mean imputation:</strong> If no past data is available, replace the missing value with the sector average for that day.</li>
  <li><strong>Default values:</strong>
    <ul>
      <li>For z-scoring, set missing values to 0.</li>
      <li>For ranking, set missing values to 0.5 (midpoint of the ranking scale).</li>
    </ul>
  </li>
</ul>

<p>This approach is simple, effective, and works well for now.</p>

<h2 id="modeling-the-cross-sectional-normalized-score">Modeling the Cross-Sectional Normalized Score</h2>

<p>At the core of this strategy, I’m building a model to predict a stock’s cross-sectional normalized score, which could be its Sharpe ratio, return, or another performance measure. I think of this as a function mapping available information at time $t$ to an expected score at $t+1$. To ensure comparability across stocks, the score is normalized in the cross-section before modeling.</p>

<p>Mathematically, I assume that there exists a function $g(\cdot)$ such that:</p>

\[s_{i,t+1} = g(\mathbf{z}_{i,t}) + \epsilon_{i,t+1}\]

<p>where:</p>
<ul>
  <li>$s_{i,t+1}$ is the true cross-sectional normalized score for stock $i$ at time $t+1$.</li>
  <li>$z_{i,t}$ is a vector of predictor variables for stock $i$ at time $t$.</li>
  <li>$\epsilon_{i,t+1}$ is the error term, representing what the model cannot predict.</li>
</ul>

<p>The objective is to approximate $g(\cdot)$ using historical data. This function follows two key principles:</p>

<ul>
  <li>it leverage the entire panel of stocks, meaning the same functional form applies universally.</li>
  <li>It depends only on stock-specific features at time $t$. While some features contain past information (such as return over the past 20 days), these are explicitly engineered rather than dynamically learned. In addition, the model does not learn interactions between different stocks.</li>
</ul>

<h3 id="ridge-regression-as-a-baseline">Ridge Regression as a Baseline</h3>

<p>To estimate $g(\cdot)$, I use Ridge Regression, a simple yet effective baseline, particularly when predictors are highly correlated. It solves the following optimization problem:</p>

\[\underset{\boldsymbol{\beta}}{\min} \frac{1}{n} \sum_{i=1}^n (s_{i,t+1} - \mathbf{x}_i^\top \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \beta_j^2\]

<p>where the second term, $\lambda \sum_{j=1}^p \beta_j^2$, regularizes the coefficients to prevent instability.</p>

<p>Ridge is a reasonable choice here because:</p>
<ul>
  <li>Stocks with similar characteristics often exhibit collinearity, and Ridge helps stabilize coefficient estimates.</li>
  <li>The regularization term shrinks extreme values, reducing sensitivity to noise.</li>
  <li>It provides a simple reference point before exploring more complex models.</li>
</ul>

<p>The model is estimated using historical data, and to assess its effectiveness, I apply an expanding walkforward validation which I explain just below in a bit more detail.</p>

<h2 id="expanding-walkforward-validation">Expanding Walkforward Validation</h2>

<p>To see how well the model holds up over time, I use an expanding walkforward validation. The idea is simple:</p>

<ol>
  <li>Start with a 3-year burn-in period – The model isn’t tested yet; it just learns from the data.</li>
  <li>Update the model every 2 years – Each time, I add the latest data and refit the model.</li>
  <li>Keep expanding the dataset – Older data stays in, and new data gets added.</li>
</ol>

<p>With stock data, I’ve always found that the more historical data, the better. The signal-to-noise ratio is low, so keeping as much information as possible helps the model the find the signal in all the noise.</p>

<p>A rolling validation window could be an alternative, but it discards older data that might still be valuable. In my experience, an expanding window works better because it allows the model to pick up long-term relationships, leading to more stable predictions.</p>

<p>For hyperparameter tuning, one option is to split the training set into separate train and validation sets. But honestly, I’ve never found this to be worth the extra time. Optimizing hyperparameters can take a while, and in most cases, default values that make sense are already a very good starting point.</p>

<p>Below is a schematic of the expanding walkforward approach:</p>

<p><img src="/assets/ridge/walk-forward.png" alt="Figure 2" /></p>

<p><strong>Figure 2</strong>: Expanding walkforward validation process.</p>

<h2 id="portfolio-construction">Portfolio Construction</h2>

<p>Once I have stock rankings, I build a long-short portfolio:</p>

<ul>
  <li>I go long on the 75 stocks with the highest scores.</li>
  <li>I short the 75 stocks with the lowest scores.</li>
</ul>

<p>The approach is robust across different portfolio sizes, whether using 50, 100, or 150 stocks.</p>

<p>To keep risk under control, I use volatility targeting:</p>
<ul>
  <li>Higher-volatility stocks get smaller weights.</li>
  <li>Lower-volatility stocks get larger weights.</li>
</ul>

<p>This ensures that the portfolio maintains a stable risk profile instead of being dominated by a few volatile names.</p>

<p>For a deeper dive into my portfolio construction process, check out my <a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">previous article</a> where I go into more detail.</p>

<h2 id="results">Results</h2>
<p>To evaluate different modeling choices, I tested 10 model variations, combining:</p>
<ul>
  <li>5 normalization methods: Raw, Z-score (global &amp; sector), Ranking (global &amp; sector).</li>
  <li>2 target labels: Sharpe Ratio (SR 20) and Return (Return 20).</li>
  <li>A combined strategy (“Combo”), which equally weights all strategies.</li>
</ul>

<p>To ensure a fair comparison visually, all strategies are scaled to 10% volatility. The goal is to understand how normalization, sector adjustments, and target labels affect performance. Figure 3 visualizes cumulative returns across all strategies—without labels, adding a bit of suspense.</p>

<p>While all models deliver positive returns, there is a clear differences in performance.</p>

<p><img src="/assets/ridge/all_lines.png" alt="Figure 3" /></p>

<p><strong>Figure 3</strong>: Cumulative returns of all strategies, scaled to 10% volatility.</p>

<p>Figure 4 shows how different normalization methods, sector adjustments, and target label choices affect Sharpe Ratios across the models. It compares Z-scoring, raw features, and ranking, and also looks at the effect of normalizing within sectors versus globally. Plus, it shows how using Sharpe Ratios or raw returns as the target label changes things.</p>

<p>First off, normalization is pretty clear—Z-scoring works best, then ranking, and raw features consistently underperform. I was honestly expecting ranking to be the top performer because it’s supposed to stabilize things, but it seems like it might also take out some useful signals. Z-scoring holds onto more of that valuable information, which is why it does so well. Raw features just add noise, so they end up being the weakest choice.</p>

<p>Then, sector adjustment comes in and has an even bigger impact. Normalizing within sectors really improves Sharpe Ratios compared to global normalization. It makes sense because comparing stocks within the same sector gives us more relevant context. By normalizing within sectors, I’m making sure that sector-wide noise doesn’t interfere with the real signals, so the rankings are more stable.</p>

<p>Lastly, the target label is by far the most important factor. When I use Sharpe Ratios as the target, the models consistently perform better than when I use raw returns. This is no surprise—it’s easier to predict volatility than raw returns, so Sharpe Ratios give more reliable, risk-adjusted performance.</p>

<p>To sum it up, while normalization and sector adjustments matter, the key takeaway is the target label. Sharpe Ratios beat raw returns every time, and sector normalization makes the rankings a lot stronger.</p>

<p><img src="/assets/ridge/summary_barplot.png" alt="Figure 4" /></p>

<p><strong>Figure 4</strong>: Sharpe Ratio performance across key modeling choices.</p>

<h3 id="normalization-effects-depend-on-the-target-label">Normalization Effects Depend on the Target Label</h3>

<p>Digging a little bit deeper, Figure 5 visualizes the effect of normalization conditioned on the target label.</p>

<p>For Return 20 models, normalization had a smaller effect, but ranking and Z-scoring still outperformed raw features. Interestingly, Z-scoring has regained popularity in recent years.</p>

<p>For Sharpe Ratio models, the impact of normalization was stronger. Z-scoring was clearly the best performer, followed by ranking, then raw features.</p>

<p><img src="/assets/ridge/normalization_target.png" alt="Figure 5" /></p>

<p><strong>Figure 5</strong>: Cumulative return of different normalization methods, conditioned on the target label. Volatility is set at 10% for all strategies.</p>

<h3 id="key-takeaways">Key Takeaways</h3>
<ul>
  <li>Normalization improves signal stability, helping models generalize better.</li>
  <li>Sector-based adjustments on the target label refine comparisons, preventing large sector-specific biases.</li>
  <li>Target label choice affects robustness, with Sharpe Ratio-based models performing better.</li>
</ul>

<h2 id="the-combo-strategy-holds-up-well">The “Combo” Strategy Holds Up Well</h2>

<p>I was surprised to see that the “Combo” strategy performed really well, coming in second for Sharpe ratio (you can see it in Table 1). Instead of picking just one top model, it weights all strategies equally—and still ended up second overall.</p>

<p>Even without fine-tuning, blending multiple models helped smooth the performance and made it more stable. It’s pretty clear to me that diversifying across models can outperform just sticking with a single “best” model.</p>

<h2 id="full-performance-breakdown">Full Performance Breakdown</h2>

<p>To quantify these findings, here’s the performance breakdown across models:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Return (ann. %)</th>
      <th>Volatility (ann. %)</th>
      <th>Sharpe Ratio (ann.)</th>
      <th>Max. Drawdown (%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SR Z-Score By Sector</td>
      <td>12.24</td>
      <td>7.94</td>
      <td>1.54</td>
      <td>16.09</td>
    </tr>
    <tr>
      <td>Combo</td>
      <td>8.99</td>
      <td>6.62</td>
      <td>1.36</td>
      <td>15.19</td>
    </tr>
    <tr>
      <td>SR Z-Score Global</td>
      <td>10.68</td>
      <td>8.30</td>
      <td>1.29</td>
      <td>18.39</td>
    </tr>
    <tr>
      <td>SR Ranking By Sector</td>
      <td>9.95</td>
      <td>7.83</td>
      <td>1.27</td>
      <td>13.94</td>
    </tr>
    <tr>
      <td>SR Ranking Global</td>
      <td>10.40</td>
      <td>8.41</td>
      <td>1.24</td>
      <td>15.93</td>
    </tr>
    <tr>
      <td>SR Raw Global</td>
      <td>9.76</td>
      <td>8.39</td>
      <td>1.16</td>
      <td>15.89</td>
    </tr>
    <tr>
      <td>Return Ranking By Sector</td>
      <td>8.14</td>
      <td>7.50</td>
      <td>1.09</td>
      <td>15.96</td>
    </tr>
    <tr>
      <td>Return Z-Score By Sector</td>
      <td>8.07</td>
      <td>7.43</td>
      <td>1.09</td>
      <td>19.37</td>
    </tr>
    <tr>
      <td>Return Raw Global</td>
      <td>6.95</td>
      <td>7.54</td>
      <td>0.92</td>
      <td>18.99</td>
    </tr>
    <tr>
      <td>Return Ranking Global</td>
      <td>6.48</td>
      <td>7.26</td>
      <td>0.89</td>
      <td>18.86</td>
    </tr>
    <tr>
      <td>Return Z-Score Global</td>
      <td>6.44</td>
      <td>7.67</td>
      <td>0.84</td>
      <td>25.95</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1</strong>: Performance metrics across different modeling choices, ranked by Sharpe Ratio in descending order. Results exclude transaction costs and slippage.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>Sector normalization (target label), normalization method, and target label choice all had a meaningful impact on performance:</p>
<ul>
  <li>Sector normalization was a game-changer—comparing stocks within their sector led to major improvements.</li>
  <li>Normalization method mattered more than expected—Z-scoring outperformed ranking, contradicting my initial intuition.</li>
  <li>Sharpe Ratio models consistently outperformed return-based models, reinforcing the importance of risk-adjusted metrics.</li>
</ul>

<p>Instead of searching for a single best model, it may be smarter to combine perspectives. The “Combo” strategy showed that diversification across models stabilizes results—even without fine-tuning.</p>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[TODO: refer to combo in tabel interpration a bit better also say that things are scaled to all the same vol make it a bit more personalized Add some numbers in the text Go a little bit deeper on why things works: Sharpe ratio because of the volatility, sector comparisoin. A lot of it is just a tradeoff between removing noise, adding simple making it simpler for the model se]]></summary></entry><entry><title type="html">The Low Volatility Factor: A Steady Approach</title><link href="http://localhost:4000/quant/2024/12/15/low-volatility-factor.html" rel="alternate" type="text/html" title="The Low Volatility Factor: A Steady Approach" /><published>2024-12-15T00:00:00+01:00</published><updated>2024-12-15T00:00:00+01:00</updated><id>http://localhost:4000/quant/2024/12/15/low-volatility-factor</id><content type="html" xml:base="http://localhost:4000/quant/2024/12/15/low-volatility-factor.html"><![CDATA[<p>The low-volatility factor is based on a simple idea: stocks that move less tend to deliver better risk-adjusted returns than those with more extreme price swings. It’s a pattern that has been observed not only in equities but also in other asset classes.</p>

<p>This post is the first in a series on cross-sectional stock selection. I’ll begin with a single-factor strategy, then gradually build up: combining multiple signals using linear regression, testing more advanced design choices, and later exploring interactions and non-linearities with models like LightGBM. At the end, I’ll compare all approaches to see whether complexity actually improves performance. But first, let’s keep things simple—and see how far a basic low-volatility sort can take us.</p>

<h2 id="tradeable-universe">Tradeable Universe</h2>

<p>The dataset covers the Russell 1000 (RIY), which tracks the largest U.S. stocks. To keep it realistic, I filter out stocks priced under $5. The sample runs from 1995 to 2024, covering around 3,300 stocks as companies enter and exit the index. At any given time, about 1,000 stocks are tradeable. Since it uses point-in-time constituents, there’s no survivorship bias. Figures 1 visualizes the number of tradeable stocks over time.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/nr_stocks.svg" alt="Figure 1" /><br />
Figure 1: Number of tradeable stocks over time.</p>

<h2 id="measuring-volatility">Measuring Volatility</h2>

<p>To identify low-volatility stocks, I compute the standard deviation of daily returns—a standard way to quantify how much a stock’s price fluctuates. Specifically, I use three short-term rolling windows:</p>

<ul>
  <li>5 trading days</li>
  <li>10 trading days</li>
  <li>21 trading days</li>
</ul>

<p>Shorter windows react more quickly to changes in market conditions, while longer windows provide more stable estimates. By combining them, I aim for a volatility signal that’s both responsive and robust.</p>

<p>Volatility is computed as:</p>

\[\hat{\sigma}_{i,t} = \sqrt{\frac{1}{N-1} \sum_{j=1}^N \left( r_{i,t-j} - \bar{r}_{i,t} \right)^2}\]

<p>where $r_{i,t}$ is the daily return of stock $i$ at time $t$, and $N$ is the length of the rolling window. I annualize this by multiplying the daily volatility by $\sqrt{252}$, assuming 252 trading days in a year.</p>

<p>Across the dataset, the average annualized volatility is about 33%, with most stocks falling between 18% and 39%. The median is 26%. A few names have extreme swings, so I winsorize the volatility values at 5% and 200% to prevent outliers from distorting the rankings.</p>

<p>As shown in Figure 2, the distribution is right-skewed: most stocks cluster around moderate volatility levels, but a small number show very high fluctuations.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/distribution_volatilities.svg" alt="Figure 2" /><br />
<strong>Figure 2</strong>: Distribution of annualized volatility across all stocks.</p>

<h2 id="does-low-volatility-matter">Does Low Volatility Matter?</h2>

<p>To check whether low-volatility stocks behave differently, I looked at their returns over the next 10 trading days. I focused on two things: raw returns and risk-adjusted returns, using the Sharpe ratio.</p>

<p>The Pearson correlation between volatility and raw return came out slightly positive, around 0.03. So, more volatile stocks seemed to perform a little better—at least at first glance. That wasn’t exactly what I expected, but it’s a small effect. Switching to Spearman correlation, which is more robust to outliers, the result flattened out to zero.</p>

<p>The picture changes when looking at Sharpe ratios. Here, the correlation with volatility was negative: -0.035 with Pearson, and -0.04 with Spearman. So while higher-vol stocks might deliver the occasional bigger return, they tend to do so with more noise. On a risk-adjusted basis, they come out worse.</p>

<p>The signal is weak, but that’s typical. What matters is that it shows up consistently, especially when applied across a large universe.</p>

<h2 id="sorting-stocks-into-portfolios">Sorting Stocks into Portfolios</h2>

<p>To turn this into a tradeable strategy, I rank stocks by volatility at each point in time and sort them into five portfolios. This ensures that portfolio assignments are always relative to the current market.</p>

<p>Here’s how it works:</p>
<ol>
  <li>Compute rolling volatility for each stock.</li>
  <li>Rank stocks by volatility within the universe.</li>
  <li>Normalize ranks to a 0-1 scale.</li>
  <li>Assign stocks to one of five portfolios based on percentile rank.</li>
</ol>

<p>Let $r_{i,t}$ be the cross-sectional rank of stock $i$ at time $t$, and $N$ be the number of stocks. The normalized rank is:</p>

\[\frac{r_{i,t}}{N}\]

<p>Stocks are then grouped into these buckets:</p>

<ul>
  <li>Portfolio 1: Lowest 10% of stocks ($0 \leq \text{Rank Score} &lt; 0.1$) → Low volatility</li>
  <li>Portfolio 2: 10% to 20% of stocks ($0.1 \leq \text{Rank Score} &lt; 0.2$)</li>
  <li>Portfolio 3: 20% to 80% ($0.2 \leq \text{Rank Score} &lt; 0.8$)</li>
  <li>Portfolio 4: 80% to 90% of stocks ($0.8 \leq \text{Rank Score} &lt; 0.9$)</li>
  <li>Portfolio 5: Highest 10% ($0.9 \leq \text{Rank Score} \leq 1.0$) → High volatility</li>
</ul>

<p>This way, every stock’s classification is determined relative to the cross-sectional volatility of the market at that time.</p>

<h2 id="portfolio-construction">Portfolio Construction</h2>

<p>Once the stocks are grouped into buckets, I construct two types of portfolios: one that assigns equal weights to each stock, and another that adjusts weights to target a specific volatility level.</p>

<h3 id="1-equal-weighted-portfolio">1. Equal-Weighted Portfolio</h3>

<p>In the equal-weighted portfolio, each stock receives the same weight:</p>

\[w_{i,t} = \frac{1}{N_t}\]

<p>where \(N_t\) is the number of stocks in the portfolio at time \(t\). The portfolio remains fully invested. To create the long-short strategy, I go long the low-volatility portfolio (P1) and short the high-volatility portfolio (P5), using equal weights on both sides.</p>

<p>This setup introduces a problem: the two legs have different levels of volatility. Low-volatility stocks naturally exhibit less risk, so the short leg tends to dominate in terms of exposure. This imbalance reduces the effectiveness of the strategy on a risk-adjusted basis.</p>

<h3 id="2-volatility-targeted-portfolio">2. Volatility-Targeted Portfolio</h3>

<p>To correct for this imbalance, I use volatility targeting. The idea is to scale the weight of each stock based on its volatility relative to a fixed target. The scaling factor is defined as:</p>

\[\alpha_{i,t} = \frac{\sigma_{\text{target}}}{\hat{\sigma}_{i,t}}\]

<p>where:</p>

<ul>
  <li>\(\sigma_{\text{target}} = 20\%\) is a fixed target volatility level</li>
  <li>\(\hat{\sigma}_{i,t}\) is the estimated future volatility of stock \(i\), computed using a 60-day rolling standard deviation</li>
</ul>

<p>Each stock’s weight becomes:</p>

\[w_{i,t} = \frac{1}{N_t} \cdot \alpha_{i,t}\]

<p>This approach increases the weight of stocks with below-target volatility and reduces the weight of those above it. To avoid excessive concentration, I cap individual weights at 4%. The total portfolio weight is also constrained to remain below or equal to 1.0.</p>

<p>During periods of high volatility, the portfolio may become partially invested to stay within the exposure limit. This isn’t market timing—it’s a way to control portfolio-level risk dynamically.</p>

<p>Portfolios are rebalanced weekly based on updated volatility estimates. Transaction costs are not included in this analysis.</p>

<h2 id="results">Results</h2>

<h3 id="equal-weighted-portfolio">Equal-Weighted Portfolio</h3>

<p>I start by evaluating the equal-weighted version of the strategy. The long side holds the lowest-volatility stocks (P1), and the short side holds the highest-volatility ones (P5). Within each group, stocks are given equal weight.</p>

<p>On their own, the results make sense. The low-volatility portfolio delivers a geometric return of 11.8% with a Sharpe ratio of 1.0. The high-volatility portfolio returns 4.7%, with much higher volatility and little risk-adjusted performance.</p>

<p>But when combining the two into a long-short portfolio, the performance breaks down. The issue is a volatility mismatch: the short side is much more volatile than the long side—38.6% versus 12.2%. Equal weighting doesn’t account for this difference. As a result, the short leg ends up driving most of the portfolio’s risk.</p>

<p>The final long-short portfolio has high volatility (32.6%), a negative Sharpe ratio (-0.2), and a drawdown over 90%. Figure 3 shows the key performance metrics before applying volatility targeting. A full summary is included in Table 1.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/barplot_metrics_ew.png" alt="Figure 3" /></p>

<p><strong>Figure 3</strong>: Geometric Return, Volatility, and Sharpe Ratio for equal-weighted portfolios (before volatility targeting).</p>

<h3 id="volatility-targeted-portfolio">Volatility-Targeted Portfolio</h3>

<p>Volatility targeting adjusts for the imbalance between long and short legs by scaling each stock’s weight relative to a fixed volatility target (set here at 20%). This helps align the total risk of the long and short portfolios and results in more stable performance.</p>

<p>After applying volatility targeting, the improvements are clear. The long-short portfolio volatility drops from 32.6% to 8.3%, and the Sharpe ratio rises from -0.2 to 0.9. The max drawdown is also cut significantly—from over 90% to 33.6%.</p>

<p>The long-only portfolio becomes slightly more stable as well. Its volatility decreases from 12.2% to 9.7%, while the Sharpe ratio improves from 1.0 to 1.1.</p>

<p>Figure 4 shows how volatility, return, and Sharpe ratio change across the five portfolios after targeting is applied. The result is more balanced exposure and better overall risk-adjusted returns.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/barplot_metrics_ew_vt.png" alt="Figure 4" /><br />
<strong>Figure 4</strong>: Geometric return, volatility, and Sharpe ratio after volatility targeting.</p>

<h3 id="performance-over-time">Performance Over Time</h3>

<p>The impact of volatility targeting is also visible over time. Figure 5 shows the net asset value of the long-short portfolio after adjustment. Compared to the equal-weighted version, returns are smoother and drawdowns less severe.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/perf_backtest_ew_vt.png" alt="Figure 5" /><br />
<strong>Figure 5</strong>: Net asset value of the volatility-targeted long-short portfolio.</p>

<h3 id="summary-table">Summary Table</h3>

<p>To summarize the improvements, here are the core metrics before and after applying volatility targeting:</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Long (No VT)</th>
      <th>Long (VT)</th>
      <th>Short (No VT)</th>
      <th>Short (VT)</th>
      <th>L/S (No VT)</th>
      <th>L/S (VT)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Geom. Return (%)</td>
      <td>11.8</td>
      <td>11.1</td>
      <td>4.7</td>
      <td>2.9</td>
      <td>-5.3</td>
      <td>7.7</td>
    </tr>
    <tr>
      <td>Volatility (%)</td>
      <td>12.2</td>
      <td>9.7</td>
      <td>38.6</td>
      <td>9.7</td>
      <td>32.6</td>
      <td>8.3</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>1.0</td>
      <td>1.1</td>
      <td>0.1</td>
      <td>0.3</td>
      <td>-0.2</td>
      <td>0.9</td>
    </tr>
    <tr>
      <td>Max Drawdown (%)</td>
      <td>40.2</td>
      <td>29.5</td>
      <td>89.6</td>
      <td>36.7</td>
      <td>91.6</td>
      <td>33.6</td>
    </tr>
    <tr>
      <td>Max TUW (Days)</td>
      <td>863</td>
      <td>612</td>
      <td>4802</td>
      <td>1647</td>
      <td>5539</td>
      <td>944</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1</strong>: Performance metrics before and after volatility targeting. “VT” applies targeting; “No VT” uses equal weighting.</p>

<h3 id="portfolio-exposure-over-time">Portfolio Exposure Over Time</h3>

<p>Figure 6 shows how total portfolio weights evolve over time for both the long (low-volatility) and short (high-volatility) sides after applying volatility targeting.</p>

<p>By total weight, I mean the sum of all individual stock weights within each leg. Since the short side holds more volatile stocks, it naturally receives less capital—typically between 0.2 and 0.6. The long side, made up of more stable names, stays closer to fully invested.</p>

<p>This behaviour is expected. Volatility targeting reduces exposure to riskier assets and increases it for more stable ones. During periods of market stress—like the dot-com crash, the financial crisis, or COVID—both legs reduce their exposure. The mechanism reacts by shrinking position sizes to stay within the target risk level.</p>

<p>The result is a portfolio that adapts to changing market conditions without needing explicit market timing.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/portfolio_weights_long_short_vol_target.svg" alt="Figure 6" /><br />
<strong>Figure 6:</strong> Total weight of all positions in the long (P1) and short (P5) portfolios after volatility targeting.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p>The low-volatility factor delivers stronger risk-adjusted returns. Lower-vol stocks tend to outperform, which goes against the usual link between higher risk and higher return.</p>
  </li>
  <li>
    <p>Equal weighting in a long-short setup creates a risk imbalance. The short leg ends up dominating volatility, which pulls down performance.</p>
  </li>
  <li>
    <p>Volatility targeting fixes that imbalance. It adjusts weights to align the risk on both sides, making the strategy more stable and improving Sharpe ratios.</p>
  </li>
</ol>

<p>Even this simple adjustment already helps a lot. In the next post, I’ll look at how combining multiple signals can take things a step further.</p>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quant" /><summary type="html"><![CDATA[The low-volatility factor is based on a simple idea: stocks that move less tend to deliver better risk-adjusted returns than those with more extreme price swings. It’s a pattern that has been observed not only in equities but also in other asset classes.]]></summary></entry></feed>