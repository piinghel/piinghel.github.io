<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4006/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4006/" rel="alternate" type="text/html" /><updated>2025-12-07T23:04:47+01:00</updated><id>http://localhost:4006/feed.xml</id><title type="html">Quant Notes</title><subtitle>Systematic trading and data science things.
</subtitle><author><name>anon</name></author><entry><title type="html">Forecasting Volatility with Panel Regressions</title><link href="http://localhost:4006/quants/2025/11/08/risk-modeling.html" rel="alternate" type="text/html" title="Forecasting Volatility with Panel Regressions" /><published>2025-11-08T00:00:00+01:00</published><updated>2025-11-08T00:00:00+01:00</updated><id>http://localhost:4006/quants/2025/11/08/risk-modeling</id><content type="html" xml:base="http://localhost:4006/quants/2025/11/08/risk-modeling.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>I scale positions by volatility. And so should you.</p>

<p>The idea is simple: take smaller positions in volatile stocks, larger positions in stable ones. This keeps your portfolio risk roughly constant over time. It’s called volatility targeting, and it works.</p>

<p>But here’s the thing—to do this well, you need to <em>predict</em> future volatility. Not yesterday’s volatility, but what volatility will be over your holding period. Most people don’t spend much time on this. They use the last month’s realized vol and call it a day.</p>

<div style="border: 2px solid #333; padding: 1em; margin: 1em 0; background-color: #f9f9f9;">
<strong>The question:</strong> Is it worth spending time trying to predict volatility better?
</div>

<p>This post answers that question. Spoiler: yes, it’s worth it. Perfect volatility forecasts would add 3-4% annualized return to my strategy. Even modest improvements (correlation 0.70 → 0.86) should capture a good chunk of that.</p>

<h3 id="how-i-use-volatility-forecasts">How I Use Volatility Forecasts</h3>

<p>In my <a href="/quant/2024/12/15/low-volatility-factor.html">previous post on the low-volatility factor</a>, I built a long-short equity strategy. Position sizing combines two things: ML signal quality and volatility forecasts.</p>

<p>The allocation has two stages. First, I convert ML scores into initial weights $\alpha_i$. Better predictions get more capital on the long side; worse predictions get more capital on the short side (weights are always positive, just assigned to different legs).</p>

<p>Then I scale each position by volatility to target a fixed risk level:</p>

\[w_i = \alpha_i \cdot \min\left(\frac{\sigma_{\text{target}}}{\hat{\sigma}_i}, \lambda_{\max}\right)\]

<p>where:</p>
<ul>
  <li>\(\hat{\sigma}_i\) = volatility forecast for stock \(i\)</li>
  <li>\(\sigma_{\text{target}}\) = target volatility (20% annualized)</li>
  <li>\(\lambda_{\max} = 3\) = leverage cap</li>
</ul>

<p>The intuition: if a stock has half the target vol, I can take twice the position (up to the cap).</p>

<p>Finally, I apply constraints: 5% max per stock, total exposure ≤ 100%.</p>

<p>The key term is $\hat{\sigma}_i$. Currently I estimate it with simple moving averages—20-day rolling vol for longs, 60-day for shorts. When these forecasts are wrong, position sizing suffers:</p>

<ul>
  <li><strong>Underestimate vol</strong> → positions too large → deeper drawdowns</li>
  <li><strong>Overestimate vol</strong> → positions too small → missed returns</li>
</ul>

<h3 id="the-perfect-foresight-experiment">The Perfect Foresight Experiment</h3>

<p>To see what’s at stake, I run a backtest with perfect foresight. What if I knew the exact realized volatility for the next 21 days? This is obviously impossible in practice, but it shows the upper bound.</p>

<p>I test three scenarios:</p>
<ul>
  <li><strong>Current:</strong> Rolling volatility estimates (what I use now)</li>
  <li><strong>Perfect Long:</strong> Perfect foresight on the long leg only</li>
  <li><strong>Perfect:</strong> Perfect foresight on both legs (the upper bound)</li>
</ul>

<h3 id="long-short-performance">Long-Short Performance</h3>

<figure style="margin: 0.3em 0 0 0; padding: 0; line-height: 1;">
<iframe src="/assets/vol_impact/vol_impact_comparison.html" width="100%" height="420" frameborder="0" style="display: block; margin: 0; padding: 0; border: none; vertical-align: bottom;"></iframe>
<figcaption style="margin: 0.1em 0 0 0; padding: 0; font-size: 0.9em; line-height: 1.3; color: #888;">Figure 1: Long-short portfolio performance with different volatility estimation methods.</figcaption>
</figure>

<h3 id="individual-legs">Individual Legs</h3>

<figure style="margin: 0.25em 0 0 0; padding: 0; line-height: 1;">
<iframe src="/assets/vol_impact/long_short_legs_comparison.html" width="100%" height="410" frameborder="0" style="display: block; margin: 0; padding: 0; border: none; vertical-align: bottom;"></iframe>
<figcaption style="margin: 0.08em 0 0 0; padding: 0; font-size: 0.9em; line-height: 1.3; color: #888;">Figure 2: Long and short legs separately—where does the improvement come from?</figcaption>
</figure>

<h3 id="the-numbers">The Numbers</h3>

<p>Here’s the punchline. First, the long-short portfolios:</p>

<div style="overflow-x: auto;">
<table style="width: 100%; border-collapse: collapse; margin: 0.5em 0;">
<thead>
<tr>
<th style="padding: 0.6em; text-align: left; border-bottom: 2px solid #333;">Scenario</th>
<th style="padding: 0.6em; text-align: center; border-bottom: 2px solid #333;">Return</th>
<th style="padding: 0.6em; text-align: center; border-bottom: 2px solid #333;">Sharpe</th>
<th style="padding: 0.6em; text-align: center; border-bottom: 2px solid #333;">Max DD</th>
</tr>
</thead>
<tbody>
<tr style="background: #fafafa;">
<td style="padding: 0.6em; border-bottom: 1px solid #eee;">Current</td>
<td style="padding: 0.6em; text-align: center; border-bottom: 1px solid #eee;">12.3%</td>
<td style="padding: 0.6em; text-align: center; border-bottom: 1px solid #eee;">1.76</td>
<td style="padding: 0.6em; text-align: center; border-bottom: 1px solid #eee; color: #b00;">-16.2%</td>
</tr>
<tr style="background: #e8f5e9;">
<td style="padding: 0.6em; font-weight: bold; border-bottom: 1px solid #eee;">Perfect Long</td>
<td style="padding: 0.6em; text-align: center; border-bottom: 1px solid #eee; color: #2e7d32; font-weight: bold;">16.0%</td>
<td style="padding: 0.6em; text-align: center; border-bottom: 1px solid #eee; color: #2e7d32; font-weight: bold;">2.65</td>
<td style="padding: 0.6em; text-align: center; border-bottom: 1px solid #eee; color: #2e7d32;">-7.8%</td>
</tr>
<tr style="background: #f5f5f5;">
<td style="padding: 0.6em;">Perfect</td>
<td style="padding: 0.6em; text-align: center;">12.5%</td>
<td style="padding: 0.6em; text-align: center;">2.18</td>
<td style="padding: 0.6em; text-align: center; color: #2e7d32;">-7.8%</td>
</tr>
</tbody>
</table>
</div>
<p style="margin: 0.3em 0 1em 0; font-size: 0.9em; color: #888;">Table 1: Long-short portfolio performance (with fees). Perfect Long = perfect foresight on long leg only.</p>

<p>Wait—Perfect Long beats Perfect? Yes. When you give the short leg perfect foresight, it takes larger positions in low-vol stocks that happen to be bad shorts. The long leg is where better vol forecasts really pay off.</p>

<p>And here’s the leg-by-leg breakdown:</p>

<div style="overflow-x: auto;">
<table style="width: 100%; border-collapse: collapse; margin: 1em 0;">
<thead>
<tr>
<th style="padding: 0.6em; text-align: left; border-bottom: 2px solid #333;"></th>
<th colspan="3" style="padding: 0.6em; text-align: center; border-bottom: 2px solid #333; background: #f8f8f8;">Current</th>
<th colspan="3" style="padding: 0.6em; text-align: center; border-bottom: 2px solid #333; background: #e8f5e9;">Perfect Foresight</th>
</tr>
<tr style="font-size: 0.8em; color: #666;">
<th style="padding: 0.4em; border-bottom: 1px solid #ddd;"></th>
<th style="padding: 0.4em; text-align: center; border-bottom: 1px solid #ddd; background: #f8f8f8;">Return</th>
<th style="padding: 0.4em; text-align: center; border-bottom: 1px solid #ddd; background: #f8f8f8;">Sharpe</th>
<th style="padding: 0.4em; text-align: center; border-bottom: 1px solid #ddd; background: #f8f8f8;">Max DD</th>
<th style="padding: 0.4em; text-align: center; border-bottom: 1px solid #ddd; background: #e8f5e9;">Return</th>
<th style="padding: 0.4em; text-align: center; border-bottom: 1px solid #ddd; background: #e8f5e9;">Sharpe</th>
<th style="padding: 0.4em; text-align: center; border-bottom: 1px solid #ddd; background: #e8f5e9;">Max DD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="padding: 0.6em; font-weight: bold; border-bottom: 1px solid #eee;">Long</td>
<td style="padding: 0.6em; text-align: center; border-bottom: 1px solid #eee; background: #fafafa;">13.1%</td>
<td style="padding: 0.6em; text-align: center; border-bottom: 1px solid #eee; background: #fafafa;">1.14</td>
<td style="padding: 0.6em; text-align: center; border-bottom: 1px solid #eee; background: #fafafa; color: #b00;">-29.7%</td>
<td style="padding: 0.6em; text-align: center; border-bottom: 1px solid #eee; background: #e8f5e9; color: #2e7d32; font-weight: bold;">17.1%</td>
<td style="padding: 0.6em; text-align: center; border-bottom: 1px solid #eee; background: #e8f5e9; color: #2e7d32; font-weight: bold;">1.92</td>
<td style="padding: 0.6em; text-align: center; border-bottom: 1px solid #eee; background: #e8f5e9; color: #2e7d32;">-14.1%</td>
</tr>
<tr>
<td style="padding: 0.6em; font-weight: bold;">Short</td>
<td style="padding: 0.6em; text-align: center; background: #fafafa;">0.7%</td>
<td style="padding: 0.6em; text-align: center; background: #fafafa;">0.07</td>
<td style="padding: 0.6em; text-align: center; background: #fafafa; color: #b00;">-29.3%</td>
<td style="padding: 0.6em; text-align: center; background: #e8f5e9;">3.8%</td>
<td style="padding: 0.6em; text-align: center; background: #e8f5e9;">0.42</td>
<td style="padding: 0.6em; text-align: center; background: #e8f5e9;">-24.3%</td>
</tr>
</tbody>
</table>
</div>
<p style="margin: 0.3em 0 1em 0; font-size: 0.9em; color: #888;">Table 2: Individual leg performance with Current vs Perfect volatility forecasts.</p>

<p>The long leg improves dramatically: +4% return, nearly double the Sharpe, half the drawdown. The short leg improves too, but it’s a smaller effect—it runs at lower leverage and benefits less from accurate position sizing.</p>

<h3 id="so-what">So What?</h3>

<p>The takeaway: perfect volatility forecasts on the long leg alone would boost Sharpe from 1.76 to 2.65. That’s huge. Obviously we can’t achieve perfect foresight, but it shows what’s at stake.</p>

<p>The rest of this post is about getting closer to that upper bound. Can we beat the simple rolling averages I currently use? Spoiler: yes. Global panel regressions with sector structure achieve 0.86 correlation with realized vol (vs 0.70 for moving averages). That should translate to meaningful alpha.</p>

<p><strong>What I test:</strong></p>
<ul>
  <li>Baselines: simple moving averages</li>
  <li>Per-asset regressions: one model per stock</li>
  <li>Per-sector: one model per sector</li>
  <li>Global: one model for all assets</li>
  <li>Global + sector dummies: shared dynamics, sector-specific levels</li>
</ul>

<p><strong>Structure:</strong></p>
<ol>
  <li>Data exploration</li>
  <li>Walk-forward methodology (no lookahead!)</li>
  <li>Step-by-step model comparison</li>
  <li>Robustness checks</li>
</ol>

<h2 id="data">Data</h2>

<p>Russell 1000 constituents, 1995–2024. After cleaning, that’s about 7 million date × asset observations. Roughly 1,000 stocks in the universe at any point.</p>

<h2 id="data-exploration">Data Exploration</h2>

<p>Before throwing regressions at the problem, let’s look at what we’re working with.</p>

<h3 id="volatility-distribution-by-sector">Volatility Distribution by Sector</h3>

<figure style="margin: 0.3em 0 1em 0; padding: 0; line-height: 1;">
<iframe src="/assets/vol_forecasting/sector_distribution.html" width="100%" height="820" frameborder="0" style="display: block; margin: 0; padding: 0; border: none; vertical-align: bottom;"></iframe>
<figcaption style="margin: 0.1em 0 0 0; padding: 0; font-size: 0.9em; line-height: 1.3; color: #888;">Figure 3: Volatility distribution by sector—box plots show median and quartiles, density curves show the full distribution.</figcaption>
</figure>

<p>Volatility varies a lot by sector. Median vol ranges from ~15% in Utilities to ~30% in Energy. Two things stand out:</p>

<ol>
  <li>Different sectors have structurally different volatility levels. This motivates adding sector dummies.</li>
  <li>The distribution is right-skewed (long right tail). This motivates testing log-space models.</li>
</ol>

<h3 id="feature-correlations">Feature Correlations</h3>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>Correlation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Composite volatility (5/21/63d avg)</td>
      <td>0.738</td>
    </tr>
    <tr>
      <td>EWM volatility (21d)</td>
      <td>0.715</td>
    </tr>
    <tr>
      <td>Rolling volatility (21d)</td>
      <td>0.696</td>
    </tr>
  </tbody>
</table>

<p style="margin: 0.3em 0 1em 0; font-size: 0.9em; color: #888;">Table 3: Feature correlations with 21-day forward volatility.</p>

<p>All volatility features are predictive. The composite (averaging multiple horizons) slightly outperforms single-horizon measures, motivating the use of multi-horizon features.</p>

<h2 id="walk-forward-methodology">Walk-Forward Methodology</h2>

<p>This is important: all predictions use strict day-by-day walk-forward testing. Train on the past 2 years, predict today. No lookahead bias—yesterday’s coefficients predict today.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training:    Day t-504 ──────────────────── Day t-1
Prediction:                                          Day t
</code></pre></div></div>

<p>For each date, I estimate coefficients on the past 504 days, then apply them to today’s features. This is powered by <a href="https://github.com/azmyrajab/polars_ols">polars-ols</a>—a fast, Rust-based OLS implementation for Polars:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">polars</span> <span class="k">as</span> <span class="n">pl</span>
<span class="kn">import</span> <span class="n">polars_ols</span>  <span class="c1"># Registers least_squares namespace
</span>
<span class="n">feature_cols</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">X_feature_vol_5</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">X_feature_vol_21</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">X_feature_vol_63</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">X_feature_vol_126</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># Step 1: Estimate rolling coefficients (global model)
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">with_columns</span><span class="p">(</span>
    <span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">y_target_vol_21</span><span class="sh">"</span><span class="p">)</span>
    <span class="p">.</span><span class="n">least_squares</span><span class="p">.</span><span class="nf">rolling_ols</span><span class="p">(</span>
        <span class="o">*</span><span class="p">[</span><span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">feature_cols</span><span class="p">],</span>
        <span class="n">window_size</span><span class="o">=</span><span class="mi">504</span><span class="p">,</span>      <span class="c1"># 2-year rolling window
</span>        <span class="n">min_periods</span><span class="o">=</span><span class="mi">252</span><span class="p">,</span>      <span class="c1"># Start after 1 year
</span>        <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">coefficients</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="p">.</span><span class="nf">over</span><span class="p">(</span><span class="n">order_by</span><span class="o">=</span><span class="sh">"</span><span class="s">date</span><span class="sh">"</span><span class="p">)</span>    <span class="c1"># Pool all assets (global model)
</span>    <span class="p">.</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">coefficients_raw</span><span class="sh">"</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Step 2: Lag coefficients by 1 day for out-of-sample predictions
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">with_columns</span><span class="p">(</span>
    <span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">coefficients_raw</span><span class="sh">"</span><span class="p">)</span>
    <span class="p">.</span><span class="nf">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>                 <span class="c1"># Use yesterday's coefficients
</span>    <span class="p">.</span><span class="nf">over</span><span class="p">(</span><span class="n">order_by</span><span class="o">=</span><span class="sh">"</span><span class="s">date</span><span class="sh">"</span><span class="p">)</span>
    <span class="p">.</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">coefficients_oos</span><span class="sh">"</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Step 3: Generate predictions from lagged coefficients
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">with_columns</span><span class="p">(</span>
    <span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">coefficients_oos</span><span class="sh">"</span><span class="p">)</span>
    <span class="p">.</span><span class="n">least_squares</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">feature_cols</span><span class="p">])</span>
    <span class="p">.</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">prediction</span><span class="sh">"</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">.shift(1)</code> in Step 2 is critical—it ensures predictions on day $t$ only use coefficients estimated through day $t-1$.</p>

<p><strong>What does <code class="language-plaintext highlighter-rouge">.predict()</code> do?</strong> The <code class="language-plaintext highlighter-rouge">coefficients_oos</code> column contains a struct with fitted coefficients (intercept + betas). The <code class="language-plaintext highlighter-rouge">.predict()</code> method computes:</p>

\[\hat{\sigma} = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4\]

<p>This is equivalent to writing it manually:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span>
    <span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">coefficients_oos</span><span class="sh">"</span><span class="p">).</span><span class="n">struct</span><span class="p">.</span><span class="nf">field</span><span class="p">(</span><span class="sh">"</span><span class="s">const</span><span class="sh">"</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">coefficients_oos</span><span class="sh">"</span><span class="p">).</span><span class="n">struct</span><span class="p">.</span><span class="nf">field</span><span class="p">(</span><span class="sh">"</span><span class="s">X_feature_vol_5</span><span class="sh">"</span><span class="p">)</span> <span class="o">*</span> <span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">X_feature_vol_5</span><span class="sh">"</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">coefficients_oos</span><span class="sh">"</span><span class="p">).</span><span class="n">struct</span><span class="p">.</span><span class="nf">field</span><span class="p">(</span><span class="sh">"</span><span class="s">X_feature_vol_21</span><span class="sh">"</span><span class="p">)</span> <span class="o">*</span> <span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">X_feature_vol_21</span><span class="sh">"</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">coefficients_oos</span><span class="sh">"</span><span class="p">).</span><span class="n">struct</span><span class="p">.</span><span class="nf">field</span><span class="p">(</span><span class="sh">"</span><span class="s">X_feature_vol_63</span><span class="sh">"</span><span class="p">)</span> <span class="o">*</span> <span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">X_feature_vol_63</span><span class="sh">"</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">coefficients_oos</span><span class="sh">"</span><span class="p">).</span><span class="n">struct</span><span class="p">.</span><span class="nf">field</span><span class="p">(</span><span class="sh">"</span><span class="s">X_feature_vol_126</span><span class="sh">"</span><span class="p">)</span> <span class="o">*</span> <span class="n">pl</span><span class="p">.</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">X_feature_vol_126</span><span class="sh">"</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">.predict()</code> shortcut keeps the code clean.</p>

<h2 id="target--evaluation">Target &amp; Evaluation</h2>

<h3 id="target-forward-realized-volatility">Target: Forward Realized Volatility</h3>

<p>The target is 21-day forward realized volatility:</p>

\[\sigma_{i,t}^{\text{fwd}} = \sqrt{\frac{252}{21} \sum_{k=1}^{21} r_{i,t+k}^2}\]

<p>where $r_{i,t}$ is the daily return of stock $i$ on day $t$. The factor $\sqrt{252}$ annualizes the volatility.</p>

<p>This target is observable and directly measurable—unlike GARCH’s latent conditional variance. Clean target, clean evaluation.</p>

<h3 id="evaluation-metrics">Evaluation Metrics</h3>

<p>I evaluate predictions using:</p>
<ul>
  <li><strong>RMSE:</strong> Root mean squared error</li>
  <li><strong>MAE:</strong> Mean absolute error</li>
  <li><strong>Correlation:</strong> Spearman rank correlation with realized volatility</li>
</ul>

<h2 id="feature-engineering">Feature Engineering</h2>

<p>Each feature is motivated by something we know about how volatility behaves.</p>

<h3 id="rolling-volatility">Rolling Volatility</h3>

<p>Volatility clusters. High-vol days follow high-vol days. This is the most robust finding in finance. Recent realized vol is the single best predictor of future vol.</p>

<p>For each stock, I compute rolling volatility at multiple horizons:</p>

\[\hat{\sigma}_{i,t}^{(w)} = \sqrt{\frac{252}{w} \sum_{k=0}^{w-1} r_{i,t-k}^2}\]

<p>with windows $w \in {5, 21, 63, 126}$ days:</p>
<ul>
  <li><strong>5-day:</strong> Captures very recent shocks (earnings, news)</li>
  <li><strong>21-day:</strong> Standard monthly volatility, balances noise and signal</li>
  <li><strong>63-day:</strong> Quarterly horizon, smooths out short-term noise</li>
  <li><strong>126-day:</strong> Semi-annual, provides mean-reversion anchor</li>
</ul>

<h3 id="asymmetric-features-why-not-egarch">Asymmetric Features (Why Not EGARCH?)</h3>

<p>Volatility responds asymmetrically to returns. Bad news increases vol more than good news reduces it—the “leverage effect.” EGARCH is the standard way to model this.</p>

<p>But EGARCH has problems for what we’re doing:</p>

<ol>
  <li>
    <p>No observable target. EGARCH models latent conditional variance. You can’t measure it directly—only infer it from the model. Our target (21-day realized vol) is directly observable.</p>
  </li>
  <li>
    <p>Can’t pool across assets. EGARCH is fit per asset. You can’t share parameters across stocks, so you’re back to the per-asset overfitting problem.</p>
  </li>
  <li>
    <p>Computationally expensive. MLE optimization for each stock, each day, in a rolling window? That’s millions of optimizations.</p>
  </li>
</ol>

<p>The alternative: I capture the leverage effect with explicit downside/upside volatility features:</p>

\[\hat{\sigma}_{i,t}^{\text{down}} = \sqrt{\frac{252}{w} \sum_{k=0}^{w-1} r_{i,t-k}^2 \cdot \mathbf{1}_{r_{i,t-k} &lt; 0}}\]

\[\hat{\sigma}_{i,t}^{\text{up}} = \sqrt{\frac{252}{w} \sum_{k=0}^{w-1} r_{i,t-k}^2 \cdot \mathbf{1}_{r_{i,t-k} \geq 0}}\]

<p>The downside volatility counts only negative return days; upside counts only positive days. If the leverage effect exists, the regression will learn a higher coefficient on downside vol than upside vol—and that’s exactly what we see in the coefficient heatmap.</p>

<p>This gives us:</p>
<ul>
  <li>Observable target (realized vol, not latent variance)</li>
  <li>Poolable (works in cross-sectional regression)</li>
  <li>Fast (just rolling sums, no optimization)</li>
  <li>Interpretable (downside coef &gt; upside coef = leverage effect)</li>
</ul>

<h3 id="sector-dummies">Sector Dummies</h3>

<p>Different sectors have structurally different vol levels. Utilities are stable. Energy is volatile. Tech sits somewhere in between. I add sector dummies (BICS Level 1) so the model can learn these level differences while sharing the dynamics across all assets.</p>

<h3 id="log-market-cap">Log Market Cap</h3>

<p>Small firms are more volatile than large firms. Less diversified revenue, higher leverage, lower liquidity, more idiosyncratic risk.</p>

<p>I include log market cap as a feature:</p>

\[\text{LogMktCap}_{i,t} = \log(\text{MarketCap}_{i,t})\]

<p>The log transformation handles the skewed distribution of market caps (a few mega-caps, many small-caps) and makes the relationship with volatility more linear.</p>

<h2 id="model-specification">Model Specification</h2>

<h3 id="the-regression">The Regression</h3>

<p>For each date $t$, I fit a cross-sectional Ridge regression:</p>

\[\sigma_{i,t}^{\text{fwd}} = \beta_0 + \sum_{j=1}^{J} \beta_j X_{i,t}^{(j)} + \sum_{s=1}^{S} \gamma_s D_{i,s} + \varepsilon_{i,t}\]

<p>where:</p>
<ul>
  <li>$X_{i,t}^{(j)}$ are volatility features (5d, 21d, 63d, 126d, downside, upside) and log market cap</li>
  <li>$D_{i,s}$ are sector dummies</li>
  <li>$\varepsilon_{i,t}$ is the error term</li>
</ul>

<h3 id="ridge-regularization">Ridge Regularization</h3>

<p>Volatility features at different horizons are highly correlated (~0.8+). OLS estimates become unstable. Ridge regression stabilizes them:</p>

\[\hat{\boldsymbol{\beta}} = \arg\min_{\boldsymbol{\beta}} \left\{ \sum_{i} \left( y_i - \mathbf{X}_i \boldsymbol{\beta} \right)^2 + \alpha \|\boldsymbol{\beta}\|_2^2 \right\}\]

<p>I use $\alpha = 1$. This shrinks coefficients toward zero without eliminating any feature.</p>

<h3 id="clipping--bounds">Clipping &amp; Bounds</h3>

<p>Volatility data can contain outliers—stocks that jump 10x during earnings or crash during liquidations. These extreme values corrupt coefficient estimates.</p>

<p><strong>Training:</strong> I clip volatility features and targets to [2.5%, 200%] during model training:</p>
<ul>
  <li>Values below 2.5% are unrealistic for daily-rebalanced portfolios (bid-ask spreads alone create this much noise)</li>
  <li>Values above 200% annualized volatility are tail events that shouldn’t drive regression weights</li>
  <li>Clipping during training prevents outliers from distorting coefficient estimates</li>
</ul>

<p><strong>Evaluation:</strong> All reported metrics (correlation, RMSE, MAE) are computed on <strong>raw, unclipped volatility</strong>. This gives an honest assessment of model performance on real data, including tail events.</p>

<p><strong>Predictions:</strong> I clip final predictions to [2%, 200%] for visualization in figures. This prevents nonsensical outputs (negative volatility, infinite leverage recommendations) and keeps plots readable. The metrics themselves are evaluated on raw volatility.</p>

<p><strong>Note:</strong> Performance on clipped volatility would be slightly higher (outliers are easier to predict when bounded), but the difference is small. I report raw metrics to be conservative.</p>

<h3 id="ensemble-subsampling">Ensemble Subsampling</h3>

<p>Instead of a single model, I run 3 models with offset estimation windows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model 1: trained on days 0, 3, 6, 9, ...
Model 2: trained on days 1, 4, 7, 10, ...
Model 3: trained on days 2, 5, 8, 11, ...
</code></pre></div></div>

<p>Final prediction is the average:</p>

\[\hat{\sigma}_{i,t}^{\text{ensemble}} = \frac{1}{3} \sum_{m=1}^{3} \hat{\sigma}_{i,t}^{(m)}\]

<p>Each model sees slightly different data. Averaging reduces variance.</p>

<h2 id="results-step-by-step-model-comparison">Results: Step-by-Step Model Comparison</h2>

<p>The central question: should we fit a model per asset, per sector, or globally? Let me build up the answer step by step.</p>

<h3 id="step-1-baselines">Step 1: Baselines</h3>

<p>What most practitioners use:</p>

<ul>
  <li>Composite average: equal-weighted average of 5, 21, and 63-day rolling vol</li>
  <li>Weighted blend: 70% × 21-day vol + 30% × 252-day vol (RiskMetrics-style)</li>
</ul>

<p>These achieve correlations around 0.70. Here’s what the predictions look like:</p>

<figure style="margin: 0.3em 0 1.5em 0; padding: 0; line-height: 1;">
<iframe src="/assets/vol_forecasting/residuals_baseline.html" width="100%" height="420" frameborder="0" style="display: block; margin: 0; padding: 0; border: none; vertical-align: bottom;"></iframe>
<figcaption style="margin: 0.1em 0 0 0; padding: 0; font-size: 0.9em; line-height: 1.3; color: #888;">Figure 4: Predicted vs actual volatility for the weighted blend baseline.</figcaption>
</figure>

<p>The predictions follow the diagonal but with significant scatter. Can we do better by learning the weights from data instead of fixing them?</p>

<h3 id="step-2-per-asset-regression">Step 2: Per-Asset Regression</h3>

<p>First attempt: fit a separate model for each stock. Each asset gets its own coefficients estimated on its own 2-year history.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Correlation</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline (weighted avg)</td>
      <td>0.70</td>
      <td>0.17</td>
    </tr>
    <tr>
      <td>Per-asset regression</td>
      <td>0.75</td>
      <td>0.15</td>
    </tr>
  </tbody>
</table>

<p style="margin: 0.3em 0 1em 0; font-size: 0.9em; color: #888;">Table 4: Per-asset regression vs baseline.</p>

<p>Correlation improves to 0.75. The regression learns better weights than fixed heuristics.</p>

<p>But there’s a problem: with only ~500 observations per stock in a 2-year window, coefficients are noisy. The model overfits to idiosyncratic patterns. Can we do better by sharing information?</p>

<h3 id="step-3-pooling-across-stocks">Step 3: Pooling Across Stocks</h3>

<p>Here’s the key insight: what if volatility dynamics are shared across assets? Mean reversion, clustering, the leverage effect—these should work similarly for Apple and Exxon.</p>

<p>I test two pooling levels:</p>

<ul>
  <li>Per-sector: one model per BICS sector (~700k obs/sector)</li>
  <li>Global: one model for all assets (~7M obs total)</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Correlation</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Per-asset</td>
      <td>0.75</td>
      <td>0.15</td>
    </tr>
    <tr>
      <td>Per-sector</td>
      <td>0.84</td>
      <td>0.13</td>
    </tr>
    <tr>
      <td>Global</td>
      <td>0.85</td>
      <td>0.13</td>
    </tr>
  </tbody>
</table>

<p style="margin: 0.3em 0 1em 0; font-size: 0.9em; color: #888;">Table 5: Impact of pooling across stocks.</p>

<p>Both jump to 0.84-0.85—a 10-point improvement over per-asset. Pooling works. Volatility dynamics are indeed shared across assets. Estimating them on more data reduces noise.</p>

<p>This is bias-variance tradeoff in action. Per-asset models: low bias (flexible), high variance (noisy). Global models: slightly higher bias (same coefficients for all), much lower variance (millions of observations).</p>

<h3 id="step-4-adding-sector-structure">Step 4: Adding Sector Structure</h3>

<p>Global pooling assumes all stocks share identical dynamics. But we saw that volatility <em>levels</em> differ by sector (15% for Utilities, 30% for Energy).</p>

<p>Can we get the best of both worlds? Shared coefficients for dynamics, sector-specific intercepts for levels?</p>

<p>Add sector dummies to the global model:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Correlation</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Global</td>
      <td>0.85</td>
      <td>0.130</td>
    </tr>
    <tr>
      <td>Global + sector dummies</td>
      <td>0.86</td>
      <td>0.124</td>
    </tr>
  </tbody>
</table>

<p style="margin: 0.3em 0 1em 0; font-size: 0.9em; color: #888;">Table 6: Adding sector structure.</p>

<p>A further boost to 0.86. The dummies let the model learn that Energy stocks have higher baseline vol than Utilities, while still sharing the dynamics across all assets.</p>

<p>Here’s what the best model looks like compared to the baseline:</p>

<figure style="margin: 0.3em 0 1.5em 0; padding: 0; line-height: 1;">
<iframe src="/assets/vol_forecasting/residuals_best.html" width="100%" height="420" frameborder="0" style="display: block; margin: 0; padding: 0; border: none; vertical-align: bottom;"></iframe>
<figcaption style="margin: 0.1em 0 0 0; padding: 0; font-size: 0.9em; line-height: 1.3; color: #888;">Figure 5: Predicted vs actual volatility for the global + dummies model.</figcaption>
</figure>

<p>The predictions are much tighter around the diagonal. The improvement is visible.</p>

<h3 id="step-5-log-space-models">Step 5: Log-Space Models</h3>

<p>The volatility distribution is right-skewed. Log-transforming should normalize the target. Does it help?</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Raw-Space</th>
      <th>Log-Space</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Global</td>
      <td>0.845</td>
      <td>0.849</td>
    </tr>
    <tr>
      <td>Global + dummies</td>
      <td>0.857</td>
      <td>0.860</td>
    </tr>
  </tbody>
</table>

<p style="margin: 0.3em 0 1em 0; font-size: 0.9em; color: #888;">Table 7: Raw-space vs log-space models.</p>

<p>Marginally better (~0.003). The difference is tiny. I stick with raw-space for interpretability—coefficients are directly in volatility units, easier to sanity-check.</p>

<h3 id="step-6-additional-risk-factors">Step 6: Additional Risk Factors</h3>

<p>Can we improve further with macro risk factors?</p>

<ul>
  <li>Market volatility factor: rolling 21-day vol of the market index</li>
  <li>Group Risk Factor (GRF): sector-level vol relative to long-run norm (AQR-style)</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Correlation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Global + dummies</td>
      <td>0.857</td>
    </tr>
    <tr>
      <td>+ market factor</td>
      <td>0.855</td>
    </tr>
    <tr>
      <td>+ GRF</td>
      <td>0.844</td>
    </tr>
  </tbody>
</table>

<p style="margin: 0.3em 0 1em 0; font-size: 0.9em; color: #888;">Table 8: Adding macro risk factors.</p>

<p>Neither helps. In fact, they slightly hurt. Why? The asset-level vol features already capture this. If the market is volatile, individual stocks are volatile, and our features pick that up. Adding redundant factors just adds noise.</p>

<h3 id="summary">Summary</h3>

<figure style="margin: 0.3em 0 1.5em 0; padding: 0; line-height: 1;">
<iframe src="/assets/vol_forecasting/metrics_comparison.html" width="100%" height="420" frameborder="0" style="display: block; margin: 0; padding: 0; border: none; vertical-align: bottom;"></iframe>
<figcaption style="margin: 0.1em 0 0 0; padding: 0; font-size: 0.9em; line-height: 1.3; color: #888;">Figure 6: Comparison of all model variants across RMSE, MAE, correlation, and MAPE metrics.</figcaption>
</figure>

<table>
  <thead>
    <tr>
      <th>Step</th>
      <th>Model</th>
      <th>Correlation</th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline</td>
      <td>Moving averages</td>
      <td>0.70</td>
      <td>0.17</td>
    </tr>
    <tr>
      <td>+Regression</td>
      <td>Per-asset</td>
      <td>0.75</td>
      <td>0.15</td>
    </tr>
    <tr>
      <td>+Pooling</td>
      <td>Global</td>
      <td>0.85</td>
      <td>0.13</td>
    </tr>
    <tr>
      <td>+Structure</td>
      <td>Global + dummies</td>
      <td>0.86</td>
      <td>0.124</td>
    </tr>
    <tr>
      <td>+Log-space</td>
      <td>Global + dummies (log)</td>
      <td>0.86</td>
      <td>0.124</td>
    </tr>
    <tr>
      <td>+Risk factors</td>
      <td>Global + dummies + market</td>
      <td>0.86</td>
      <td>0.125</td>
    </tr>
  </tbody>
</table>

<p>The progression is clear: learning weights beats heuristics, pooling beats per-asset, sector structure squeezes out the last bit of signal. Log-space and macro factors don’t help.</p>

<h2 id="robustness--validation">Robustness &amp; Validation</h2>

<h3 id="window-sensitivity">Window Sensitivity</h3>

<p>I swept rolling windows from 6 months to 10 years. How sensitive are results to this choice?</p>

<figure style="margin: 0.3em 0 1.5em 0; padding: 0; line-height: 1;">
<iframe src="/assets/vol_forecasting/window_trends.html" width="100%" height="420" frameborder="0" style="display: block; margin: 0; padding: 0; border: none; vertical-align: bottom;"></iframe>
<figcaption style="margin: 0.1em 0 0 0; padding: 0; font-size: 0.9em; line-height: 1.3; color: #888;">Figure 7: Model performance across different rolling window sizes (126 to 2520 days).</figcaption>
</figure>

<p>Performance is stable between 1–3 years. Very short windows are noisier; very long windows lag regime changes. I use 504 days (2 years).</p>

<h3 id="regime-robustness">Regime Robustness</h3>

<p>Do models hold up when volatility spikes? Here’s performance bucketed by market vol regime:</p>

<figure style="margin: 0.3em 0 1.5em 0; padding: 0; line-height: 1;">
<iframe src="/assets/vol_forecasting/regime_analysis.html" width="100%" height="420" frameborder="0" style="display: block; margin: 0; padding: 0; border: none; vertical-align: bottom;"></iframe>
<figcaption style="margin: 0.1em 0 0 0; padding: 0; font-size: 0.9em; line-height: 1.3; color: #888;">Figure 8: Correlation by market volatility regime—low (&lt;20%), neutral (20-30%), high (&gt;30%).</figcaption>
</figure>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Low (&lt;20%)</th>
      <th>Neutral (20-30%)</th>
      <th>High (&gt;30%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Global + dummies</td>
      <td>0.79</td>
      <td>0.83</td>
      <td>0.87</td>
    </tr>
    <tr>
      <td>Global + log dummies</td>
      <td>0.79</td>
      <td>0.83</td>
      <td>0.87</td>
    </tr>
    <tr>
      <td>Per-sector</td>
      <td>0.79</td>
      <td>0.82</td>
      <td>0.85</td>
    </tr>
    <tr>
      <td>Per-asset</td>
      <td>0.67</td>
      <td>0.69</td>
      <td>0.76</td>
    </tr>
    <tr>
      <td>Baselines</td>
      <td>0.61-0.62</td>
      <td>0.62-0.66</td>
      <td>0.68-0.70</td>
    </tr>
  </tbody>
</table>

<p>A few observations:</p>

<ol>
  <li>
    <p>All models improve in high-vol regimes. When vol is elevated, it’s more persistent and easier to predict. During calm periods, vol is compressed and harder to differentiate.</p>
  </li>
  <li>
    <p>Global models maintain their edge across all regimes. The 10+ point gap vs per-asset holds whether markets are calm or stressed.</p>
  </li>
  <li>
    <p>Per-asset models suffer most in low-vol regimes. With less signal, the noisy estimates hurt more. Global pooling provides stability when there’s less to work with.</p>
  </li>
</ol>

<h3 id="coefficient-interpretability">Coefficient Interpretability</h3>

<figure style="margin: 0.3em 0 1.5em 0; padding: 0; line-height: 1;">
<iframe src="/assets/vol_forecasting/coefficient_heatmap.html" width="100%" height="400" frameborder="0" style="display: block; margin: 0; padding: 0; border: none; vertical-align: bottom;"></iframe>
<figcaption style="margin: 0.1em 0 0 0; padding: 0; font-size: 0.9em; line-height: 1.3; color: #888;">Figure 9: Rolling regression coefficients over time. Red = predicts higher vol, Blue = predicts lower vol.</figcaption>
</figure>

<p>The heatmap reveals which features drive predictions.</p>

<p>Most important features:</p>

<ol>
  <li>126-day vol (~0.5): Strongest positive predictor. Long-term vol acts as the mean-reversion anchor.</li>
  <li>63-day vol (~0.3): Second strongest. Bridges short-term noise and long-term structure.</li>
  <li>5-day vol (negative!): Surprisingly negative. This is mean-reversion—when short-term vol spikes above long-term, the model predicts it will come down.</li>
  <li>Log market cap (~-0.1): Negative as expected. Smaller firms are more volatile.</li>
</ol>

<p>The sector dummies tell a story:</p>

<ul>
  <li>Energy: huge spike in 2020 (COVID oil crash, negative prices). Also elevated during 2008 and 2014-2016.</li>
  <li>Financials: spike in 2008-2009 (GFC). The model learned Financials needed a higher vol intercept during the crisis.</li>
  <li>Technology: elevated in early 2000s (dot-com bust), moderate since.</li>
  <li>Utilities: consistently low. Defensive stocks live up to their reputation.</li>
</ul>

<p>Asymmetric features (downside/upside vol) both have small negative coefficients. The leverage effect is present but subtle—most asymmetry is already captured by the level dynamics.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The final model: global Ridge regression with sector dummies, 2-year rolling window, 3-model ensemble.</p>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Choice</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Target</td>
      <td>21-day forward realized vol</td>
    </tr>
    <tr>
      <td>Features</td>
      <td>Rolling vol (5/21/63/126d), downside/upside vol, log market cap, sector dummies</td>
    </tr>
    <tr>
      <td>Model</td>
      <td>Ridge regression (α=1)</td>
    </tr>
    <tr>
      <td>Estimation</td>
      <td>2-year rolling window, 3-model ensemble</td>
    </tr>
    <tr>
      <td>Pooling</td>
      <td>Global (all assets)</td>
    </tr>
    <tr>
      <td>Implementation</td>
      <td>polars-ols for fast rolling regressions</td>
    </tr>
  </tbody>
</table>

<p>The key insight: volatility dynamics are shared across assets. Mean reversion, clustering, the leverage effect—they work the same way for Apple and Exxon. Per-asset models waste data chasing idiosyncratic noise. Global models with sector structure extract signal efficiently.</p>

<p>We started with the question: is it worth predicting volatility better? The answer is yes. Perfect foresight would boost Sharpe from 1.76 to 2.65. We can’t achieve perfect foresight, but improving correlation from 0.70 to 0.86 should capture a meaningful chunk of that improvement.</p>

<h2 id="whats-next">What’s Next</h2>

<p>ML models (LightGBM, neural nets). Can they beat these structured regressions? The linear model can’t capture interactions—for example, how 21-day vol might predict forward vol differently for Energy vs Utilities. Tree-based models can discover these automatically. That’s the next experiment.</p>

<h2 id="things-to-work-on">Things to Work On</h2>

<p>A few things I want to improve:</p>

<p>Heatmap interpretation. I sometimes get the interpretation wrong. The red/blue color coding and temporal patterns need clearer explanation. This is on my list.</p>

<p>Computational performance. <a href="https://github.com/azmyrajab/polars_ols">polars-ols</a> is <em>incredibly</em> fast. Rolling regression every day—refitting on a 504-day window, shifting coefficients, refitting again. Millions of regressions across thousands of assets over 20+ years. Full model run takes ~30 seconds. The Rust-based implementation makes this feasible.</p>

<p>EGARCH testing. I mentioned EGARCH is slow, but I haven’t actually benchmarked the <code class="language-plaintext highlighter-rouge">arch</code> package myself. If someone tells me it’s worth testing, I’ll give it a shot. I don’t know everything.</p>]]></content><author><name>anon</name></author><category term="Quants" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Reducing Rebalancing Timing Risk with Tranching</title><link href="http://localhost:4006/quants/2025/05/10/rebalancing-luck.html" rel="alternate" type="text/html" title="Reducing Rebalancing Timing Risk with Tranching" /><published>2025-05-10T00:00:00+02:00</published><updated>2025-05-10T00:00:00+02:00</updated><id>http://localhost:4006/quants/2025/05/10/rebalancing%20luck</id><content type="html" xml:base="http://localhost:4006/quants/2025/05/10/rebalancing-luck.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Rebalance timing plays a big role in medium- to long-term strategies that don’t trade often. Shifting the execution day by just a few days can meaningfully impact performance, changing the exact positions held, and affecting returns, volatility, and drawdowns.</p>

<p>This issue is often referred to as <em>rebalance timing luck</em>: the variation in performance that comes purely from the day you happen to rebalance, even when the strategy and signals stay the same.</p>

<p>Others have explored this too. <a href="https://www.thinknewfound.com/rebalance-timing-luck">Newfound Research</a> wrote a great piece highlighting how impactful this effect can be. And the <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5230603">Concretum Group</a> analyzed it in a GTAA context, showing that spreading rebalances out over time, instead of doing it all at once, can reduce timing risk and even lower turnover.</p>

<p>That got me curious: how much does timing luck affect my own long-short strategy? And could a more gradual rebalancing approach make it more stable, without changing the core logic?</p>

<h2 id="setup">Setup</h2>

<p>The strategy is a long-short market-neutral model based on LightGBM predictions (see <a href="https://piinghel.github.io/quants/2025/02/20/lgbm.html">this earlier post</a>). It rebalances every three weeks, with the entire portfolio updated on a fixed weekday at the closing price. Since there are three possible starting weeks (offsets) and five weekdays, that gives us 15 possible rebalancing schedules.</p>

<p>I simulate all 15 combinations:</p>
<ul>
  <li>3 offsets × 5 weekdays = 15 setups</li>
  <li>Each uses the same trained model and applies predictions available on that specific day</li>
  <li>Signals, logic, and universe stay exactly the same</li>
</ul>

<p>Then, I compare this to a <em>tranching</em> setup, a common approach where the portfolio is divided into parts and rebalanced gradually. Here, the portfolio is split into three equal tranches. One-third is rebalanced each week, rotating through the offsets. Over three weeks, the full portfolio is still updated, just in smaller, smoother steps. This helps reduce sensitivity to any single day’s noise.</p>

<h2 id="rebalancing-day-does-matter">Rebalancing Day Does Matter</h2>

<p>Here’s the cumulative return of the 15 full-rebalance variants:</p>

<p><img src="/assets/tranching/all_perf_plots.png" alt="Figure 1" /></p>

<p><strong>Figure 1:</strong> Cumulative returns for each rebalancing schedule (15 variants).</p>

<p>And the table of results:</p>

<table>
  <thead>
    <tr>
      <th>Tranche</th>
      <th>Geometric Return</th>
      <th>Volatility</th>
      <th>Sharpe Ratio</th>
      <th>Max Drawdown</th>
      <th>Time Underwater</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>offset=0, day=1</td>
      <td>10.38%</td>
      <td>6.58%</td>
      <td>1.53</td>
      <td>12.41%</td>
      <td>548 days</td>
    </tr>
    <tr>
      <td>offset=0, day=2</td>
      <td>10.10%</td>
      <td>6.54%</td>
      <td>1.50</td>
      <td>14.77%</td>
      <td>573 days</td>
    </tr>
    <tr>
      <td>offset=0, day=3</td>
      <td>10.61%</td>
      <td>6.54%</td>
      <td>1.57</td>
      <td>13.79%</td>
      <td>582 days</td>
    </tr>
    <tr>
      <td>offset=0, day=4</td>
      <td>12.10%</td>
      <td>6.73%</td>
      <td>1.73</td>
      <td>13.19%</td>
      <td>548 days</td>
    </tr>
    <tr>
      <td>offset=0, day=5</td>
      <td>12.17%</td>
      <td>6.91%</td>
      <td>1.70</td>
      <td>13.59%</td>
      <td>369 days</td>
    </tr>
    <tr>
      <td>offset=1, day=1</td>
      <td>12.53%</td>
      <td>6.90%</td>
      <td>1.75</td>
      <td>12.35%</td>
      <td>360 days</td>
    </tr>
    <tr>
      <td>offset=1, day=2</td>
      <td>12.21%</td>
      <td>6.97%</td>
      <td>1.69</td>
      <td>12.13%</td>
      <td>387 days</td>
    </tr>
    <tr>
      <td>offset=1, day=3</td>
      <td>12.30%</td>
      <td>6.87%</td>
      <td>1.72</td>
      <td>10.70%</td>
      <td>375 days</td>
    </tr>
    <tr>
      <td>offset=1, day=4</td>
      <td>11.64%</td>
      <td>6.70%</td>
      <td>1.68</td>
      <td>12.72%</td>
      <td>360 days</td>
    </tr>
    <tr>
      <td>offset=1, day=5</td>
      <td>11.62%</td>
      <td>6.76%</td>
      <td>1.66</td>
      <td>14.76%</td>
      <td>298 days</td>
    </tr>
    <tr>
      <td>offset=2, day=1</td>
      <td>10.90%</td>
      <td>6.50%</td>
      <td>1.67</td>
      <td>11.38%</td>
      <td>248 days</td>
    </tr>
    <tr>
      <td>offset=2, day=2</td>
      <td>11.27%</td>
      <td>6.60%</td>
      <td>1.56</td>
      <td>12.56%</td>
      <td>331 days</td>
    </tr>
    <tr>
      <td>offset=2, day=3</td>
      <td>10.61%</td>
      <td>6.60%</td>
      <td>1.63</td>
      <td>11.01%</td>
      <td>275 days</td>
    </tr>
    <tr>
      <td>offset=2, day=4</td>
      <td>10.99%</td>
      <td>6.53%</td>
      <td>1.63</td>
      <td>13.10%</td>
      <td>352 days</td>
    </tr>
    <tr>
      <td>offset=2, day=5</td>
      <td>10.37%</td>
      <td>6.45%</td>
      <td>1.56</td>
      <td>13.10%</td>
      <td>593 days</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Performance metrics for each full,rebalance variant.</p>

<p>I was actually surprised by how much divergence there was. Even though all variants follow the same model, returns varied quite a bit, from around 10.1% to 12.5% annually. Sharpe ratios ranged from 1.50 to 1.75, and some variants spent hundreds of days longer in drawdown than others. These are meaningful differences for something as simple as shifting the rebalance day.</p>

<p>None of this comes from the model or the signal—it’s purely due to small shifts in rebalance timing that compound over time. Even when overall performance looks stable, this kind of noise makes it harder to tell whether a change is genuinely better or just lucky on timing.</p>

<h2 id="tranching-a-simple-effective-fix">Tranching: A Simple, Effective Fix</h2>

<p>Instead of picking a single rebalancing day, we can average across them.</p>

<p>In the tranching setup, I split the portfolio into three equal parts. Each week, I rebalance one-third of the portfolio, always on the same weekday (say, every Wednesday), but each tranche follows a different offset within the three-week cycle. So over three weeks, the full portfolio is refreshed, just not all at once.</p>

<p>This staggered execution spreads risk more evenly across time without changing the model or signals.</p>

<p>To analyze the results, I group performance by weekday and average across the three offsets. This gives a cleaner comparison to the previous “all-at-once” setup.</p>

<p><img src="/assets/tranching/tranched_perf_plots.png" alt="Figure 2" /><br />
<strong>Figure 2:</strong> Cumulative returns with tranching by weekday (averaged over offsets).</p>

<table>
  <thead>
    <tr>
      <th>Weekday</th>
      <th>Geometric Return</th>
      <th>Volatility</th>
      <th>Sharpe Ratio</th>
      <th>Max Drawdown</th>
      <th>Time Underwater</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1 (Mon)</td>
      <td>11.32%</td>
      <td>6.08%</td>
      <td>1.80</td>
      <td>11.08%</td>
      <td>370 days</td>
    </tr>
    <tr>
      <td>2 (Tue)</td>
      <td>11.23%</td>
      <td>6.04%</td>
      <td>1.79</td>
      <td>11.63%</td>
      <td>301 days</td>
    </tr>
    <tr>
      <td>3 (Wed)</td>
      <td>11.24%</td>
      <td>6.08%</td>
      <td>1.78</td>
      <td>11.89%</td>
      <td>299 days</td>
    </tr>
    <tr>
      <td>4 (Thu)</td>
      <td>11.63%</td>
      <td>6.06%</td>
      <td>1.85</td>
      <td>11.01%</td>
      <td>303 days</td>
    </tr>
    <tr>
      <td>5 (Fri)</td>
      <td>11.43%</td>
      <td>6.11%</td>
      <td>1.80</td>
      <td>12.37%</td>
      <td>364 days</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 2:</strong> Tranche,averaged performance by weekday.</p>

<p>The improvements are actually quite solid. Sharpe ratios climb into the 1.78-1.85 range, with Friday and Thursday performing best. Volatility compresses to just above 6%, and drawdowns are generally smaller and recover more quickly. On the cumulative return chart, the dispersion tightens massively—the lines almost sit on top of each other, which wasn’t the case before.</p>

<p>This isn’t just about higher returns—most of the gains come from smoother execution. By spreading out rebalances, you avoid abrupt shifts in exposure, which reduces noise and leads to more stable performance.</p>

<p>From a research perspective, this makes a big difference. It cuts down the randomness introduced by timing luck and makes it easier to tell whether a model change is actually improving things, or just benefiting from better timing.</p>

<h2 id="why-it-works">Why It Works</h2>

<p>Tranching softens the impact of short-term shocks. A single full-book rebalance can pick up temporary price dislocations or liquidity gaps, injecting noise into an otherwise stable strategy. By spreading rebalancing over time, those effects average out.</p>

<p>The logic and signal stay exactly the same. The full portfolio still turns over every three weeks, just more gradually. That reduces sensitivity to any one day’s conditions, without changing overall turnover.</p>

<p>There’s also a subtle benefit on the signal side. In the standard setup, you only act on the model’s predictions once every three weeks. With tranching, you fold in new predictions each week, even if it’s only on a third of the capital. That introduces a kind of temporal diversification and makes the strategy more responsive to new information.</p>

<h2 id="a-few-trade-offs-and-extra-benefits">A Few Trade-Offs and Extra Benefits</h2>

<p>Tranching isn’t free. Weekly rebalancing means running the pipeline more often, sending more orders, and keeping an eye on execution regularly. That adds a bit of overhead, especially when some trades are small and barely move the portfolio.</p>

<p>But in practice, it’s felt worth it. Not just in research—even in live runs, it makes the strategy feel more stable and less twitchy around timing noise.</p>

<p>I also came across something interesting in the GTAA literature: tranching might reduce turnover and transaction costs. I’m not entirely sure why—maybe it avoids large shifts or cancels out trades across tranches. Still figuring that part out. If you’ve got ideas or experience with it, let me know—I’m curious.</p>

<p>And a small bonus: by updating just a slice each week, you keep injecting fresh signal without going all-in. The portfolio stays a bit more responsive without overtrading.</p>

<h2 id="conclusion">Conclusion</h2>

<p>What started as a small curiosity around rebalancing schedules turned into a strong case for using tranching by default.</p>

<p>The signal and logic don’t change, but the results become cleaner and more stable. That’s especially helpful when evaluating model tweaks, since it reduces noise from execution timing.</p>

<p>I’ll be using this setup going forward for any strategy that rebalances on a fixed cycle. It’s simple, effective, and makes the whole process easier to trust.</p>]]></content><author><name>anon</name></author><category term="Quants" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Good Reads</title><link href="http://localhost:4006/quants/2025/04/20/dope-articles.html" rel="alternate" type="text/html" title="Good Reads" /><published>2025-04-20T00:00:00+02:00</published><updated>2025-04-20T00:00:00+02:00</updated><id>http://localhost:4006/quants/2025/04/20/dope%20articles</id><content type="html" xml:base="http://localhost:4006/quants/2025/04/20/dope-articles.html"><![CDATA[<h2 id="roadmap">Roadmap</h2>

<p>This is just a running list of articles, papers, and blog posts I’ve found interesting — mostly around cross-sectional asset pricing, machine learning models, and quant investing. I’ll keep adding to it over time as I go deeper.</p>

<p>Some of this is academic, some more hands-on.</p>

<h3 id="machine-learning--asset-pricing">Machine Learning + Asset Pricing</h3>

<ul>
  <li><strong><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5103546">Artificial Intelligence Asset Pricing Models</a></strong><br />
<em>Bryan T. Kelly, Boris Kuznetsov, Semyon Malamud, Teng Andrea Xu (2025)</em>
    <ul>
      <li>Uses transformers to directly learn the stochastic discount factor from raw panel data. A step toward end-to-end portfolio optimization.</li>
    </ul>
  </li>
  <li><strong><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3751012">Building Cross-Sectional Systematic Strategies by Learning to Rank</a></strong><br />
<em>Daniel Poh, Bryan Lim, Stefan Zohren, Stephen Roberts (2021)</em>
    <ul>
      <li>Applies learning-to-rank techniques to directly optimize for cross-sectional ordering instead of predicting returns. Tightly aligned with how quant signals are actually used.</li>
    </ul>
  </li>
</ul>

<h3 id="return-predictability">Return Predictability</h3>

<ul>
  <li><strong><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=">Global Return Predictability</a></strong><br />
<em>Lasse Heje Pedersen</em>
    <ul>
      <li>Framework for understanding global return predictability across markets and asset classes.</li>
    </ul>
  </li>
</ul>

<h3 id="risk-management">Risk Management</h3>

<ul>
  <li><strong><a href="https://www.aqr.com/Insights/Research/Working-Paper/Risk-Everywhere-Modeling-and-Managing-Volatility">Risk Everywhere: Modeling and Managing Volatility</a></strong><br />
<em>Tim Bollerslev, Benjamin Hood, John Huss, Lasse Heje Pedersen</em>
    <ul>
      <li>Examines realized volatility patterns across 50+ commodities, currencies, equity indices, and fixed-income instruments. Uses panel-based estimation to achieve superior out-of-sample risk forecasts.</li>
    </ul>
  </li>
</ul>

<p>More coming soon — this is my scratchpad for everything worth revisiting.</p>]]></content><author><name>anon</name></author><category term="Quants" /><summary type="html"><![CDATA[Roadmap]]></summary></entry><entry><title type="html">Stock Ranking with Boosting</title><link href="http://localhost:4006/quants/2025/02/20/lgbm.html" rel="alternate" type="text/html" title="Stock Ranking with Boosting" /><published>2025-02-20T00:00:00+01:00</published><updated>2025-02-20T00:00:00+01:00</updated><id>http://localhost:4006/quants/2025/02/20/lgbm</id><content type="html" xml:base="http://localhost:4006/quants/2025/02/20/lgbm.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In my last two articles, I explored different ways to rank stocks and build long-short portfolios.</p>

<p>The first article explored a basic approach: ranking stocks by recent volatility and taking a position based on the idea that low-volatility stocks outperform higher volatility ones.</p>

<p>In the second article, I moved to a more flexible approach by combining multiple features (like momentum, volatility, size, and liquidity) using Ridge regression, a linear model that learns how to weigh these features. I also examined some important design choices, like whether to use ranking or z-scoring for feature normalization, choise of the target label, and whether to normalize the target label by sector.</p>

<p>In this article, I take it a step further by replacing the linear model with a boosted decision tree model (LightGBM). The setup stays exactly the same (same dataset, features, target, and allocation), so I can isolate the effect of using a non-linear model that captures more complex relationships.</p>

<p>I’ll then compare the results to both Ridge and the original low-volatility factor to see how performance evolves as we move from a simple factor to a linear model, and then to a non-linear one.</p>

<h2 id="recap-of-the-process">Recap of the Process</h2>

<p>In the first two articles (<a href="https://piinghel.github.io/quants/2025/01/14/lowvol.html"><em>The Low Volatility Factor: A Steady Approach</em></a> and <a href="https://piinghel.github.io/quants/2025/02/09/ridge.html"><em>Multi-Factor Stock Selection with Ridge Regression</em></a>), I explored two ways to rank stocks: a simple volatility-based factor, and a linear model (Ridge) combining multiple features. The setup stayed consistent throughout: Russell 1000 universe, point-in-time data, a 20-day forward Sharpe ratio as the target, and a risk-aware allocation model.</p>

<p>In this third article, I’m taking the next step by replacing Ridge with a boosted tree model (LightGBM). Everything else stays the same, so I can isolate the effect of switching to a non-linear model that captures interactions and more complex patterns.</p>

<p>I’ll compare results across all three approaches to see how performance evolves as we move from a simple factor to a linear model, and then to a non-linear one.</p>

<p>All features are normalized using cross-sectional ranking, turning each feature into a percentile rank between 0 and 1 on each day. Ranking has become my default because it makes features comparable across stocks and over time, and it avoids the impact of extreme outliers. I know z-scoring could sometimes be a better choice, and that’s something I want to revisit in the future, but for now I’m sticking with ranking.</p>

<p>Once I have model scores, I use a volatility-targeted allocation model to build long-short portfolios. Specifically, I go long the top 75 and short the bottom 75 stocks, adjusting each position based on volatility to avoid concentrating risk in a few names. This helps keep the portfolio more stable over time.</p>

<p>Portfolios are rebalanced every three weeks, which I’ve found strikes a good balance between adapting to new information and keeping turnover manageable. To reduce sensitivity to rebalance timing, I also stagger the portfolio into three tranches, a technique I explained in more detail in <a href="https://piinghel.github.io/quants/2025/05/10/rebalancing-luck.html"><em>Rebalancing Luck</em></a>.</p>

<h2 id="why-move-to-lightgbm">Why Move to LightGBM</h2>

<p>So far, the Ridge model combined multiple features in a linear way: it learned fixed weights for each feature, and that was it. But markets are rarely linear, and I suspect there are more complex patterns that Ridge simply can’t capture. For example, a feature like volatility might only matter when combined with a momentum signal, or extreme values of a feature might behave differently than moderate ones.</p>

<p>To address this, I’m moving to gradient boosting—a method that builds strong predictive models by combining many small decision trees. Gradient boosting works iteratively: each new tree tries to fix the mistakes of the previous ones, making it well suited to capture non-linear effects and feature interactions.</p>

<p>There are several well-known implementations of gradient boosting, like XGBoost, CatBoost, and LightGBM. I’m using LightGBM because it’s fast and efficient, especially when working with large datasets like mine. In practice, I don’t expect a big difference between these libraries, but LightGBM tends to be much quicker to train and predict, so it’s the obvious choice for this kind of project.</p>

<p>Another reason to choose gradient boosting is that these models are often among the best-performing approaches for tabular datasets, including many winning solutions in Kaggle competitions. So it makes sense to see whether this kind of model can push performance beyond what Ridge and the low-volatility factor can achieve.</p>

<p>With that in mind, I applied LightGBM to the same setup as before: ranking stocks in the Russell 1000, going long the top 75, short the bottom 75, and using volatility-based position sizing. This keeps everything consistent, so I can isolate the impact of the model itself.</p>

<h2 id="performance">Performance</h2>

<p>Let’s take a look at how LightGBM performs when applied to the same setup as before. The results are quite strong.</p>

<p><img src="/assets/lightgbm/perf_lgbm.png" alt="Figure 1" />
<strong>Figure 1:</strong> Performance of the LightGBM strategy, before and after transaction costs.</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Short Sleeve</th>
      <th>Long Sleeve</th>
      <th>L/S (No Fees)</th>
      <th>L/S (Fees)</th>
      <th>Russell 1000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Return (Annualized)</td>
      <td>1.46%</td>
      <td>12.48%</td>
      <td>12.33%</td>
      <td>10.69%</td>
      <td>7.29%</td>
    </tr>
    <tr>
      <td>Volatility (Annualized)</td>
      <td>9.95%</td>
      <td>10.54%</td>
      <td>6.67%</td>
      <td>6.69%</td>
      <td>19.58%</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.15</td>
      <td>1.18</td>
      <td>1.85</td>
      <td>1.60</td>
      <td>0.37</td>
    </tr>
    <tr>
      <td>Max Drawdown</td>
      <td>30.78%</td>
      <td>31.10%</td>
      <td>13.79%</td>
      <td>13.97%</td>
      <td>56.88%</td>
    </tr>
    <tr>
      <td>Max Time Underwater</td>
      <td>1,619 days</td>
      <td>599 days</td>
      <td>234 days</td>
      <td>265 days</td>
      <td>1,666 days</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Performance statistics of the LightGBM strategy, with and without transaction costs (5 basis points per trade).</p>

<p>After accounting for transaction costs, the strategy posts a 10.7% annualized return with volatility just under 7%, leading to a Sharpe ratio of 1.60. Drawdowns stay around 14%, and the strategy tends to bounce back relatively quickly after periods of underperformance.</p>

<p>What’s nice to see is that both sides of the book are doing their job. The long leg delivers a solid 12.5% return, well above the Russell 1000’s 7.3%. The short leg returns just 1.5%, meaning those bottom-ranked stocks are underperforming the market, as intended. That’s exactly the kind of clean separation you want in a ranking strategy.</p>

<p>That said, it’s worth noting the Russell 1000 isn’t a proper benchmark here (it’s long-only and far more volatile), but it’s still helpful as a rough point of reference since we’re trading within that universe.</p>

<p>Because both legs are volatility-targeted, the difference in returns translates cleanly into long-short performance, without relying on one side to carry all the weight. The result is a fairly smooth and balanced profile</p>

<h2 id="signal-quality">Signal Quality</h2>

<p>To get a feel for why LightGBM performs well, I like to zoom in on signal quality: how well each model ranks stocks by expected performance <em>before</em> any portfolio construction happens.</p>

<p>Each day, both Ridge and LightGBM assign a score to every stock $i$ in the universe. These scores are trained to predict a performance metric, specifically the Sharpe ratio over the next 20 trading days. All of this is done completely out-of-sample: the model has no access to future data when making predictions.</p>

<p>So for each day, I end up with a cross-section of predicted scores $\hat{y}_{i,t}$, along with realized outcomes $y_{i,t}$ based on forward-looking Sharpe ratios.</p>

<p>The low-volatility signal works differently. It’s just a simple rule: rank stocks based on their trailing volatility. No learning, no optimization, just a static heuristic.</p>

<p>To measure how well the predicted rankings align with actual performance, I compute the Spearman rank correlation between predicted and realized values:</p>

\[\rho_t = \text{Spearman} \left( \{ \hat{y}_{i,t} \}, \{ y_{i,t} \} \right)\]

<p>This gives a daily time series ${ \rho_t }$, from which I compute:</p>
<ul>
  <li>Mean correlation: how strong the signal is on average</li>
  <li>Standard deviation: how much it varies</li>
  <li>Sharpe ratio: how consistent it is over time (mean divided by standard deviation)</li>
</ul>

<p><img src="/assets/lightgbm/signal.png" alt="Figure 2" />
<strong>Figure 2:</strong> Cumulative Spearman correlation between model signals and future Sharpe ratio rankings, computed daily.</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Low Volatility</th>
      <th>Ridge Regression</th>
      <th>LightGBM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Mean</td>
      <td>4.96%</td>
      <td>5.14%</td>
      <td>5.43%</td>
    </tr>
    <tr>
      <td>Standard Deviation</td>
      <td>14.39%</td>
      <td>8.70%</td>
      <td>7.03%</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.34</td>
      <td>0.59</td>
      <td>0.77</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 2:</strong> Daily signal quality metrics based on Spearman correlation between predicted and realized Sharpe ratio rankings.</p>

<p>The results might seem modest (average correlations around 5%), but that’s expected when working with financial data. Predicting returns is noisy by nature. The key is stability. LightGBM delivers a more consistent signal than the other models, with less variability and a higher signal-level Sharpe ratio.</p>

<p>This kind of stability is valuable. Even weak signals, if reliable, can drive meaningful long-term performance when applied systematically.</p>

<h2 id="financial-performance">Financial Performance</h2>

<p><img src="/assets/lightgbm/perf_return_comp.png" alt="Figure 3" />
<strong>Figure 3:</strong> Performance over the full sample (1997–2024).</p>

<p><img src="/assets/lightgbm/perf_return_comp_last_10yr.png" alt="Figure 4" />
<strong>Figure 4:</strong> Performance over the last decade (2015–2024).</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Low Vol (Full)</th>
      <th>LR (Full)</th>
      <th>LGBM (Full)</th>
      <th>Low Vol (10Y)</th>
      <th>LR (10Y)</th>
      <th>LGBM (10Y)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Return (Ann. %)</td>
      <td>5.62%</td>
      <td>8.76%</td>
      <td>11.68%</td>
      <td>7.94%</td>
      <td>8.91%</td>
      <td>9.33%</td>
    </tr>
    <tr>
      <td>Volatility (Ann. %)</td>
      <td>8.67%</td>
      <td>7.98%</td>
      <td>7.09%</td>
      <td>9.76%</td>
      <td>8.91%</td>
      <td>8.17%</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.67</td>
      <td>1.09</td>
      <td>1.59</td>
      <td>0.83</td>
      <td>1.00</td>
      <td>1.13</td>
    </tr>
    <tr>
      <td>Max. Drawdown (%)</td>
      <td>35.27%</td>
      <td>20.18%</td>
      <td>13.71%</td>
      <td>12.11%</td>
      <td>17.07%</td>
      <td>11.69%</td>
    </tr>
    <tr>
      <td>Max. Time Under Water</td>
      <td>844 days</td>
      <td>862 days</td>
      <td>297 days</td>
      <td>350 days</td>
      <td>307 days</td>
      <td>297 days</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 3:</strong> Strategy performance over the full sample (1997–2024) and the last decade (2015–2024).</p>

<p>Figures 3 and 4, along with <span class="reference">Table 3</span>, show a clear pattern: LightGBM outperforms both Ridge and Low Vol across nearly every metric. It delivers higher returns and lower volatility, resulting in the strongest Sharpe ratios. Drawdowns are smaller, and recovery is faster, with time under water cut by more than half compared to the linear model.</p>

<p>That said, the last decade has been tougher. Performance has declined across the board, and while LightGBM still leads, the gap has narrowed. Its Sharpe ratio drops from 1.59 to 1.13, and Ridge isn’t far behind at 1.00. Returns are flatter, and the edge is less pronounced.</p>

<p>What’s driving this? Has the market become more efficient? Are the features decaying? Or is the model overfitting to an older regime? These are open questions I plan to explore in a future post.</p>

<p>For now, the takeaway is straightforward: LightGBM delivers more stable and stronger results than the simpler models. But like any strategy, its performance isn’t static, and understanding the conditions under which it holds up (or breaks down) is just as important.</p>

<h2 id="conclusion">Conclusion</h2>

<p>There’s clearly still a lot to improve. The model works, but performance has softened in the last decade, and it’s not obvious why. It could be changing regimes, feature decay, more competition… or all of the above.</p>

<p>Here’s what I’m exploring:</p>

<ul>
  <li>
    <p><strong>Portfolio construction</strong><br />
I’ve started digging into <em>Advanced Portfolio Management</em> and <em>The Elements of Quantitative Investing</em> by Paleologo, both really solid so far. I want to better understand what’s actually driving P&amp;L. Am I just picking up factor risk? Can I improve how I size positions or manage constraints?</p>
  </li>
  <li>
    <p><strong>Feature and target design</strong><br />
Right now, everything is based on price, volume, and market cap. There’s probably more signal out there, or better ways to use the existing data. Also thinking about prediction horizon: why stick to 20 days? What if I blend different horizons?</p>
  </li>
  <li>
    <p><strong>Rethinking the objective</strong><br />
The current pipeline scores stocks, then allocates separately. But maybe it makes more sense to predict portfolio weights directly. I’m especially intrigued by the recent paper <em>Artificial Intelligence Asset Pricing Models</em> (Kelly et al., 2025), which uses transformers to learn the stochastic discount factor from raw panel data. Super ambitious, but a really interesting direction.</p>
  </li>
</ul>

<p>Lots of cool stuff to explore, and I’ll share more as I go.</p>]]></content><author><name>anon</name></author><category term="Quants" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Multi-Factor Stock Selection with Ridge Regression</title><link href="http://localhost:4006/quants/2025/02/09/ridge.html" rel="alternate" type="text/html" title="Multi-Factor Stock Selection with Ridge Regression" /><published>2025-02-09T00:00:00+01:00</published><updated>2025-02-09T00:00:00+01:00</updated><id>http://localhost:4006/quants/2025/02/09/ridge</id><content type="html" xml:base="http://localhost:4006/quants/2025/02/09/ridge.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p><a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">In the last article</a> I showed how scaling returns by volatility helped improve performance with minimal complexity. This time I want to let the model learn how to combine different features instead of me setting the rules manually.</p>

<p>Stock prediction is hard. Instead of trying to guess exact returns, I’m focusing on something more practical: figuring out whether stock A will perform better than stock B. I don’t need to know by how much—just which one is likely to do better.</p>

<p>I’m using a multiple linear regression model that takes in many features (like price changes, trading volume, and risk levels) and learns how to weigh them. The model is trained on a target label—basically the outcome I want it to predict, like future returns or the Sharpe ratio.</p>

<p>But there are a few challenges. Features behave differently over time: their distributions shift, patterns that worked before can break down, and some signals that seem useful in one market environment might completely disappear in another. On top of that, the signal-to-noise ratio is low, making it tough to extract meaningful insights.</p>

<h2 id="data">Data</h2>

<p>I’m using the same dataset as <a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">the previous article</a>: daily price, volume, and market cap data for all Russell 1000 (RIY) constituents, covering about 3,300 stocks historically. I use point-in-time constituents and filter out stocks priced below $5 to keep things realistic.</p>

<h2 id="feature-engineering">Feature Engineering</h2>

<p>I come from a stats and CS background, so I naturally lean toward letting the data figure out relationships rather than imposing assumptions. This is different from traditional multifactor approaches where you manually decide how to combine features. Instead, I use a regression model that learns the feature weights directly from the data.</p>

<p>For now, I’m keeping it simple with linear regression—assuming relationships are linear and avoiding interaction terms. It’s a straightforward approach that focuses on finding direct connections between features and the target.</p>

<h2 id="choosing-predictive-features">Choosing Predictive Features</h2>

<p>Before the model can learn anything useful, I need to define the right features and pick a target variable. I focus on price, volume, market cap, and market-derived indicators, computing them daily for all 3,300 stocks in my universe. Here’s what I’m using:</p>

<p><strong>1. Momentum Features</strong><br />
These capture trend-following behavior.</p>
<ul>
  <li>Lagged returns over 1 to 10 days</li>
  <li>Rolling cumulative returns over 21 to 252 days</li>
  <li>MACD to detect momentum shifts</li>
</ul>

<p><strong>2. Volatility Features</strong><br />
These measure risk.</p>
<ul>
  <li>Rolling historical volatility over 21, 63, or 126 days</li>
  <li>Average True Range (ATR) to normalize price fluctuations</li>
</ul>

<p><strong>3. Liquidity Features</strong><br />
These assess trading activity.</p>
<ul>
  <li>Rolling mean and standard deviation of trading volume</li>
  <li>Ratio of current volume to its rolling maximum to spot unusual activity</li>
</ul>

<p><strong>4. Size Features</strong><br />
These measure company scale.</p>
<ul>
  <li>Rolling mean and minimum of market cap</li>
  <li>Helps distinguish small-cap from large-cap stocks</li>
</ul>

<p><strong>5. Short Mean Reversion Features</strong><br />
These identify when prices revert to historical norms.</p>
<ul>
  <li>Price deviation from its rolling moving average</li>
  <li>Position relative to rolling min and max values</li>
  <li>Bollinger Bands to spot overbought or oversold conditions</li>
</ul>

<p><strong>6. Correlation with the Market</strong><br />
These capture systematic risk.</p>
<ul>
  <li>Rolling correlation with the Russell 1000 over 63-day windows</li>
  <li>Helps separate defensive stocks from high-beta names</li>
</ul>

<p>In total, I’m working with around 150 features—obviously many are correlated, but that’s fine.</p>

<h2 id="target-variable">Target Variable</h2>

<p>The model is trained to predict return over the next 20 days and Sharpe ratio over the next 20 days. I could explore other time horizons, but I’m keeping it simple and focusing on 20 days for now.</p>

<h2 id="preprocessing-cross-sectional-normalization">Preprocessing: Cross-Sectional Normalization</h2>

<p>Cross-sectional normalization adjusts each feature relative to all other stocks on the same day. This makes the model focus on relative differences rather than absolute values, and it makes interpretation easier too.</p>

<p>By doing this, I make sure stocks are evaluated on a comparable basis at each point in time. This helps the model learn the relative order of stocks rather than absolute levels, and it prevents certain features from dominating the predictions.</p>

<h2 id="mathematical-formulation">Mathematical Formulation</h2>

<p>For a given feature $X^p$, the normalized value for stock $i$ at time $t$ is:</p>

\[X_{i,t}^{p,\text{norm}} = f\left(X_{i,t}^{p} X_{1:N,t}^{p}\right)\]

<p>where $X^p_{i,t}$ is the raw feature value for stock $i$ at time $t$, $X^p_{1:N,t}$ is the set of values for all stocks at time $t$ for feature $p$, and $f(\cdot)$ is the normalization method.</p>

<p>I’ll be comparing two methods: Z-scoring and ranking (sometimes called uniformization). Each has its own strengths and tradeoffs.</p>

<h2 id="z-scoring">Z-Scoring</h2>

<p>One common approach is z-scoring, which standardizes features by centering them around zero and scaling them to have a standard deviation of one:</p>

\[X_{i,t}^{p,\text{norm}} = \frac{X_{i,t}^{p}  \hat{\mu}^p_t}{\hat{\sigma}^p_t}\]

<p>where $\hat{\mu}^p_t$ is the mean across all stocks at time $t$ for feature $p$, and $\hat{\sigma}^p_t$ is the standard deviation.</p>

<p>Z-scoring keeps the relative magnitudes of the original values, so the model can distinguish between small and large variations. But it’s sensitive to extreme outliers, so I clip values beyond ±5 standard deviations.</p>

<h2 id="ranking-normalization">Ranking Normalization</h2>

<p>Another approach is ranking normalization, which transforms feature values into ranks and scales them between 0 and 1:</p>

\[R_{i,t}^{p} = \frac{r_{i,t}^{p}}{N}\]

<p>where $r^p_{i,t}$ is the rank of stock $i$ at time $t$ based on feature $p$ (0 for the lowest value, $N$ for the highest), and $R^p_{i,t}$ is the normalized rank.</p>

<p>Unlike z-scoring, ranking ensures the distribution stays the same over time. This makes it robust to extreme values but removes magnitude information—only relative positioning is preserved.</p>

<h2 id="visualizing-the-effect-of-normalization">Visualizing the Effect of Normalization</h2>

<p>Below in Figure 1 I summarize the different normalization methods applied to a single feature (20-day return). From left to right: the original distribution z-scored and ranked.</p>

<p><img src="/assets/ridge/example_normalization.png" alt="Figure 1" /></p>

<p><strong>Figure 1:</strong> Effect of normalization on 20-day return distribution. Left: Original data Middle: Z-scored Right: Ranked between 0 and 1.</p>

<h2 id="choosing-the-right-normalization-method">Choosing the Right Normalization Method</h2>

<p>How I normalize features has a big impact on how the model interprets stock differences. The choice between z-scoring and ranking depends on what I want the model to focus on.</p>

<ul>
  <li><strong>Z-scoring</strong> keeps magnitude differences intact, which helps when the strength of a signal matters. But it’s more sensitive to distribution shifts over time.</li>
  <li><strong>Ranking</strong> is more stable and removes extreme outliers since values are always mapped to a uniform distribution. But it also discards information about the magnitude of differences between stocks.</li>
</ul>

<p>Both methods make sure stocks are processed in a comparable way on any given day, but they emphasize different aspects of the data.</p>

<h2 id="evaluating-the-impact-of-normalization">Evaluating the Impact of Normalization</h2>

<p>To see if normalization improves results I compare three approaches:</p>

<ol>
  <li>Using raw unnormalized features</li>
  <li>Applying z-scoring across all stocks</li>
  <li>Using ranking across all stocks</li>
</ol>

<p>If normalization improves performance the next step is to refine how I apply it,especially to the target label.</p>

<h2 id="should-the-target-label-be-normalized-by-sector">Should the Target Label Be Normalized by Sector?</h2>

<p>Normalizing features ensures consistency over time, but what about the target label? Instead of normalizing returns across all stocks, I test whether normalizing them within each sector improves results while keeping all other features globally normalized.</p>

<ul>
  <li><strong>Global normalization</strong> applies the same normalization across the full stock universe</li>
  <li><strong>Sector-specific normalization</strong> adjusts returns within each sector while keeping all other features globally normalized</li>
</ul>

<p>My hypothesis is that sector-normalizing the target label might help by preventing cross-sector differences from distorting the model’s learning. Stocks in different industries often have structurally different return profiles, so this adjustment could make return comparisons more meaningful. Whether this actually improves performance is something I want to find out.</p>

<h2 id="handling-missing-data">Handling Missing Data</h2>

<p>Some models (like decision trees) handle missing data automatically, but others don’t. To keep things simple, I use:</p>

<ul>
  <li><strong>Forward fill:</strong> Use the last known value if past data exists</li>
  <li><strong>Cross-sectional mean imputation:</strong> If no past data is available, replace the missing value with the sector average for that day</li>
  <li><strong>Default values:</strong> For z-scoring, set missing values to 0. For ranking, set missing values to 0.5 (midpoint of the ranking scale)</li>
</ul>

<p>This approach is simple and works well for now.</p>

<h2 id="modeling-the-cross-sectional-normalized-score">Modeling the Cross-Sectional Normalized Score</h2>

<p>At the core of this strategy, I’m building a model to predict a stock’s cross-sectional normalized score—could be its Sharpe ratio, return, or another performance measure. I think of this as a function mapping available information at time $t$ to an expected score at $t+1$. To make stocks comparable, the score is normalized in the cross-section before modeling.</p>

<p>Mathematically, I assume there exists a function $g(\cdot)$ such that:</p>

\[s_{i,t+1} = g(\mathbf{z}_{i,t}) + \epsilon_{i,t+1}\]

<p>where $s_{i,t+1}$ is the true cross-sectional normalized score for stock $i$ at time $t+1$, $z_{i,t}$ is a vector of predictor variables for stock $i$ at time $t$, and $\epsilon_{i,t+1}$ is the error term.</p>

<p>The goal is to approximate $g(\cdot)$ using historical data. This function follows two key principles:</p>

<ol>
  <li>It leverages the entire panel of stocks—the same functional form applies universally</li>
  <li>It depends only on stock-specific features at time $t$. While some features contain past information (like return over the past 20 days), these are explicitly engineered rather than dynamically learned. The model doesn’t learn interactions between different stocks.</li>
</ol>

<h3 id="ridge-regression-as-a-baseline">Ridge Regression as a Baseline</h3>

<p>To estimate $g(\cdot)$, I use Ridge Regression—a simple but effective baseline, especially when predictors are highly correlated. It solves:</p>

\[\underset{\boldsymbol{\beta}}{\min} \frac{1}{n} \sum_{i=1}^n (s_{i,t+1}  \mathbf{x}_i^\top \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \beta_j^2\]

<p>where the second term $\lambda \sum_{j=1}^p \beta_j^2$ regularizes the coefficients to prevent instability.</p>

<p>Ridge is a good choice here because:</p>
<ul>
  <li>Stocks with similar characteristics often exhibit collinearity, and Ridge helps stabilize coefficient estimates</li>
  <li>The regularization term shrinks extreme values, reducing sensitivity to noise</li>
  <li>It provides a simple reference point before exploring more complex models</li>
</ul>

<p>The model is estimated using historical data, and to assess its effectiveness, I apply an expanding walk-forward validation, which I explain below.</p>

<h2 id="expanding-walk-forward-validation">Expanding Walk-Forward Validation</h2>

<p>To see how well the model holds up over time, I use an expanding walk-forward validation. The idea is simple:</p>

<ol>
  <li>Start with a 3-year burn-in period—the model isn’t tested yet; it just learns from the data</li>
  <li>Update the model every 2 years—each time, I add the latest data and refit the model</li>
  <li>Keep expanding the dataset—older data stays in, and new data gets added</li>
</ol>

<p>With stock data, I’ve always found that more historical data is better. The signal-to-noise ratio is low, so keeping as much information as possible helps the model find the signal in all the noise.</p>

<p>A rolling validation window could work, but it discards older data that might still be valuable. In my experience, an expanding window works better because it lets the model pick up long-term relationships, leading to more stable predictions.</p>

<p>For hyperparameter tuning, I could split the training set into separate train and validation sets. But honestly, I’ve never found this to be worth the extra time. Optimizing hyperparameters can take a while, and in most cases, default values that make sense are already a good starting point.</p>

<p>Below is a schematic of the expanding walkforward approach:</p>

<p><img src="/assets/ridge/walk-forward.png" alt="Figure 2" /></p>

<p><strong>Figure 2:</strong> Expanding walkforward validation process.</p>

<h2 id="portfolio-construction">Portfolio Construction</h2>

<p>Once I have stock rankings, I build a long-short portfolio:</p>

<ul>
  <li>I go long on the 75 stocks with the highest scores</li>
  <li>I short the 75 stocks with the lowest scores</li>
</ul>

<p>This approach is robust across different portfolio sizes—whether using 50, 100, or 150 stocks.</p>

<p>To keep risk under control, I use volatility targeting:</p>
<ul>
  <li>Higher-volatility stocks get smaller weights</li>
  <li>Lower-volatility stocks get larger weights</li>
</ul>

<p>This ensures the portfolio maintains a stable risk profile instead of being dominated by a few volatile names.</p>

<p>For more detail on my portfolio construction process, check out my <a href="https://piinghel.github.io/quant/2024/12/15/low-volatility,factor.html">previous article</a>.</p>

<h2 id="results">Results</h2>

<p>To evaluate different modeling choices I tested 10 model variations combining:</p>
<ul>
  <li>5 normalization methods: Raw Z-score (global &amp; sector) Ranking (global &amp; sector).</li>
  <li>2 target labels: Sharpe Ratio (SR 20) and Return (Return 20).</li>
  <li>A combined strategy (“Combo”) which equally weights all strategies.</li>
</ul>

<p>To ensure a fair comparison, all strategies are scaled to 10% volatility. The goal is to understand how normalization, sector adjustments, and target labels affect performance. Figure 3 shows cumulative returns across all strategies (without labels, to keep things interesting).</p>

<p>While all models deliver positive returns, there are clear differences in performance.</p>

<p><img src="/assets/ridge/all_lines.png" alt="Figure 3" /></p>

<p><strong>Figure 3:</strong> Cumulative returns of all strategies scaled to 10% volatility.</p>

<p>Figure 4 shows how different normalization methods, sector adjustments, and target label choices affect Sharpe ratios across the models. It compares z-scoring, raw features, and ranking, and also looks at the effect of normalizing within sectors versus globally. Plus it shows how using Sharpe ratios or raw returns as the target label changes things.</p>

<p>First off, normalization is pretty clear: z-scoring works best, then ranking, and raw features consistently underperform. I was honestly expecting ranking to be the top performer because it’s supposed to stabilize things, but it seems like it might also take out some useful signals. Z-scoring holds onto more of that valuable information, which is why it does so well. Raw features just add noise, so they end up being the weakest choice.</p>

<p>Then sector adjustment comes in and has an even bigger impact. Normalizing within sectors really improves Sharpe ratios compared to global normalization. It makes sense because comparing stocks within the same sector gives us more relevant context. By normalizing within sectors, I’m making sure that sector-wide noise doesn’t interfere with the real signals, so the rankings are more stable.</p>

<p>Lastly, the target label is by far the most important factor. When I use Sharpe ratios as the target, the models consistently perform better than when I use raw returns. This is no surprise—it’s easier to predict volatility than raw returns, so Sharpe ratios give more reliable risk-adjusted performance.</p>

<p>To sum it up: while normalization and sector adjustments matter, the key takeaway is the target label. Sharpe ratios beat raw returns every time, and sector normalization makes the rankings a lot stronger.</p>

<p><img src="/assets/ridge/summary_barplot.png" alt="Figure 4" /></p>

<p><strong>Figure 4:</strong> Sharpe Ratio performance across key modeling choices.</p>

<h3 id="normalization-effects-depend-on-the-target-label">Normalization Effects Depend on the Target Label</h3>

<p>Digging a bit deeper, Figure 5 shows the effect of normalization conditioned on the target label.</p>

<p>For Return 20 models, normalization had a smaller effect, but ranking and z-scoring still outperformed raw features. Interestingly, z-scoring has regained popularity in recent years.</p>

<p>For Sharpe Ratio models, the impact of normalization was stronger. Z-scoring was clearly the best performer, followed by ranking, then raw features.</p>

<p><img src="/assets/ridge/normalization_target.png" alt="Figure 5" /></p>

<p><strong>Figure 5:</strong> Cumulative return of different normalization methods conditioned on the target label. Volatility is set at 10% for all strategies.</p>

<h3 id="key-takeaways">Key Takeaways</h3>

<ul>
  <li>Normalization improves signal stability, helping models generalize better</li>
  <li>Sector-based adjustments on the target label refine comparisons, preventing large sector-specific biases</li>
  <li>Target label choice affects robustness—Sharpe ratio-based models perform better</li>
</ul>

<h2 id="the-combo-strategy-holds-up-well">The “Combo” Strategy Holds Up Well</h2>

<p>I was surprised to see that the “Combo” strategy performed really well, coming in second for Sharpe ratio (you can see it in Table 1). Instead of picking just one top model, it weights all strategies equally, and still ended up second overall.</p>

<p>Even without fine-tuning, blending multiple models helped smooth the performance and made it more stable. It’s pretty clear to me that diversifying across models can outperform just sticking with a single “best” model.</p>

<h2 id="full-performance-breakdown">Full Performance Breakdown</h2>

<p>To quantify these findings here’s the performance breakdown across models:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Return (ann.)</th>
      <th>Volatility (ann.)</th>
      <th>Sharpe Ratio</th>
      <th>Max Drawdown</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SR Z-Score By Sector</td>
      <td>12.24%</td>
      <td>7.94%</td>
      <td>1.54</td>
      <td>16.09%</td>
    </tr>
    <tr>
      <td>Combo</td>
      <td>8.99%</td>
      <td>6.62%</td>
      <td>1.36</td>
      <td>15.19%</td>
    </tr>
    <tr>
      <td>SR Z-Score Global</td>
      <td>10.68%</td>
      <td>8.30%</td>
      <td>1.29</td>
      <td>18.39%</td>
    </tr>
    <tr>
      <td>SR Ranking By Sector</td>
      <td>9.95%</td>
      <td>7.83%</td>
      <td>1.27</td>
      <td>13.94%</td>
    </tr>
    <tr>
      <td>SR Ranking Global</td>
      <td>10.40%</td>
      <td>8.41%</td>
      <td>1.24</td>
      <td>15.93%</td>
    </tr>
    <tr>
      <td>SR Raw Global</td>
      <td>9.76%</td>
      <td>8.39%</td>
      <td>1.16</td>
      <td>15.89%</td>
    </tr>
    <tr>
      <td>Return Ranking By Sector</td>
      <td>8.14%</td>
      <td>7.50%</td>
      <td>1.09</td>
      <td>15.96%</td>
    </tr>
    <tr>
      <td>Return Z-Score By Sector</td>
      <td>8.07%</td>
      <td>7.43%</td>
      <td>1.09</td>
      <td>19.37%</td>
    </tr>
    <tr>
      <td>Return Raw Global</td>
      <td>6.95%</td>
      <td>7.54%</td>
      <td>0.92</td>
      <td>18.99%</td>
    </tr>
    <tr>
      <td>Return Ranking Global</td>
      <td>6.48%</td>
      <td>7.26%</td>
      <td>0.89</td>
      <td>18.86%</td>
    </tr>
    <tr>
      <td>Return Z-Score Global</td>
      <td>6.44%</td>
      <td>7.67%</td>
      <td>0.84</td>
      <td>25.95%</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Performance metrics across different modeling choices ranked by Sharpe Ratio in descending order. Results exclude transaction costs and slippage.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Sector normalization (target label), normalization method, and target label choice all had a meaningful impact on performance:</p>

<ul>
  <li>Sector normalization was a game-changer—comparing stocks within their sector led to major improvements</li>
  <li>Normalization method mattered more than expected—z-scoring outperformed ranking, contradicting my initial intuition</li>
  <li>Sharpe ratio models consistently outperformed return-based models, reinforcing the importance of risk-adjusted metrics</li>
</ul>

<p>Instead of searching for a single best model, it may be smarter to combine perspectives. The “Combo” strategy showed that diversifying across models stabilizes results, even without fine-tuning.</p>]]></content><author><name>anon</name></author><category term="Quants" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">The Low Volatility Factor: A Steady Approach</title><link href="http://localhost:4006/quant/2024/12/15/low-volatility-factor.html" rel="alternate" type="text/html" title="The Low Volatility Factor: A Steady Approach" /><published>2024-12-15T00:00:00+01:00</published><updated>2024-12-15T00:00:00+01:00</updated><id>http://localhost:4006/quant/2024/12/15/low-volatility-factor</id><content type="html" xml:base="http://localhost:4006/quant/2024/12/15/low-volatility-factor.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Here’s a simple idea: stocks that move less tend to deliver better risk-adjusted returns than those with wild price swings. It’s a pattern that shows up in equities and other asset classes too.</p>

<p>This is the first in a series on cross-sectional stock selection. I’m starting with a single-factor strategy, then I’ll gradually build up: combining multiple signals with linear regression, testing different design choices, and later exploring non-linear models like LightGBM. At the end, I’ll compare everything to see if complexity actually helps. But first, let’s keep it simple and see how far a basic low-volatility sort can take us.</p>

<h2 id="tradeable-universe">Tradeable Universe</h2>

<p>I’m using the Russell 1000 (RIY), which tracks the largest U.S. stocks. To keep things realistic, I filter out stocks priced under $5. The data runs from 1995 to 2024, covering around 3,300 stocks as companies enter and exit the index. At any given time, about 1,000 stocks are tradeable. Since I’m using point-in-time constituents, there’s no survivorship bias. Figure 1 shows how many tradeable stocks there are over time.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/nr_stocks.svg" alt="Figure 1" /><br />
<strong>Figure 1:</strong> Number of tradeable stocks over time.</p>

<h2 id="measuring-volatility">Measuring Volatility</h2>

<p>To find low-volatility stocks, I compute the standard deviation of daily returns—basically measuring how much a stock’s price bounces around. I use three short-term rolling windows:</p>

<ul>
  <li>5 trading days</li>
  <li>10 trading days</li>
  <li>21 trading days</li>
</ul>

<p>Shorter windows react more quickly to changes in market conditions, while longer windows provide more stable estimates. By combining them, I aim for a volatility signal that’s both responsive and robust.</p>

<p>Volatility is computed as:</p>

\[\hat{\sigma}_{i,t} = \sqrt{\frac{1}{N,1} \sum_{j=1}^N \left( r_{i,t,j} , \bar{r}_{i,t} \right)^2}\]

<p>where $r_{i,t}$ is the daily return of stock $i$ at time $t$, and $N$ is the length of the rolling window. I annualize this by multiplying the daily volatility by $\sqrt{252}$, assuming 252 trading days in a year.</p>

<p>Across the dataset, the average annualized volatility is about 33%, with most stocks falling between 18% and 39%. The median is 26%. A few names have extreme swings, so I winsorize the volatility values at 5% and 200% to prevent outliers from distorting the rankings.</p>

<p>As shown in Figure 2, the distribution is right,skewed: most stocks cluster around moderate volatility levels, but a small number show very high fluctuations.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/distribution_volatilities.svg" alt="Figure 2" /><br />
<strong>Figure 2:</strong> Distribution of annualized volatility across all stocks.</p>

<h2 id="does-low-volatility-matter">Does Low Volatility Matter?</h2>

<p>I wanted to see if low-volatility stocks actually behave differently, so I looked at their returns over the next 10 trading days. I checked two things: raw returns and risk-adjusted returns (using the Sharpe ratio).</p>

<p>The correlation between volatility and raw return came out slightly positive, around 0.03. So more volatile stocks seemed to perform a bit better—at least at first glance. That wasn’t what I expected, but it’s a small effect. When I switched to Spearman correlation (which handles outliers better), it flattened out to zero.</p>

<p>Things change when I look at Sharpe ratios. Here, the correlation with volatility was negative: -0.035 with Pearson, and -0.04 with Spearman. So while high-vol stocks might deliver bigger returns sometimes, they do it with way more noise. On a risk-adjusted basis, they come out worse.</p>

<p>The signal is weak, but that’s normal for this stuff. What matters is that it shows up consistently, especially when you apply it across a large universe.</p>

<h2 id="sorting-stocks-into-portfolios">Sorting Stocks into Portfolios</h2>

<p>To turn this into a tradeable strategy, I rank stocks by volatility at each point in time and sort them into five portfolios. This ensures that portfolio assignments are always relative to the current market.</p>

<p>Here’s how it works:</p>
<ol>
  <li>Compute rolling volatility for each stock.</li>
  <li>Rank stocks by volatility within the universe.</li>
  <li>Normalize ranks to a 0,1 scale.</li>
  <li>Assign stocks to one of five portfolios based on percentile rank.</li>
</ol>

<p>Let $r_{i,t}$ be the cross-sectional rank of stock $i$ at time $t$, and $N$ be the number of stocks. The normalized rank is:</p>

\[\frac{r_{i,t}}{N}\]

<p>Stocks are then grouped into these buckets:</p>

<ul>
  <li>Portfolio 1: Lowest 10% of stocks ($0 \leq \text{Rank Score} &lt; 0.1$) → Low volatility</li>
  <li>Portfolio 2: 10% to 20% of stocks ($0.1 \leq \text{Rank Score} &lt; 0.2$)</li>
  <li>Portfolio 3: 20% to 80% ($0.2 \leq \text{Rank Score} &lt; 0.8$)</li>
  <li>Portfolio 4: 80% to 90% of stocks ($0.8 \leq \text{Rank Score} &lt; 0.9$)</li>
  <li>Portfolio 5: Highest 10% ($0.9 \leq \text{Rank Score} \leq 1.0$) → High volatility</li>
</ul>

<p>This way, every stock’s classification is determined relative to the cross-sectional volatility of the market at that time.</p>

<h2 id="portfolio-construction">Portfolio Construction</h2>

<p>Once the stocks are grouped into buckets, I construct two types of portfolios: one that assigns equal weights to each stock, and another that adjusts weights to target a specific volatility level.</p>

<h3 id="1-equalweighted-portfolio">1. Equal,Weighted Portfolio</h3>

<p>In the equal-weighted portfolio, each stock receives the same weight:</p>

\[w_{i,t} = \frac{1}{N_t}\]

<p>where \(N_t\) is the number of stocks in the portfolio at time \(t\). The portfolio remains fully invested. To create the long-short strategy, I go long the low-volatility portfolio (P1) and short the high-volatility portfolio (P5), using equal weights on both sides.</p>

<p>This setup introduces a problem: the two legs have different levels of volatility. Low,volatility stocks naturally exhibit less risk, so the short leg tends to dominate in terms of exposure. This imbalance reduces the effectiveness of the strategy on a risk-adjusted basis.</p>

<h3 id="2-volatilitytargeted-portfolio">2. Volatility,Targeted Portfolio</h3>

<p>To correct for this imbalance, I use volatility targeting. The idea is to scale the weight of each stock based on its volatility relative to a fixed target. The scaling factor is defined as:</p>

\[\alpha_{i,t} = \frac{\sigma_{\text{target}}}{\hat{\sigma}_{i,t}}\]

<p>where:</p>

<ul>
  <li>\(\sigma_{\text{target}} = 20\%\) is a fixed target volatility level</li>
  <li>\(\hat{\sigma}_{i,t}\) is the estimated future volatility of stock \(i\), computed using a 60-day rolling standard deviation</li>
</ul>

<p>Each stock’s weight becomes:</p>

\[w_{i,t} = \frac{1}{N_t} \cdot \alpha_{i,t}\]

<p>This approach increases the weight of stocks with below,target volatility and reduces the weight of those above it. To avoid excessive concentration, I cap individual weights at 4%. The total portfolio weight is also constrained to remain below or equal to 1.0.</p>

<p>During periods of high volatility, the portfolio may become partially invested to stay within the exposure limit. This isn’t market timing,it’s a way to control portfolio,level risk dynamically.</p>

<p>Portfolios are rebalanced weekly based on updated volatility estimates. Transaction costs are not included in this analysis.</p>

<h2 id="results">Results</h2>

<h3 id="equalweighted-portfolio">Equal,Weighted Portfolio</h3>

<p>I start by evaluating the equal-weighted version of the strategy. The long side holds the lowest,volatility stocks (P1), and the short side holds the highest,volatility ones (P5). Within each group, stocks are given equal weight.</p>

<p>On their own, the results make sense. The low-volatility portfolio delivers a geometric return of 11.8% with a Sharpe ratio of 1.0. The high-volatility portfolio returns 4.7%, with much higher volatility and little risk-adjusted performance.</p>

<p>But when combining the two into a long-short portfolio, the performance breaks down. The issue is a volatility mismatch: the short side is much more volatile than the long side,38.6% versus 12.2%. Equal weighting doesn’t account for this difference. As a result, the short leg ends up driving most of the portfolio’s risk.</p>

<p>The final long-short portfolio has high volatility (32.6%), a negative Sharpe ratio (-0.2), and a drawdown over 90%. Figure 3 shows the key performance metrics before applying volatility targeting. A full summary is included in Table 1.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/barplot_metrics_ew.png" alt="Figure 3" /></p>

<p><strong>Figure 3:</strong> Geometric Return, Volatility, and Sharpe Ratio for equal-weighted portfolios (before volatility targeting).</p>

<h3 id="volatilitytargeted-portfolio">Volatility,Targeted Portfolio</h3>

<p>Volatility targeting adjusts for the imbalance between long and short legs by scaling each stock’s weight relative to a fixed volatility target (set here at 20%). This helps align the total risk of the long and short portfolios and results in more stable performance.</p>

<p>After applying volatility targeting, the improvements are clear. The long-short portfolio volatility drops from 32.6% to 8.3%, and the Sharpe ratio rises from -0.2 to 0.9. The max drawdown is also cut significantly,from over 90% to 33.6%.</p>

<p>The long,only portfolio becomes slightly more stable as well. Its volatility decreases from 12.2% to 9.7%, while the Sharpe ratio improves from 1.0 to 1.1.</p>

<p>Figure 4 shows how volatility, return, and Sharpe ratio change across the five portfolios after targeting is applied. The result is more balanced exposure and better overall risk-adjusted returns.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/barplot_metrics_ew_vt.png" alt="Figure 4" /><br />
<strong>Figure 4:</strong> Geometric return, volatility, and Sharpe ratio after volatility targeting.</p>

<h3 id="performance-over-time">Performance Over Time</h3>

<p>The impact of volatility targeting is also visible over time. Figure 5 shows the net asset value of the long-short portfolio after adjustment. Compared to the equal-weighted version, returns are smoother and drawdowns less severe.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/perf_backtest_ew_vt.png" alt="Figure 5" /><br />
<strong>Figure 5:</strong> Net asset value of the volatility,targeted long-short portfolio.</p>

<h3 id="summary-table">Summary Table</h3>

<p>To summarize the improvements, here are the core metrics before and after applying volatility targeting:</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Long (No VT)</th>
      <th>Long (VT)</th>
      <th>Short (No VT)</th>
      <th>Short (VT)</th>
      <th>L/S (No VT)</th>
      <th>L/S (VT)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Geometric Return</td>
      <td>11.8%</td>
      <td>11.1%</td>
      <td>4.7%</td>
      <td>2.9%</td>
      <td>-5.3%</td>
      <td>7.7%</td>
    </tr>
    <tr>
      <td>Volatility</td>
      <td>12.2%</td>
      <td>9.7%</td>
      <td>38.6%</td>
      <td>9.7%</td>
      <td>32.6%</td>
      <td>8.3%</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>1.00</td>
      <td>1.14</td>
      <td>0.12</td>
      <td>0.30</td>
      <td>-0.19</td>
      <td>0.93</td>
    </tr>
    <tr>
      <td>Max Drawdown</td>
      <td>40.2%</td>
      <td>29.5%</td>
      <td>89.6%</td>
      <td>36.7%</td>
      <td>91.6%</td>
      <td>33.6%</td>
    </tr>
    <tr>
      <td>Max Time Underwater</td>
      <td>863 days</td>
      <td>612 days</td>
      <td>4,802 days</td>
      <td>1,647 days</td>
      <td>5,539 days</td>
      <td>944 days</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Performance metrics before and after volatility targeting. “VT” applies volatility targeting; “No VT” uses equal weighting.</p>

<h3 id="portfolio-exposure-over-time">Portfolio Exposure Over Time</h3>

<p>Figure 6 shows how total portfolio weights evolve over time for both the long (low-volatility) and short (high-volatility) sides after applying volatility targeting.</p>

<p>By total weight, I mean the sum of all individual stock weights within each leg. Since the short side holds more volatile stocks, it naturally receives less capital,typically between 0.2 and 0.6. The long side, made up of more stable names, stays closer to fully invested.</p>

<p>This behaviour is expected. Volatility targeting reduces exposure to riskier assets and increases it for more stable ones. During periods of market stress,like the dot,com crash, the financial crisis, or COVID,both legs reduce their exposure. The mechanism reacts by shrinking position sizes to stay within the target risk level.</p>

<p>The result is a portfolio that adapts to changing market conditions without needing explicit market timing.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/portfolio_weights_long_short_vol_target.svg" alt="Figure 6" /><br />
<strong>Figure 6:</strong> Total weight of all positions in the long (P1) and short (P5) portfolios after volatility targeting.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The low-volatility factor delivers stronger risk-adjusted returns. Lower,vol stocks tend to outperform, which goes against the usual link between higher risk and higher return.</p>

<p>Equal weighting in a long-short setup creates a risk imbalance. The short leg ends up dominating volatility, which pulls down performance. Volatility targeting fixes that imbalance by adjusting weights to align the risk on both sides, making the strategy more stable and improving Sharpe ratios.</p>

<p>Even this simple adjustment already helps a lot. In the next post, I’ll look at how combining multiple signals can take things a step further.</p>]]></content><author><name>anon</name></author><category term="Quant" /><summary type="html"><![CDATA[Introduction]]></summary></entry></feed>