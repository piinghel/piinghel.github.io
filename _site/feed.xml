<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-02-25T13:43:15+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Pieter-Jan</title><subtitle>Systematic trading and data science things.
</subtitle><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><entry><title type="html">Unleashing The Beast: LightGBM</title><link href="http://localhost:4000/quants/2025/02/20/beast.html" rel="alternate" type="text/html" title="Unleashing The Beast: LightGBM" /><published>2025-02-20T00:00:00+01:00</published><updated>2025-02-20T00:00:00+01:00</updated><id>http://localhost:4000/quants/2025/02/20/beast</id><content type="html" xml:base="http://localhost:4000/quants/2025/02/20/beast.html"><![CDATA[<h1 id="roadmap">Roadmap</h1>
<ul>
  <li>The idea to show performance of Lighgbm first using the same approach as the last article and then also compare it to the low vol and linear model and compare performance and show the impressive added value you get from including new linearities</li>
</ul>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[Roadmap The idea to show performance of Lighgbm first using the same approach as the last article and then also compare it to the low vol and linear model and compare performance and show the impressive added value you get from including new linearities]]></summary></entry><entry><title type="html">Ridge regression</title><link href="http://localhost:4000/quants/2025/02/09/ridge.html" rel="alternate" type="text/html" title="Ridge regression" /><published>2025-02-09T00:00:00+01:00</published><updated>2025-02-09T00:00:00+01:00</updated><id>http://localhost:4000/quants/2025/02/09/ridge</id><content type="html" xml:base="http://localhost:4000/quants/2025/02/09/ridge.html"><![CDATA[<p><a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">In the last article</a>, by scaling returns based on volatility, we improved performance with minimal complexity. But what if we could do better?</p>

<p>This time, we’ll predict stock rankings using a linear regression model that combines multiple features—like price changes, trading volume, and risk levels—and learns the optimal ranking based on a target label. A target label is simply the outcome we want the model to predict, such as stock returns or Sharpe ratio. By learning to predict this label, the model will rank the stocks accordingly.</p>

<p>Predicting stock rankings isn’t as simple as it seems. We’re dealing with features on different scales—price changes, trading volume, risk—and if we don’t normalize them, bigger features like market cap will overshadow smaller ones, like short-term returns. Additionally, the signal-to-noise ratio is quite low, meaning there’s a lot of random fluctuation in the data, which makes it harder for the model to find meaningful patterns. My intuition tells me normalizing these features will help the model learn a better ranking. I’m not totally sure yet, but that’s exactly what I’m testing today. If we get the normalization right, the model might perform better—but we’ll see.</p>

<p>The dataset stays the same as the one used in <a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">the previous article</a>: daily price, volume, and market capitalization data for all Russell 1000 (RIY) constituents, covering about 3,300 stocks historically. We use point-in-time constituents of the RIY and apply basic liquidity filters, excluding stocks priced below $5 to ensure realistic implementation.</p>

<h2 id="feature-engineering">Feature Engineering</h2>

<p>Coming from a statistical and computer science background, it makes sense to let the data figure out how the relationships should be learned, instead of imposing too many assumptions. This contrasts with the typical approach in multifactor portfolios, where the modeler decides how features should be combined. The advantage of using a regression model is that it learns the weights for how features should be combined. We’re sticking with linear regression for now, assuming the relationships are linear and no interaction terms. It’s a simple, data-driven approach that focuses on finding those direct, linear connections between features and the target.</p>

<p>Obviously, we need to define our predictive features and choose a target variable. We focus on price, volume, market capitalization, and market-derived features. Below is a breakdown of the main feature groups, computed daily for all 3,300 stocks in our universe.</p>

<ol>
  <li><strong>Momentum Features</strong>
    <ul>
      <li>Capture trend-following behavior.</li>
      <li>Examples:
        <ul>
          <li>Lagged returns over 1 to 10 days.</li>
          <li>Rolling cumulative returns over 21 to 252 days.</li>
          <li>Moving Average Convergence Divergence (MACD) to detect shifts in momentum.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Volatility Features</strong>
    <ul>
      <li>Measure risk.</li>
      <li>Examples:
        <ul>
          <li>Rolling historical volatility over 21, 63, or 126 days.</li>
          <li>Separate downside and upside volatility.</li>
          <li>Average True Range (ATR) to normalize price fluctuations.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Liquidity Features</strong>
    <ul>
      <li>Assess trading activity.</li>
      <li>Examples:
        <ul>
          <li>Rolling mean and standard deviation of trading volume.</li>
          <li>Ratio of current volume to its rolling maximum to highlight unusual trading activity.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Size Features</strong>
    <ul>
      <li>Measure company scale.</li>
      <li>Examples:
        <ul>
          <li>Rolling mean and minimum of market cap.</li>
          <li>Distinguishes small-cap from large-cap stocks.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Short Mean Reversion Features</strong>
    <ul>
      <li>Identify when prices revert to historical norms.</li>
      <li>Examples:
        <ul>
          <li>Price deviation from its rolling moving average.</li>
          <li>Position relative to rolling minimum and maximum values.</li>
          <li>Bollinger Bands to spot overbought or oversold conditions.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Correlation with the Market</strong>
    <ul>
      <li>Capture systematic risk.</li>
      <li>Examples:
        <ul>
          <li>Rolling correlation with the Russell 1000 over 63-day windows.</li>
          <li>Helps separate defensive stocks from high-beta names.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p>That leaves us with around 150 predictive features, some of which are obviously correlated.</p>

<h3 id="target-variable">Target Variable</h3>
<p>Now, what exactly are we trying to predict? We’ll focus on two targets:</p>

<ul>
  <li>Return over the next 20 days</li>
  <li>Sharpe ratio over the next 20 days</li>
</ul>

<p>Could we explore other horizons? Sure. But to keep things simple, we’ll stick to 20 days for now.</p>

<h2 id="preprocessing-cross-sectional-normalizaton">Preprocessing: Cross-Sectional Normalizaton</h2>

<p>To help the model learn more effectively, we apply cross-sectional normalization. This means adjusting each feature relative to all other stocks on the same day so the model can focus on relative differences rather than raw values.</p>

<p>Stock features vary widely—some, like market cap, are in the billions, while others, like daily returns, are much smaller. The model will still work without normalization, but when features are on vastly different scales, it can make it harder to learn relationships effectively.</p>

<p>Cross-sectional normalization helps with stationarity, stabilizing the data and ensuring the model focuses on how features relate to each other rather than their absolute values. This improves ranking consistency across time. To make this clearer, let’s introduce some mathematical notation.</p>

<p>For a given feature $X$, the normalized value for stock $i$ on time $t$ is:</p>

\[X^{\text{norm}}_{i,t} = f(X_{i,t}, X_{1:N,t})\]

<p>where:</p>
<ul>
  <li>$X_{i,t}$ is the raw feature value for stock $i$ at time $t$.</li>
  <li>$X_{1:N,t}$ is the set of values for all stocks at time $t$.</li>
  <li>$f(\cdot)$ is the chosen normalization method.</li>
</ul>

<p>Below, I discuss z-scoring and ranking, two common normalization techniques.</p>

<h3 id="1-z-scoring">1. Z-Scoring</h3>
<p>Z-scoring standardizes features to have a mean of 0 and a standard deviation of 1:</p>

\[X^{\text{norm}}_{i,t} = \frac{X_{i,t} - \mu_t}{\sigma_t}\]

<p>where:</p>
<ul>
  <li>
    <p>$\mu_t$ is the mean across all stocks at time $t$:</p>

\[\mu_t = \frac{1}{N} \sum_{i=1}^{N} X_{i,t}\]
  </li>
  <li>
    <p>$\sigma_t$ is the standard deviation:</p>

\[\sigma_t = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (X_{i,t} - \mu_t)^2}\]
  </li>
</ul>

<p>To prevent extreme values from distorting results, we clip any outliers beyond ±5 standard deviations.</p>

<h3 id="2-ranking-normalization">2. Ranking Normalization</h3>
<p>Instead of standardizing, we rank stocks and scale the ranks between 0 and 1:</p>

\[R_{i,t} = \frac{r_{i,t}}{N}\]

<p>where:</p>
<ul>
  <li>$r_{i,t}$ is the rank of stock $i$ at time $t$ (1 for the lowest value, $N$ for the highest).</li>
  <li>$R_{i,t}$ is the normalized rank.</li>
</ul>

<p>This approach is sometimes called uniformization, as it transforms a feature into a uniform distribution over $[0,1]$, ensuring all features are comparable while preserving their relative rankings.</p>

<h3 id="global-vs-sector-specific-normalization">Global vs. Sector-Specific Normalization</h3>

<p>Normalization can be applied in two ways:</p>

<ul>
  <li><strong>Global normalization</strong>: Features are normalized across the entire stock universe at each time step.</li>
  <li><strong>Sector-specific normalization</strong>: Features are normalized within each sector separately.</li>
</ul>

<p>Applying normalization within sectors prevents biases where certain sectors dominate due to large-scale differences (e.g., tech stocks having higher market caps than utilities). Sector-specific normalization ensures that features remain meaningful within each sector group while preserving their relative differences globally.</p>

<h3 id="does-normalization-actually-help">Does Normalization Actually Help?</h3>

<p>The model doesn’t need normalization to work—it will still produce rankings using raw features. But does normalization lead to better rankings? That’s what we’re testing.</p>

<p>We compare four approaches:</p>

<ol>
  <li>No normalization (raw features) – A baseline.</li>
  <li>Z-scoring (global) – Standardizes each feature across all stocks.</li>
  <li>Ranking (global) – Maps feature values to a uniform scale.</li>
  <li>Sector-specific normalization – Normalizes features within each sector instead of across the entire market.</li>
</ol>

<p>Below, in Figure 1, we show how different normalization methods affect a single feature (20-day return). From left to right: the original distribution, z-scored, and ranked.</p>

<p><img src="/assets/ridge/example_normalization.png" alt="Figure 1" /></p>

<p><strong>Figure 1</strong>: Effect of normalization on 20-day return distribution. Left: Original data, Middle: Z-scored, Right: Ranked between 0 and 1.</p>

<h3 id="handling-missing-data">Handling Missing Data</h3>

<p>Stock data isn’t always perfect—sometimes values are missing. The model can still run without them, but leaving gaps in the data isn’t ideal. So how do we fill them in?</p>

<p>We keep it simple:</p>

<ul>
  <li>Forward fill: If a stock has prior data, we use the last known value.</li>
  <li>Cross-sectional mean imputation: If no past data exists, we replace the missing value with the sector average for that day.</li>
  <li>Default values: If neither of the above works, we apply:
    <ul>
      <li>Z-scoring: Missing values are set to 0.</li>
      <li>Ranking: Missing values are set to 0.5 (midpoint of the ranking scale).</li>
    </ul>
  </li>
</ul>

<p>Could we get more sophisticated? Absolutely. There are plenty of ways to handle missing data, from advanced statistical methods to ML-based imputations. But for now, I’m sticking with a simple, reliable approach.  A deeper dive into missing data strategies is a topic for another time.</p>

<h2 id="modeling-the-cross-sectional-normalized-score">Modeling the Cross-Sectional Normalized Score</h2>

<p>At the core of this strategy, I’m building a model to predict a stock’s cross-sectional normalized score, which could be its Sharpe ratio, return, or another performance measure. I think of this as a function mapping available information at time $t$ to an expected score at $t+1$. To ensure comparability across stocks, the score is normalized in the cross-section before modeling.</p>

<p>Mathematically, I assume that there exists a function $g(\cdot)$ such that:</p>

\[s_{i,t+1} = g(\mathbf{z}_{i,t}) + \epsilon_{i,t+1}\]

<p>where:</p>
<ul>
  <li>$s_{i,t+1}$ is the true cross-sectional normalized score for stock $i$ at time $t+1$.</li>
  <li>$z_{i,t}$ is a vector of predictor variables for stock $i$ at time $t$.</li>
  <li>$\epsilon_{i,t+1}$ is the error term, representing what the model cannot predict.</li>
</ul>

<p>The objective is to approximate $g(\cdot)$ using historical data. This function follows two key principles:</p>

<ul>
  <li>it leverage the entire panel of stockd, meaning the same functional form applies universally.</li>
  <li>It depends only on stock-specific features at time $t$. While some features contain past information (such as return over the past 20 days), these are explicitly engineered rather than dynamically learned. In addition, the model does not learn interactions between different stocks.</li>
</ul>

<h3 id="ridge-regression-as-a-baseline">Ridge Regression as a Baseline</h3>

<p>To estimate $g(\cdot)$, I use Ridge Regression, a simple yet effective baseline, particularly when predictors are highly correlated. It solves the following optimization problem:</p>

\[\underset{\boldsymbol{\beta}}{\min} \frac{1}{n} \sum_{i=1}^n (s_{i,t+1} - \mathbf{x}_i^\top \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \beta_j^2\]

<p>where the second term, $\lambda \sum_{j=1}^p \beta_j^2$, regularizes the coefficients to prevent instability.</p>

<p>Ridge is a reasonable choice here because:</p>
<ul>
  <li>Stocks with similar characteristics often exhibit collinearity, and Ridge helps stabilize coefficient estimates.</li>
  <li>The regularization term shrinks extreme values, reducing sensitivity to noise.</li>
  <li>It provides a simple reference point before exploring more complex models.</li>
</ul>

<p>The model is estimated using historical data, and to assess its effectiveness, I apply an expanding walkforward validation which I explain just below in a bit more detail.</p>

<h2 id="expanding-walkforward-validation">Expanding Walkforward Validation</h2>

<p>To see how well the model holds up over time, I use an expanding walkforward validation. The idea is simple:</p>

<ol>
  <li>Start with a 3-year burn-in period – The model isn’t tested yet; it just learns from the data.</li>
  <li>Update the model every 2 years – Each time, I add the latest data and refit the model.</li>
  <li>Keep expanding the dataset – Older data stays in, and new data gets added.</li>
</ol>

<p>With stock data, I’ve always found that the more historical data, the better. The signal-to-noise ratio is low, so keeping as much information as possible helps stabilize the model.</p>

<p>A rolling validation window could be an alternative, but it discards older data that might still be valuable. In my experience, an expanding window works better because it allows the model to pick up long-term relationships, leading to more stable predictions.</p>

<p>For hyperparameter tuning, one option is to split the training set into separate train and validation sets. But honestly, I’ve never found this to be worth the extra time. Optimizing hyperparameters can take a while, and in most cases, default values that make sense are already a very good starting point.</p>

<p>Below is a schematic of the expanding walkforward approach:</p>

<p><img src="/assets/ridge/walk-forward.png" alt="Figure 2" /></p>

<p><strong>Figure 2</strong>: Expanding walkforward validation process.</p>

<h2 id="portfolio-construction">Portfolio Construction</h2>

<p>Once I have stock rankings, I build a long-short portfolio:</p>

<ul>
  <li>I go long on the 75 stocks with the highest scores.</li>
  <li>I short the 75 stocks with the lowest scores.</li>
</ul>

<p>The approach is robust across different portfolio sizes, whether using 50, 100, or 150 stocks.</p>

<p>To keep risk under control, I use volatility targeting:</p>
<ul>
  <li>Higher-volatility stocks get smaller weights.</li>
  <li>Lower-volatility stocks get larger weights.</li>
</ul>

<p>This ensures that the portfolio maintains a stable risk profile instead of being dominated by a few volatile names.</p>

<p>For a deeper dive into my portfolio construction process, check out my <a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">previous article</a> where I go into more detail.</p>

<h2 id="results">Results</h2>

<p>I tested different modeling choices, scaling all strategies to 8% volatility to keep things fair. The goal was to see how different ways of normalizing data, adjusting for sectors, and defining the target label would impact performance. To make it more interesting, I plotted cumulative performance without labels—just to add some suspense. While all models delivered positive returns, there’s a lot of variability in how they performed.</p>

<h3 id="why-normalization-matters">Why Normalization Matters</h3>

<p>Normalization plays a huge role. Interestingly, Z-scoring turned out to be the best performer, followed by ranking, while raw features performed the worst. This was unexpected—I initially had a strong bias towards ranking because it stabilizes things and helps with stationarity. However, it turns out that ranking might also remove some valuable signals. Meanwhile, raw features introduce too much noise, making them the weakest option.</p>

<p><img src="/assets/ridge/all_lines.png" alt="Figure 3" /></p>

<p><strong>Figure 3</strong>: Performance comparison of different modeling choices, all scaled to 8% volatility.</p>

<h3 id="the-power-of-sector-normalization">The Power of Sector Normalization</h3>

<p>Adjusting for sectors had an even bigger impact than I initially expected. Normalizing within sectors instead of across the entire universe significantly improved Sharpe Ratios. This reinforces the idea that comparing stocks within their peer groups, rather than against the whole market, leads to better rankings. Each sector has its own unique characteristics, and normalizing within sectors helps separate the true signal from sector-wide noise.</p>

<h3 id="the-impact-of-target-labels">The Impact of Target Labels</h3>

<p>The choice of target label also made a big difference. Training on the Sharpe Ratio delivered better results than training on raw returns, likely because risk-adjusted rankings provide a more stable signal and reduce noise.</p>

<p><img src="/assets/ridge/summary_barplot.png" alt="Figure 4" /></p>

<p><strong>Figure 4</strong>: Sharpe Ratio performance across key modeling choices.</p>

<h2 id="normalization-effects-depend-on-the-target-label">Normalization Effects Depend on the Target Label</h2>

<p>Looking at Figure 5, normalization interacts differently depending on whether the model is trained on the Sharpe Ratio or raw returns.</p>

<p>For models trained on 20-day returns, the differences between normalization methods were smaller, but ranking and Z-scoring still outperformed raw features. Interestingly, Z-scoring has made a strong comeback in recent years, which I didn’t initially expect.</p>

<p>For Sharpe Ratio models, all normalization methods worked reasonably well, but Z-scoring was clearly the best performer, followed by ranking, and then raw features.</p>

<p><img src="/assets/ridge/normalization_target.png" alt="Figure 5" /></p>

<p><strong>Figure 5</strong>: Cumulative return of different normalization methods, conditioned on the target label.</p>

<p>The main takeaway? Sector normalization, normalization method, and target label choice all matter, but they influence performance in different ways. Normalization improves signal stability, sector-based adjustments refine comparisons, and target label choice affects robustness.</p>

<h2 id="the-combo-strategy-holds-up-well">The “Combo” Strategy Holds Up Well</h2>

<p>One of the more surprising findings was how well the “Combo” strategy performed. Instead of relying on a single best model, this approach equally weighted all models—and still ranked second overall in terms of Sharpe Ratio. Even without fine-tuning, blending multiple signals led to smoother and more stable performance. This highlights an important insight: rather than obsessing over the best model, sometimes combining different perspectives leads to better results.</p>

<h2 id="full-performance-breakdown">Full Performance Breakdown</h2>

<p>To put some numbers behind these insights, here’s a breakdown of returns, volatility, and Sharpe Ratios across all models.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Return (ann. %)</th>
      <th>Volatility (ann. %)</th>
      <th>Sharpe Ratio (ann.)</th>
      <th>Max. Drawdown (%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SR Z-Score By Sector</td>
      <td>13.36</td>
      <td>7.87</td>
      <td>1.70</td>
      <td>11.58</td>
    </tr>
    <tr>
      <td>Combo</td>
      <td>9.98</td>
      <td>6.71</td>
      <td>1.49</td>
      <td>10.90</td>
    </tr>
    <tr>
      <td>SR Ranking By Sector</td>
      <td>10.68</td>
      <td>7.90</td>
      <td>1.35</td>
      <td>11.82</td>
    </tr>
    <tr>
      <td>Return Z-Score By Sector</td>
      <td>9.67</td>
      <td>7.38</td>
      <td>1.31</td>
      <td>15.37</td>
    </tr>
    <tr>
      <td>SR Z-Score Global</td>
      <td>10.68</td>
      <td>8.30</td>
      <td>1.29</td>
      <td>18.39</td>
    </tr>
    <tr>
      <td>SR Ranking Global</td>
      <td>10.40</td>
      <td>8.41</td>
      <td>1.24</td>
      <td>15.93</td>
    </tr>
    <tr>
      <td>Return Ranking By Sector</td>
      <td>9.31</td>
      <td>7.64</td>
      <td>1.22</td>
      <td>10.63</td>
    </tr>
    <tr>
      <td>SR Raw Global</td>
      <td>9.98</td>
      <td>8.39</td>
      <td>1.19</td>
      <td>15.49</td>
    </tr>
    <tr>
      <td>Return Raw Global</td>
      <td>8.42</td>
      <td>7.28</td>
      <td>1.16</td>
      <td>10.96</td>
    </tr>
    <tr>
      <td>Return Ranking Global</td>
      <td>8.54</td>
      <td>7.63</td>
      <td>1.12</td>
      <td>13.62</td>
    </tr>
    <tr>
      <td>Return Z-Score Global</td>
      <td>7.95</td>
      <td>7.45</td>
      <td>1.07</td>
      <td>21.23</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1</strong>: Performance metrics across different modeling choices, ranked by Sharpe Ratio. Results exclude transaction costs and slippage.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>Sector normalization, normalization method, and target label choice all had a meaningful impact on performance:</p>
<ul>
  <li><strong>Sector normalization</strong> was a bigger game-changer than I expected. Comparing stocks within their sector made a huge difference.</li>
  <li><strong>Normalization method</strong> played a major role. Z-scoring surprisingly outperformed ranking, which I initially thought would be stronger due to its stabilizing effect.</li>
  <li><strong>Target label choice</strong> still mattered, with Sharpe Ratio models generally outperforming those trained on raw returns.</li>
</ul>

<p>Instead of focusing too much on a single best model, combining different perspectives may be a smarter way to go. The “Combo” strategy showed that diversification across methodologies stabilized results, even without fine-tuning. Sometimes, having multiple approaches working together just makes everything more robust.</p>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[In the last article, by scaling returns based on volatility, we improved performance with minimal complexity. But what if we could do better?]]></summary></entry><entry><title type="html">The Low Volatility Factor: A Boring Way to Make Money?</title><link href="http://localhost:4000/quant/2024/12/15/low-volatility-factor.html" rel="alternate" type="text/html" title="The Low Volatility Factor: A Boring Way to Make Money?" /><published>2024-12-15T00:00:00+01:00</published><updated>2024-12-15T00:00:00+01:00</updated><id>http://localhost:4000/quant/2024/12/15/low-volatility-factor</id><content type="html" xml:base="http://localhost:4000/quant/2024/12/15/low-volatility-factor.html"><![CDATA[<h2 id="the-low-volatility-factor-a-steady-approach">The Low-Volatility Factor: A Steady Approach</h2>

<p>The <strong>low-volatility factor</strong> is a well-known idea in factor investing. It’s based on a simple observation: stocks that move <strong>more gradually</strong> tend to outperform their more volatile counterparts <strong>on a risk-adjusted basis</strong>.</p>

<p>But wait—shouldn’t higher risk mean higher returns? Not always. In this post, I’ll look at whether that assumption holds up.</p>

<p>I’ll walk through how to build a <strong>long-short portfolio</strong> using the <strong>low-volatility factor</strong> in the <strong>Russell 1000</strong>. We’ll also see how <strong>volatility targeting</strong> can help refine the strategy.</p>

<h2 id="tradeable-universe">Tradeable Universe</h2>

<p>First, let’s define the universe. I’m using the <strong>Russell 1000</strong>, which includes the <strong>largest U.S. stocks</strong>. To keep it realistic, I filter out those trading below <strong>$5</strong>, which are less liquid and more prone to erratic moves.</p>

<p>The dataset spans <strong>1995 to 2024</strong>, covering about <strong>3,300 stocks</strong> as companies come and go. At any given time, there are around <strong>1,000 tradeable stocks</strong>. Since I’m using <strong>point-in-time constituents</strong>, there’s no <strong>survivorship bias</strong>—we’re not just looking at the winners.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/nr_stocks.svg" alt="Figure 1" /></p>

<p><strong>Figure 1</strong>: Number of tradeable stocks over time.</p>

<h3 id="defining-the-low-volatility-factor">Defining the Low Volatility Factor</h3>

<p>The <strong>low-volatility factor</strong> focuses on stocks that move <strong>more gradually</strong> rather than making sharp swings. To measure this, I calculate volatility using three relatively short time windows:</p>

<ul>
  <li><strong>5-day volatility</strong></li>
  <li><strong>10-day volatility</strong></li>
  <li><strong>21-day volatility</strong></li>
</ul>

<p>These windows might seem short, but they help track <strong>recent price movements</strong> while staying reactive to shifts in volatility. Longer windows could smooth things out more, but they also take longer to adjust when market conditions change. A mix of short windows balances responsiveness with stability.</p>

<p>On average, <strong>volatility is 33%</strong>, with most stocks falling between <strong>18% and 39%</strong> and a median of <strong>26%</strong>. Since some stocks experience extreme price swings, I <strong>winsorized the data</strong>, capping the lower bound at <strong>5%</strong> and the upper bound at <strong>200%</strong>. This helps prevent outliers from distorting the analysis and also explains the <strong>small bump at the right tail</strong> of the distribution.</p>

<p>Here’s what the <strong>distribution of annualized volatilities</strong> looks like across <strong>3,300 stocks</strong> in the dataset. The shape is <strong>positively skewed</strong>, meaning most stocks have moderate volatility, but a few are far more erratic.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/distribution_volatilities.svg" alt="Figure 2" /></p>

<p><strong>Figure 2</strong>: Histogram showing the distribution of volatility across the tradeable universe.</p>

<h3 id="does-low-volatility-actually-matter">Does Low Volatility Actually Matter?</h3>

<p>To see if lower volatility has any predictive power, I checked its correlation with <strong>forward returns (next 10 days)</strong> and the <strong>Sharpe ratio</strong>.</p>

<ul>
  <li><strong>Returns</strong>: The Pearson correlation is <strong>0.03</strong>, meaning higher-volatility stocks tend to have slightly higher returns. But when switching to <strong>Spearman correlation</strong>, which is less sensitive to outliers, the relationship disappears (<strong>0.00</strong>).</li>
  <li><strong>Sharpe Ratio</strong>: The trend flips. Pearson shows a <strong>-0.035</strong> correlation, and Spearman strengthens that to <strong>-0.04</strong>—suggesting that more volatile stocks tend to have <strong>lower risk-adjusted returns</strong>.</li>
</ul>

<p>These numbers might seem small, but in finance, that’s expected. The <strong>signal-to-noise ratio is low</strong>, and even weak relationships can matter when applied systematically.</p>

<h2 id="portfolio-bucketing">Portfolio Bucketing</h2>

<p>To construct the portfolios, I performed a <strong>cross-sectional ranking</strong> of stocks based on their volatilities at each point in time. Specifically, I ranked all stocks in the universe by their volatility and mapped these ranks to a uniform <strong>0 to 1</strong> distribution. Then, I assigned each stock to one of five portfolios according to its percentile rank.</p>

<p>Let $r_{i,t}$ be the cross-sectional rank of stock $i$ at point $t$ based on its volatility, and $N$ be the total number of stocks at a given time. The normalized rank is computed as:</p>

\[\text{Rank Score}_i = \frac{r_{i,t}}{N}\]

<p>Using this score, stocks are bucketed into the following volatility-based portfolios:</p>

<ul>
  <li><strong>Portfolio 1</strong>: Lowest 10% of stocks ($ 0 \leq \text{Rank Score} &lt; 0.1 $) → <strong>Low volatility</strong></li>
  <li><strong>Portfolio 2</strong>: 10% to 20% of stocks ($ 0.1 \leq \text{Rank Score} &lt; 0.2 $)</li>
  <li><strong>Portfolio 3</strong>: 20% to 80% of stocks ($ 0.2 \leq \text{Rank Score} &lt; 0.8 $)</li>
  <li><strong>Portfolio 4</strong>: 80% to 90% of stocks ($ 0.8 \leq \text{Rank Score} &lt; 0.9 $)</li>
  <li><strong>Portfolio 5</strong>: Highest 10% of stocks ($ 0.9 \leq \text{Rank Score} \leq 1.0 $) → <strong>High volatility</strong></li>
</ul>

<p>This process ensures that each stock’s portfolio assignment is determined <strong>relative to the cross-sectional volatility distribution</strong> at that specific point.</p>

<h3 id="rebalancing">Rebalancing</h3>

<p>The portfolios are rebalanced <strong>weekly</strong> to reflect changes in volatility over time. For simplicity, I haven’t factored in transaction costs in this analysis.</p>

<h2 id="constructing-a-long-only-and-long-short-portfolio">Constructing a Long-Only and Long-Short Portfolio</h2>

<p>Once the stocks are bucketed, I created two types of portfolios: <strong>equal-weighted</strong> and <strong>volatility-targeted</strong>.</p>

<h3 id="equal-weighted-portfolio">Equal-Weighted Portfolio</h3>

<p>In the <strong>equal-weighted portfolio</strong>, all stocks are given equal weight, and I remain fully invested at all times. However, this creates a mismatch between the volatilities of the long and short positions, which I’ll explain further below. To construct the long-short portfolio, I simply take the difference between the low-volatility (P1) and high-volatility portfolios (P5).</p>

<h3 id="volatility-targeted-portfolio">Volatility-Targeted Portfolio</h3>

<p><strong>Volatility targeting</strong> adjusts stock weights based on their volatility to create a more stable portfolio. Here’s how it works:</p>

<ol>
  <li><strong>Compute the Volatility Scaling Factor</strong>: For each stock, calculate:<br />
\(\text{vol_ratio} = \frac{\sigma_{target}}{\hat{\sigma}_{i,t}}\)<br />
where:
    <ul>
      <li>$\sigma_{target} = 20\%$ is an arbitrary target, chosen because it’s close to the average stock volatility. This ensures that the overall portfolio volatility remains around <strong>8%</strong>.</li>
      <li>${\hat{\sigma}_{i,t}}$ represents the stock’s estimated future volatility, typically calculated using a rolling <strong>60-day standard deviation</strong>. More complex models could be used, but we’ll keep it simple for now.</li>
    </ul>
  </li>
  <li>
    <p><strong>Adjust Equal Weights</strong>: Multiply the equal weight of each stock by its $\text{vol_ratio}$:<br />
\(w_i = \text{equal_weight} \times \text{vol_ratio}\)</p>
  </li>
  <li>
    <p><strong>Cap Individual Weights</strong>: Ensure no stock weight exceeds <strong>4%</strong> to prevent excessive concentration. This cap is somewhat arbitrary and depends on the number of stocks in the portfolio and possibly other factors, such as sector diversification or liquidity constraints.</p>
  </li>
  <li><strong>Constrain Portfolio Exposure</strong>: Ensure that the total portfolio weight does not exceed <strong>1</strong> (i.e., fully invested). During periods of high volatility, the total weight may decrease to limit risk. While this may resemble <strong>market timing</strong>, I see it as <strong>dynamically adjusting risk</strong>—a common technique in <strong>trend-following</strong> strategies.</li>
</ol>

<p>The resulting portfolio balances the <strong>long</strong> and <strong>short</strong> legs by dynamically adjusting stock weights to achieve the target volatility.</p>

<h2 id="performance-analysis">Performance Analysis</h2>

<h3 id="equal-weighted-portfolio-1">Equal-Weighted Portfolio</h3>

<p>The performance of the <strong>equal-weighted long-short portfolio</strong> is summarized below. The key metrics include <strong>return, volatility, and Sharpe ratio</strong>—all annualized.</p>

<p>Interestingly, the <strong>low-volatility portfolio</strong> delivers significantly higher returns than the <strong>high-volatility portfolio</strong>. As expected, the lowest-volatility stocks also exhibit the lowest total volatility, while the highest-volatility stocks show the highest. As a result, the low-volatility portfolio achieves <strong>better risk-adjusted performance</strong>.</p>

<p>However, there’s a catch. Since the <strong>long portfolio (low-volatility stocks)</strong> is much less volatile than the <strong>short portfolio (high-volatility stocks)</strong>, the <strong>long-short portfolio ends up unbalanced</strong>, leading to poor performance.</p>

<p><strong>Performance Before Volatility Targeting</strong>:</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/barplot_metrics_ew.png" alt="Figure 3" /></p>

<p><strong>Figure 3</strong>: Geometric Return, Volatility, and Sharpe Ratio for equal-weighted portfolios (before volatility targeting).</p>

<h3 id="improving-performance-with-volatility-targeting">Improving Performance with Volatility Targeting</h3>

<p>Volatility targeting helps correct the <strong>imbalance between long and short positions</strong> by scaling portfolio weights based on volatility. This adjustment leads to <strong>more stable performance</strong> and a <strong>higher Sharpe ratio</strong> for both long-only and long-short portfolios.</p>

<p>Figure 4 shows the effect—volatility is now <strong>more aligned across the five portfolios</strong>, reducing the mismatch. As a result, the <strong>long-short portfolio improves</strong>, benefiting from a more balanced risk distribution.</p>

<p><strong>Performance After Volatility Targeting</strong>:</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/barplot_metrics_ew_vt.png" alt="Figure 4" /></p>

<p><strong>Figure 4</strong>: Geometric Return, Volatility, and Sharpe Ratio after volatility targeting.</p>

<h3 id="metrics-after-volatility-targeting">Metrics After Volatility Targeting</h3>

<p><img src="/assets/2024-12-15-low-volatility-factor/perf_backtest_ew_vt.png" alt="Figure 5" /></p>

<p><strong>Figure 5</strong>: Net Asset Value of the volatility-targeted portfolio.</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Long</th>
      <th>Short</th>
      <th>Long-Short</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Geometric Return (ann. %)</td>
      <td>11.1</td>
      <td>2.9</td>
      <td>7.7</td>
    </tr>
    <tr>
      <td>Volatility (ann. %)</td>
      <td>9.7</td>
      <td>9.7</td>
      <td>8.3</td>
    </tr>
    <tr>
      <td>Modified Sharpe Ratio (ann.)</td>
      <td>1.1</td>
      <td>0.3</td>
      <td>0.9</td>
    </tr>
    <tr>
      <td>Maximum Drawdown (%)</td>
      <td>29.5</td>
      <td>36.7</td>
      <td>33.6</td>
    </tr>
    <tr>
      <td>Maximum Time Under Water</td>
      <td>612.0</td>
      <td>1647.0</td>
      <td>944.0</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1</strong>: Portfolio statistics for the long, short, and long-short portfolios after volatility targeting.</p>

<p>Finally, <strong>Figure 6</strong> shows the <strong>portfolio weights for the long (low-volatility) and short (high-volatility) portfolios</strong> after volatility targeting.</p>

<p>Clearly, the short portfolio (high-volatility stocks) receives a <strong>much lower allocation</strong>, fluctuating between <strong>0.2 and 0.6</strong>. Meanwhile, the <strong>low-volatility portfolio stays almost fully invested</strong>. This makes sense—since the high-volatility stocks naturally have larger price swings, <strong>less capital is allocated to them</strong> to keep risk balanced.</p>

<p>Notably, during extreme market events like the <strong>dot-com crash (2000), the financial crisis (2009), and COVID (2020)</strong>, <strong>both portfolios temporarily reduce exposure</strong>, reflecting increased overall volatility.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/portfolio_weights_long_short_vol_target.svg" alt="Figure 6" /></p>

<p><strong>Figure 6</strong>: Portfolio weights for Long and Short portfolios after volatility targeting.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li><strong>The low-volatility factor delivers strong risk-adjusted returns.</strong> Lower-volatility stocks tend to outperform, contradicting the idea that higher risk always leads to higher returns.</li>
  <li><strong>Equal weighting in long-short portfolios creates a risk imbalance.</strong> Since low-volatility stocks are naturally less risky, shorting high-volatility stocks at equal weight leads to uneven exposure and weaker performance.</li>
  <li><strong>Volatility targeting balances risk and improves performance.</strong> Adjusting weights based on volatility evens out risk between long and short positions, leading to a more stable strategy.</li>
  <li><strong>Dynamic risk adjustment smooths returns.</strong> Volatility-targeted portfolios adapt to market shifts, reducing drawdowns and improving long-term stability.</li>
</ol>

<p>A simple <strong>volatility-targeting adjustment</strong> makes a long-short portfolio <strong>more stable and effective</strong>. In the next post, I’ll explore how <strong>combining multiple factors</strong> can further enhance results.</p>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quant" /><summary type="html"><![CDATA[The Low-Volatility Factor: A Steady Approach]]></summary></entry></feed>