<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-05-17T19:09:59+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Pieter-Jan</title><subtitle>Systematic trading and data science things.
</subtitle><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><entry><title type="html">Rebalancing Timing Luck</title><link href="http://localhost:4000/quants/2025/05/10/rebalancing-luck.html" rel="alternate" type="text/html" title="Rebalancing Timing Luck" /><published>2025-05-10T00:00:00+02:00</published><updated>2025-05-10T00:00:00+02:00</updated><id>http://localhost:4000/quants/2025/05/10/rebalancing%20luck</id><content type="html" xml:base="http://localhost:4000/quants/2025/05/10/rebalancing-luck.html"><![CDATA[<p>Rebalancing timing plays a crucial role in medium- to long-term strategies that rely on binary signals and infrequent rebalancing. Shifting the execution day by even a few days can impact performance — affecting positions, returns, volatility, and drawdowns.</p>

<p>This issue is often referred to as <em>rebalance timing luck</em>: the variation in performance that comes purely from the day you happen to rebalance, even when the strategy and signals stay the same.</p>

<p>Others have explored this too. <a href="https://www.thinknewfound.com/rebalance-timing-luck">Newfound Research</a> wrote a great piece highlighting how impactful this effect can be. And the <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5230603">Concretum Group</a> analyzed it in a GTAA context, showing that spreading rebalances out over time — instead of doing it all at once — can reduce timing risk and even lower turnover.</p>

<p>That got me curious: how much does timing luck affect my own long-short strategy? And could a more gradual rebalancing approach make it more stable, without changing the core logic?</p>

<h2 id="setup">Setup</h2>

<p>The strategy is a long-short market-neutral model based on LightGBM predictions (see <a href="https://piinghel.github.io/quants/2025/02/20/lgbm.html">this earlier post</a>). It rebalances every three weeks, with the entire portfolio updated on a fixed weekday at the closing price. Since there are three possible starting weeks (offsets) and five weekdays, that gives us 15 possible rebalancing schedules.</p>

<p>I simulate all 15 combinations:</p>
<ul>
  <li>3 offsets × 5 weekdays = 15 setups</li>
  <li>Each uses the same trained model and applies predictions available on that specific day</li>
  <li>Signals, logic, and universe stay exactly the same</li>
</ul>

<p>Then, I compare this to a <em>tranching</em> setup — a common approach where the portfolio is divided into parts and rebalanced gradually. Here, the portfolio is split into three equal tranches. One-third is rebalanced each week, rotating through the offsets. Over three weeks, the full portfolio is still updated — just in smaller, smoother steps. This helps reduce sensitivity to any single day’s noise.</p>

<h2 id="rebalancing-day-does-matter">Rebalancing Day Does Matter</h2>

<p>Here’s the cumulative return of the 15 full-rebalance variants:</p>

<p><img src="/assets/tranching/all_perf_plots.png" alt="Figure 1" /></p>

<p><strong>Figure 1:</strong> Cumulative returns for each rebalancing schedule (15 variants).</p>

<p>And the table of results:</p>

<table>
  <thead>
    <tr>
      <th>Tranche</th>
      <th>Geometric Return</th>
      <th>Volatility</th>
      <th>Sharpe Ratio</th>
      <th>Max Drawdown</th>
      <th>Time Underwater</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>offset=0, day=1</td>
      <td>10.38%</td>
      <td>6.58%</td>
      <td>1.53</td>
      <td>12.41%</td>
      <td>548 days</td>
    </tr>
    <tr>
      <td>offset=0, day=2</td>
      <td>10.10%</td>
      <td>6.54%</td>
      <td>1.50</td>
      <td>14.77%</td>
      <td>573 days</td>
    </tr>
    <tr>
      <td>offset=0, day=3</td>
      <td>10.61%</td>
      <td>6.54%</td>
      <td>1.57</td>
      <td>13.79%</td>
      <td>582 days</td>
    </tr>
    <tr>
      <td>offset=0, day=4</td>
      <td>12.10%</td>
      <td>6.73%</td>
      <td>1.73</td>
      <td>13.19%</td>
      <td>548 days</td>
    </tr>
    <tr>
      <td>offset=0, day=5</td>
      <td>12.17%</td>
      <td>6.91%</td>
      <td>1.70</td>
      <td>13.59%</td>
      <td>369 days</td>
    </tr>
    <tr>
      <td>offset=1, day=1</td>
      <td>12.53%</td>
      <td>6.90%</td>
      <td>1.75</td>
      <td>12.35%</td>
      <td>360 days</td>
    </tr>
    <tr>
      <td>offset=1, day=2</td>
      <td>12.21%</td>
      <td>6.97%</td>
      <td>1.69</td>
      <td>12.13%</td>
      <td>387 days</td>
    </tr>
    <tr>
      <td>offset=1, day=3</td>
      <td>12.30%</td>
      <td>6.87%</td>
      <td>1.72</td>
      <td>10.70%</td>
      <td>375 days</td>
    </tr>
    <tr>
      <td>offset=1, day=4</td>
      <td>11.64%</td>
      <td>6.70%</td>
      <td>1.68</td>
      <td>12.72%</td>
      <td>360 days</td>
    </tr>
    <tr>
      <td>offset=1, day=5</td>
      <td>11.62%</td>
      <td>6.76%</td>
      <td>1.66</td>
      <td>14.76%</td>
      <td>298 days</td>
    </tr>
    <tr>
      <td>offset=2, day=1</td>
      <td>10.90%</td>
      <td>6.50%</td>
      <td>1.67</td>
      <td>11.38%</td>
      <td>248 days</td>
    </tr>
    <tr>
      <td>offset=2, day=2</td>
      <td>11.27%</td>
      <td>6.60%</td>
      <td>1.56</td>
      <td>12.56%</td>
      <td>331 days</td>
    </tr>
    <tr>
      <td>offset=2, day=3</td>
      <td>10.61%</td>
      <td>6.60%</td>
      <td>1.63</td>
      <td>11.01%</td>
      <td>275 days</td>
    </tr>
    <tr>
      <td>offset=2, day=4</td>
      <td>10.99%</td>
      <td>6.53%</td>
      <td>1.63</td>
      <td>13.10%</td>
      <td>352 days</td>
    </tr>
    <tr>
      <td>offset=2, day=5</td>
      <td>10.37%</td>
      <td>6.45%</td>
      <td>1.56</td>
      <td>13.10%</td>
      <td>593 days</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Performance metrics for each full-rebalance variant.</p>

<p>I expected more divergence between schedules, but returns ended up being fairly close — which was a bit surprising. Still, there’s meaningful variation: Sharpe ratios range from 1.50 to 1.75 for individual tranches, and time under water can differ by hundreds of days.</p>

<p>None of this comes from the model or the signal — it’s purely due to small shifts in rebalance timing that compound over time. Even when overall performance looks stable, this kind of noise makes it harder to tell whether a change is genuinely better or just lucky on timing.</p>

<h2 id="tranching-a-simple-effective-fix">Tranching: A Simple, Effective Fix</h2>

<p>Instead of picking a single rebalancing day, we can average across them.</p>

<p>In the tranching setup, I split the portfolio into three equal parts. Each week, I rebalance one-third of the portfolio — always on the same weekday (say, every Wednesday) — but each tranche follows a different offset within the three-week cycle. So over three weeks, the full portfolio is refreshed, just not all at once.</p>

<p>This staggered execution spreads risk more evenly across time without changing the model or signals.</p>

<p>To analyze the results, I group performance by weekday and average across the three offsets. This gives a cleaner comparison to the previous “all-at-once” setup.</p>

<p><img src="/assets/tranching/tranched_perf_plots.png" alt="Figure 2" /><br />
<strong>Figure 2:</strong> Cumulative returns with tranching by weekday (averaged over offsets).</p>

<table>
  <thead>
    <tr>
      <th>Weekday</th>
      <th>Geometric Return</th>
      <th>Volatility</th>
      <th>Sharpe Ratio</th>
      <th>Max Drawdown</th>
      <th>Time Underwater</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1 (Mon)</td>
      <td>11.32%</td>
      <td>6.08%</td>
      <td>1.80</td>
      <td>11.08%</td>
      <td>370 days</td>
    </tr>
    <tr>
      <td>2 (Tue)</td>
      <td>11.23%</td>
      <td>6.04%</td>
      <td>1.79</td>
      <td>11.63%</td>
      <td>301 days</td>
    </tr>
    <tr>
      <td>3 (Wed)</td>
      <td>11.24%</td>
      <td>6.08%</td>
      <td>1.78</td>
      <td>11.89%</td>
      <td>299 days</td>
    </tr>
    <tr>
      <td>4 (Thu)</td>
      <td>11.63%</td>
      <td>6.06%</td>
      <td>1.85</td>
      <td>11.01%</td>
      <td>303 days</td>
    </tr>
    <tr>
      <td>5 (Fri)</td>
      <td>11.43%</td>
      <td>6.11%</td>
      <td>1.80</td>
      <td>12.37%</td>
      <td>364 days</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 2:</strong> Tranche-averaged performance by weekday.</p>

<p>The improvements aren’t massive, but they’re noticeable. Sharpe ratios rise into the 1.78–1.85 range, with Friday and Thursday performing best. Volatility tightens around 6.0%, and drawdowns are generally shallower and recover faster — especially compared to the more uneven results in individual tranches.</p>

<p>The gains aren’t just about higher returns — they mostly come from smoother execution. By spreading out rebalances, you avoid abrupt shifts in positioning, which leads to more stable performance and less noise.</p>

<p>From a research perspective, this helps a lot. It reduces the randomness from rebalance timing and makes it easier to spot whether a model improvement is real or just the result of good timing.</p>

<h2 id="why-it-works">Why It Works</h2>

<p>Tranching softens the impact of short-term shocks. A single full-book rebalance can pick up temporary price dislocations or liquidity gaps — injecting noise into an otherwise stable strategy. By spreading rebalancing over time, those effects average out.</p>

<p>The logic and signal stay exactly the same. The full portfolio still turns over every three weeks, just more gradually. That reduces sensitivity to any one day’s conditions, without changing overall turnover.</p>

<p>There’s also a subtle benefit on the signal side. In the standard setup, you only act on the model’s predictions once every three weeks. With tranching, you fold in new predictions each week — even if it’s only on a third of the capital. That introduces a kind of temporal diversification and makes the strategy more responsive to new information.</p>

<h2 id="a-few-trade-offs-and-extra-benefits">A Few Trade-Offs and Extra Benefits</h2>

<p>Tranching isn’t free. Weekly rebalancing means running the pipeline more often, sending more orders, and keeping an eye on execution regularly. That adds a bit of overhead — especially when some trades are small and barely move the portfolio.</p>

<p>But in practice, it’s felt worth it. Not just in research — even in live runs, it makes the strategy feel more stable and less twitchy around timing noise.</p>

<p>I also came across something interesting in the GTAA literature: tranching might reduce turnover and transaction costs. I’m not entirely sure why — maybe it avoids large shifts or cancels out trades across tranches. Still figuring that part out. If you’ve got ideas or experience with it, let me know — I’m curious.</p>

<p>And a small bonus: by updating just a slice each week, you keep injecting fresh signal without going all-in. The portfolio stays a bit more responsive without overtrading.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>What started as a small curiosity around rebalancing schedules turned into a strong case for using tranching by default.</p>

<p>The signal and logic don’t change — but the results become cleaner and more stable. That’s especially helpful when evaluating model tweaks, since it reduces noise from execution timing.</p>

<p>I’ll be using this setup going forward for any strategy that rebalances on a fixed cycle. It’s simple, effective, and makes the whole process easier to trust.</p>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[Rebalancing timing plays a crucial role in medium- to long-term strategies that rely on binary signals and infrequent rebalancing. Shifting the execution day by even a few days can impact performance — affecting positions, returns, volatility, and drawdowns.]]></summary></entry><entry><title type="html">Good Reads</title><link href="http://localhost:4000/quants/2025/04/20/dope-articles.html" rel="alternate" type="text/html" title="Good Reads" /><published>2025-04-20T00:00:00+02:00</published><updated>2025-04-20T00:00:00+02:00</updated><id>http://localhost:4000/quants/2025/04/20/dope%20articles</id><content type="html" xml:base="http://localhost:4000/quants/2025/04/20/dope-articles.html"><![CDATA[<h2 id="roadmap">Roadmap</h2>

<p>This is just a running list of articles, papers, and blog posts I’ve found interesting — mostly around cross-sectional asset pricing, machine learning models, and quant investing. I’ll keep adding to it over time as I go deeper.</p>

<p>Some of this is academic, some more hands-on. Occasionally I might throw in stuff that’s not directly trading-related but still sparked something for me.</p>

<h3 id="machine-learning--asset-pricing">Machine Learning + Asset Pricing</h3>

<ul>
  <li><strong><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5103546">Artificial Intelligence Asset Pricing Models</a></strong><br />
<em>Bryan T. Kelly, Boris Kuznetsov, Semyon Malamud, Teng Andrea Xu (2025)</em>
    <ul>
      <li>Uses transformers to directly learn the stochastic discount factor from raw panel data. A step toward end-to-end portfolio optimization.</li>
    </ul>
  </li>
  <li><strong><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3751012">Building Cross-Sectional Systematic Strategies by Learning to Rank</a></strong><br />
<em>Daniel Poh, Bryan Lim, Stefan Zohren, Stephen Roberts (2021)</em>
    <ul>
      <li>Applies learning-to-rank techniques to directly optimize for cross-sectional ordering instead of predicting returns. Tightly aligned with how quant signals are actually used.</li>
    </ul>
  </li>
</ul>

<p>More coming soon — this is my scratchpad for everything worth revisiting.</p>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[Roadmap]]></summary></entry><entry><title type="html">LightGBM</title><link href="http://localhost:4000/quants/2025/02/20/lgbm.html" rel="alternate" type="text/html" title="LightGBM" /><published>2025-02-20T00:00:00+01:00</published><updated>2025-02-20T00:00:00+01:00</updated><id>http://localhost:4000/quants/2025/02/20/lgbm</id><content type="html" xml:base="http://localhost:4000/quants/2025/02/20/lgbm.html"><![CDATA[<h1 id="todo">TODO</h1>
<ul>
  <li>update figures and tables with more recent data.</li>
  <li>reread text and style.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>In my last two articles, I explored different ways to rank stocks and build long-short portfolios.</p>

<p>In the first article, I used a simple one-factor model based on volatility — ranking stocks by their recent volatility and betting on the fact that low-volatility stocks tend to outperform high-volatility ones.</p>

<p>In the second article, I moved to a more flexible approach by combining multiple features — like momentum, volatility, size, and liquidity — using Ridge regression, a linear model that learns how to weigh these features. I also examined some important design choices, like whether to use ranking or z-scoring for feature normalization, and whether to normalize the target label by sector.</p>

<p>Now I want to take this a step further and see what happens when I switch to a non-linear model, LightGBM. The idea is to keep everything else the same — same dataset, same features, same target label, same allocation model — and only change the model. This way, I can isolate the impact of using a non-linear approach that can capture more complex patterns in the data.</p>

<p>In the end, I’ll compare LightGBM with both Ridge and the low-volatility factor, to clearly see how much is gained (or not) when moving from a simple factor to a linear model and then to a non-linear one.</p>

<h2 id="recap-of-the-process">Recap of the process</h2>

<p>Before diving into LightGBM, let me briefly recap the setup I’ve been using, which stays unchanged for this analysis.</p>

<p>I’m working with the Russell 1000 universe, using point-in-time data and filtering out stocks priced below $5 to avoid illiquid names. The feature set is broad, covering about 150 features related to momentum, volatility, liquidity, size, mean reversion, and correlation with the market.</p>

<p>For the target label, I focus on the Sharpe ratio over the next 20 days. I’ve chosen this because it provides a more stable and risk-adjusted measure of performance compared to raw returns, which are often extremely noisy.</p>

<p>All features are normalized using cross-sectional ranking — turning each feature into a percentile rank between 0 and 1 on each day. Ranking has become my default because it makes features comparable across stocks and over time, and it avoids the impact of extreme outliers. I know z-scoring could sometimes be a better choice, and that’s something I want to revisit in the future, but for now I’m sticking with ranking.</p>

<p>Once I have model scores, I use a volatility-targeted allocation model to build long-short portfolios. Specifically, I go long the top 75 and short the bottom 75 stocks, adjusting each position based on volatility to avoid concentrating risk in a few names. This makes the portfolio more stable over time.</p>

<p>Finally, portfolios are rebalanced every three weeks. I find this strikes a good balance between adapting to new information and keeping turnover under control.</p>

<h2 id="why-move-to-lightgbm">Why move to LightGBM</h2>

<p>So far, the Ridge model combined multiple features in a linear way — it learned fixed weights for each feature, and that was it. But markets are rarely linear, and I suspect there are more complex patterns that Ridge simply can’t capture. For example, a feature like volatility might only matter when combined with a momentum signal, or extreme values of a feature might behave differently than moderate ones.</p>

<p>To address this, I’m moving to gradient boosting, a method that builds strong predictive models by combining many small decision trees. Gradient boosting works iteratively — each new tree tries to fix the mistakes of the previous ones — making it well suited to capture non-linear effects and feature interactions.</p>

<p>There are several well-known implementations of gradient boosting, like XGBoost, CatBoost, and LightGBM. I’m using LightGBM because it’s fast and efficient, especially when working with large datasets like mine. In practice, I don’t expect a big difference between these libraries, but LightGBM tends to be much quicker to train and predict, so it’s the obvious choice for this kind of project.</p>

<p>Another reason to choose gradient boosting is that these models are often among the best-performing approaches for tabular datasets — including many winning solutions in Kaggle competitions. So it makes sense to see whether this kind of model can push performance beyond what Ridge and the low-volatility factor can achieve.</p>

<p>With that in mind, I applied LightGBM to the same setup as before: ranking stocks in the Russell 1000, going long the top 75, short the bottom 75, and using volatility-based position sizing. This keeps everything consistent, so we can isolate the impact of the model itself.</p>

<h2>Performance</h2>

<p>Let’s take a look at how LightGBM performs when applied to the same setup as before — and the results are quite strong.</p>

<div style="display: block; text-align: left;">
  <img src="/assets/lightgbm/perf_lgbm.png" width="600" style="display: block; margin: 0;" />
  <p><strong>Figure 1:</strong> Performance of the LightGBM strategy, before and after transaction costs.</p>
</div>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Short Sleeve</th>
      <th>Long Sleeve</th>
      <th>L/S (No Fees)</th>
      <th>L/S (Fees)</th>
      <th>Russell 1000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Return (Annualized)</td>
      <td>1.46%</td>
      <td>12.48%</td>
      <td>12.33%</td>
      <td>10.69%</td>
      <td>7.29%</td>
    </tr>
    <tr>
      <td>Volatility (Annualized)</td>
      <td>9.95%</td>
      <td>10.54%</td>
      <td>6.67%</td>
      <td>6.69%</td>
      <td>19.58%</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.15</td>
      <td>1.18</td>
      <td>1.85</td>
      <td>1.60</td>
      <td>0.37</td>
    </tr>
    <tr>
      <td>Max Drawdown</td>
      <td>30.78%</td>
      <td>31.10%</td>
      <td>13.79%</td>
      <td>13.97%</td>
      <td>56.88%</td>
    </tr>
    <tr>
      <td>Max Time Underwater (days)</td>
      <td>1,619 days</td>
      <td>599 days</td>
      <td>234 days</td>
      <td>265 days</td>
      <td>1,666 days</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Performance statistics of the LightGBM strategy, with and without transaction costs (5 basis points per trade).</p>

<p>After accounting for transaction costs, the strategy posts a 10.7% annualized return with volatility just under 7%, leading to a Sharpe ratio of 1.60. Drawdowns stay around 14%, and the strategy tends to bounce back relatively quickly after periods of underperformance.</p>

<p>What’s nice to see is that both sides of the book are doing their job. The long leg delivers a solid 12.5% return — well above the Russell 1000’s 7.3%. The short leg returns just 1.5%, meaning those bottom-ranked stocks are underperforming the market, as intended. That’s exactly the kind of clean separation you want in a ranking strategy.</p>

<p>That said, it’s worth noting the Russell 1000 isn’t a proper benchmark here — it’s long-only and far more volatile — but it’s still helpful as a rough point of reference since we’re trading within that universe.</p>

<p>Because both legs are volatility-targeted, the difference in returns translates cleanly into long-short performance, without relying on one side to carry all the weight. The result is a fairly smooth and balanced profile</p>

<h2 id="signal-quality">Signal Quality</h2>

<p>To understand <em>why</em> LightGBM performs well, it’s useful to zoom in on signal quality — how well each model ranks stocks by expected performance before any portfolio construction takes place.</p>

<p>Each day, the Ridge and LightGBM models output a score for every stock $i$ in the universe. These scores are trained to predict the Sharpe ratio over the next 20 trading days, computed out-of-sample and normalized by sector to remove broad industry effects. This gives us a daily cross-section of predicted scores \(\hat{y}_{i,t}\) and realized outcomes $y_{i,t}$.</p>

<p>The low-volatility signal, in contrast, is a simple heuristic — ranking stocks based on their trailing volatility — and does not involve model training.</p>

<p>To measure alignment between the model’s predictions and actual future performance, we compute the Spearman rank correlation between predicted and realized rankings across all stocks:</p>

\[\rho_t = \text{Spearman} \left( \{ \hat{y}_{i,t} \}, \{ y_{i,t} \} \right)\]

<p>This yields a daily time series ${\rho_t}$, from which we compute:</p>
<ul>
  <li>Mean correlation — average predictive strength</li>
  <li>Standard deviation — signal stability</li>
  <li>Sharpe ratio — consistency over time (mean divided by standard deviation)</li>
</ul>

<p><img src="/assets/lightgbm/signal.png" width="600" /></p>
<p>Figure 2: Cumulative Spearman correlation between model signals and future Sharpe ratio rankings, computed daily.</p>
<p><br /></p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Low Volatility</th>
      <th>Ridge Regression</th>
      <th>LightGBM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Mean</td>
      <td>4.96%</td>
      <td>5.14%</td>
      <td>5.43%</td>
    </tr>
    <tr>
      <td>Standard Deviation</td>
      <td>14.39%</td>
      <td>8.70%</td>
      <td>7.03%</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.34</td>
      <td>0.59</td>
      <td>0.77</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 2: Daily signal quality metrics based on Spearman correlation between predicted and realized Sharpe ratio rankings.</strong></p>

<p>The results might seem modest — average correlations around 5% — but that’s expected when working with financial data. Predicting returns is noisy by nature. The key is stability. LightGBM delivers a more consistent signal than the other models, with less variability and a higher signal-level Sharpe ratio.</p>

<p>This kind of stability is valuable. Even weak signals, if reliable, can drive meaningful long-term performance when applied systematically.</p>

<h3 id="2-financial-performance">2. Financial Performance</h3>

<p><img src="/assets/lightgbm/perf_return_comp.png" width="600" /></p>
<p><strong>Figure 3:</strong> Performance over the full sample (1997–2024).</p>

<p><img src="/assets/lightgbm/perf_return_comp_last_10yr.png" width="600" /></p>
<p><strong>Figure 4:</strong> Performance over the last decade (2015–2024).</p>
<p><br /></p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Low Vol (Full)</th>
      <th>LR (Full)</th>
      <th>LGBM (Full)</th>
      <th>Low Vol (10Y)</th>
      <th>LR (10Y)</th>
      <th>LGBM (10Y)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Return (Ann. %)</td>
      <td>5.62%</td>
      <td>8.76%</td>
      <td>11.68%</td>
      <td>7.94%</td>
      <td>8.91%</td>
      <td>9.33%</td>
    </tr>
    <tr>
      <td>Volatility (Ann. %)</td>
      <td>8.67%</td>
      <td>7.98%</td>
      <td>7.09%</td>
      <td>9.76%</td>
      <td>8.91%</td>
      <td>8.17%</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.67</td>
      <td>1.09</td>
      <td>1.59</td>
      <td>0.83</td>
      <td>1.00</td>
      <td>1.13</td>
    </tr>
    <tr>
      <td>Max. Drawdown (%)</td>
      <td>35.27%</td>
      <td>20.18%</td>
      <td>13.71%</td>
      <td>12.11%</td>
      <td>17.07%</td>
      <td>11.69%</td>
    </tr>
    <tr>
      <td>Max. Time Under Water</td>
      <td>844 days</td>
      <td>862 days</td>
      <td>297 days</td>
      <td>350 days</td>
      <td>307 days</td>
      <td>297 days</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 3: Strategy performance over the full sample (1997–2024) and the last decade (2015–2024).</strong></p>

<p>Figures 3 and 4, along with Table 3, show a clear pattern: LightGBM outperforms both Ridge and Low Vol across nearly every metric. It delivers higher returns and lower volatility, resulting in the strongest Sharpe ratios. Drawdowns are smaller, and recovery is faster — with time under water cut by more than half compared to the linear model.</p>

<p>That said, the last decade has been tougher. Performance has declined across the board, and while LightGBM still leads, the gap has narrowed. Its Sharpe ratio drops from 1.59 to 1.13, and Ridge isn’t far behind at 1.00. Returns are flatter, and the edge is less pronounced.</p>

<p>What’s driving this? Has the market become more efficient? Are the features decaying? Or is the model overfitting to an older regime? These are open questions I plan to explore in a future post.</p>

<p>For now, the takeaway is simple: LightGBM provides a consistent improvement over simpler models. But like any strategy, its performance evolves — and understanding <em>why</em> is just as important as measuring <em>how much</em>.</p>

<h2 id="whats-next">What’s Next</h2>

<p>There’s clearly still a lot to improve. The model works, but performance has softened in the last decade — and it’s not obvious why. It could be changing regimes, feature decay, more competition… or all of the above.</p>

<p>Here’s what I’m exploring:</p>

<ul>
  <li>
    <p><strong>Portfolio construction</strong><br />
I’ve started digging into <em>Advanced Portfolio Management</em> and <em>The Elements of Quantitative Investing</em> by Paleologo — both really solid so far. I want to better understand what’s actually driving P&amp;L. Am I just picking up factor risk? Can I improve how I size positions or manage constraints?</p>
  </li>
  <li>
    <p><strong>Feature and target design</strong><br />
Right now, everything is based on price, volume, and market cap. There’s probably more signal out there — or better ways to use the existing data. Also thinking about prediction horizon: why stick to 20 days? What if I blend different horizons?</p>
  </li>
  <li>
    <p><strong>Rethinking the objective</strong><br />
The current pipeline scores stocks, then allocates separately. But maybe it makes more sense to predict portfolio weights directly. I’m especially intrigued by the recent paper <em>Artificial Intelligence Asset Pricing Models</em> (Kelly et al., 2025), which uses transformers to learn the stochastic discount factor from raw panel data. Super ambitious, but a really interesting direction.</p>
  </li>
</ul>

<p>Lots of cool stuff to explore — and I’ll share more as I go.</p>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[TODO update figures and tables with more recent data. reread text and style.]]></summary></entry><entry><title type="html">Ridge regression</title><link href="http://localhost:4000/quants/2025/02/09/ridge.html" rel="alternate" type="text/html" title="Ridge regression" /><published>2025-02-09T00:00:00+01:00</published><updated>2025-02-09T00:00:00+01:00</updated><id>http://localhost:4000/quants/2025/02/09/ridge</id><content type="html" xml:base="http://localhost:4000/quants/2025/02/09/ridge.html"><![CDATA[<h2 id="todo">TODO:</h2>
<ul>
  <li>update with new data</li>
  <li>reread + spelling and style.</li>
</ul>

<p><a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">In the last article</a>, I showed how scaling returns by volatility helped improve performance with minimal complexity. This time, I want to take a more data-driven approach—one where the model learns how to combine different features instead of me setting the rules manually.</p>

<p>Stock prediction is hard. Instead of trying to guess exact returns, I’m focusing on something more practical: figuring out whether stock A will perform better than stock B. I don’t need to know by how much—just which one is likely to do better.</p>

<p>To do this, I’m using a multiple linear regression model that takes in many features—like price changes, trading volume, and risk levels—and learns how to weigh them optimally. The model is trained on a target label, which is just the outcome I want it to predict, like future returns or the Sharpe ratio.</p>

<p>But there are a few challenges. Features behave differently over time—their distributions shift, patterns that worked before can break down, and some signals that seem useful in one market environment might completely disappear in another. On top of that, the signal-to-noise ratio is low, making it tough to extract meaningful insights.</p>

<p>The dataset stays the same as the one used in <a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">the previous article</a>: daily price, volume, and market capitalization data for all Russell 1000 (RIY) constituents, covering about 3,300 stocks historically. We use point-in-time constituents of the RIY and apply basic liquidity filters, excluding stocks priced below $5 to ensure realistic implementation.</p>

<h2 id="feature-engineering">Feature Engineering</h2>

<p>Coming from a statistical and computer science background, I naturally lean toward letting the data determine relationships rather than imposing rigid assumptions. This contrasts with traditional multifactor portfolio approaches, where the modeler manually decides how to combine features (factors). Instead, I use a regression model that learns the optimal feature weights directly from the data.</p>

<p>For now, I’m keeping it simple with linear regression, assuming the relationships are linear and avoiding interaction terms. It’s a straightforward, data-driven approach that focuses on identifying direct, linear connections between features and the target variable.</p>

<h2 id="choosing-predictive-features">Choosing Predictive Features</h2>
<p>Before the model can learn anything useful, I define the right features and select a target variable. I focus on price, volume, market capitalization, and market-derived indicators, computing them daily for all 3,300 stocks in my universe. Here’s a breakdown of the key feature groups:</p>

<p><strong>1. Momentum Features</strong><br />
Capture trend-following behavior.</p>
<ul>
  <li>Lagged returns over 1 to 10 days.</li>
  <li>Rolling cumulative returns over 21 to 252 days.</li>
  <li>MACD to detect shifts in momentum.</li>
</ul>

<p><strong>2. Volatility Features</strong><br />
Measure risk.</p>
<ul>
  <li>Rolling historical volatility over 21, 63, or 126 days.</li>
  <li>Average True Range (ATR) to normalize price fluctuations.</li>
</ul>

<p><strong>3. Liquidity Features</strong><br />
Assess trading activity.</p>
<ul>
  <li>Rolling mean and standard deviation of trading volume.</li>
  <li>Ratio of current volume to its rolling maximum to highlight unusual trading activity.</li>
</ul>

<p><strong>4. Size Features</strong><br />
Measure company scale.</p>
<ul>
  <li>Rolling mean and minimum of market cap.</li>
  <li>Distinguishes small-cap from large-cap stocks.</li>
</ul>

<p><strong>5. Short Mean Reversion Features</strong><br />
Identify when prices revert to historical norms.</p>
<ul>
  <li>Price deviation from its rolling moving average.</li>
  <li>Position relative to rolling minimum and maximum values.</li>
  <li>Bollinger Bands to spot overbought or oversold conditions.</li>
</ul>

<p><strong>6. Correlation with the Market</strong><br />
Capture systematic risk.</p>
<ul>
  <li>Rolling correlation with the Russell 1000 over 63-day windows.</li>
  <li>Helps separate defensive stocks from high-beta names.</li>
</ul>

<p>In total, I work with around 150 predictive features, obviously many of which are correlated.</p>

<h2 id="target-variable">Target Variable</h2>
<p>The model is trained to predict return over the next 20 days and Sharpe ratio over the next 20 days. While other time horizons could be explored, I’m keeping it simple and focusing on 20 days for now.</p>

<h2 id="preprocessing-cross-sectional-normalization">Preprocessing: Cross-Sectional Normalization</h2>

<p>Cross-sectional normalization adjusts each feature relative to all other stocks on the same day, ensuring the model focuses on relative differences rather than absolute values. This transformation also makes interpretation easier—when features are normalized consistently, it becomes simpler to identify high or low values over time.</p>

<p>By applying this normalization, I make sure stocks are evaluated on a comparable basis at each point in time. This should help the model focus on learning the relative order of stocks rather than absolute levels, while also preventing certain features from disproportionately influencing the predictions.</p>

<h2 id="mathematical-formulation">Mathematical Formulation</h2>

<p>For a given feature $X^p$, the normalized value for stock $i$ at time $t$ is:</p>

\[X_{i,t}^{p,\text{norm}} = f\left(X_{i,t}^{p}, X_{1:N,t}^{p}\right)\]

<p>where:</p>

<ul>
  <li>$X^p_{i,t}$ is the raw feature value for stock $i$ at time $t$.</li>
  <li>$X^p_{1:N,t}$ is the set of values for all stocks at time $t$ for feature $p$.</li>
  <li>$f(\cdot)$ is the chosen normalization method.</li>
</ul>

<p>Different methods can be used to transform the raw values, each with its own strengths and tradeoffs. I’ll be comparing Z-scoring and ranking sometimes also called uniformization.</p>

<h2 id="z-scoring">Z-Scoring</h2>

<p>One common approach is z-scoring, which standardizes features by centering them around zero and scaling them to have a standard deviation of one:</p>

\[X_{i,t}^{p,\text{norm}} = \frac{X_{i,t}^{p} - \hat{\mu}^p_t}{\hat{\sigma}^p_t}\]

<p>where:</p>

<ul>
  <li>
    <p>$\hat{\mu}^p_t$ is the estimated mean across all stocks at time $t$ for feature $p$:</p>

\[\hat{\mu}^p_t = \frac{1}{N} \sum_{i=1}^{N} X_{i,t}^{p}\]
  </li>
  <li>
    <p>$\hat{\sigma}^p_t$ is the estimated standard deviation:</p>

\[\hat{\sigma}^p_t = \sqrt{\frac{1}{N} \sum_{i=1}^{N} \left( X_{i,t}^{p} - \hat{\mu}^p_t \right)^2}\]
  </li>
</ul>

<p>Z-scoring retains the relative magnitudes of the original values, allowing the model to distinguish between small and large variations. However, it is sensitive to extreme outliers, so values beyond ±5 standard deviations are clipped.</p>

<h2 id="ranking-normalization">Ranking Normalization</h2>

<p>Another approach is ranking normalization which transforms feature values into ranks and scales them between 0 and 1:</p>

\[R_{i,t}^{p} = \frac{r_{i,t}^{p}}{N}\]

<p>where:</p>

<ul>
  <li>$r^p_{i,t}$ is the rank of stock $i$ at time $t$ based on feature $p$ (0 for the lowest value, $N$ for the highest).</li>
  <li>$R^p_{i,t}$ is the normalized rank.</li>
</ul>

<p>Unlike z-scoring, ranking ensures that the distribution remains the same over time. This makes it robust to extreme values but removes magnitude information—only relative positioning is preserved.</p>

<h2 id="visualizing-the-effect-of-normalization">Visualizing the Effect of Normalization</h2>

<p>Below, in Figure 1, I summarize the different normalization methods applied to a single feature (20-day return). From left to right: the original distribution, z-scored, and ranked.</p>

<p><img src="/assets/ridge/example_normalization.png" alt="Figure 1" /></p>

<p><strong>Figure 1</strong>: Effect of normalization on 20-day return distribution. Left: Original data, Middle: Z-scored, Right: Ranked between 0 and 1.</p>

<h2 id="choosing-the-right-normalization-method">Choosing the Right Normalization Method</h2>

<p>How I normalize features has a big impact on how the model interprets stock differences. The choice between z-scoring and ranking depends on what I want the model to focus on.</p>

<ul>
  <li><strong>Z-scoring</strong> keeps magnitude differences intact, which helps when the strength of a signal matters. But it more sensitive to distribution shifts over time.</li>
  <li><strong>Ranking</strong> is more stable and removes extreme outliers since values are always mapped to a uniform distribution. However, this process also discards information about the magnitude of differences between stocks</li>
</ul>

<p>Both methods ensure that stocks are processed in a comparable way on any given day, but they emphasize different aspects of the data.</p>

<h2 id="evaluating-the-impact-of-normalization">Evaluating the Impact of Normalization</h2>

<p>To see if normalization improves results, I compare three approaches:</p>

<ol>
  <li>Using raw, unnormalized features</li>
  <li>Applying z-scoring across all stocks</li>
  <li>Using ranking across all stocks</li>
</ol>

<p>If normalization improves performance, the next step is to refine how I apply it—especially to the target label.</p>

<h2 id="should-the-target-label-be-normalized-by-sector">Should the Target Label Be Normalized by Sector?</h2>

<p>Normalizing features ensures consistency over time, but what about the target label? Instead of normalizing returns across all stocks, I test whether normalizing them within each sector improves results while keeping all other features globally normalized.</p>

<ul>
  <li><strong>Global normalization</strong> applies the same normalization across the full stock universe.</li>
  <li><strong>Sector-specific normalization</strong> adjusts returns within each sector while keeping all other features globally normalized.</li>
</ul>

<p>My hypothesis is that sector-normalizing the target label might help by preventing cross-sector differences from distorting the model’s learning process. Stocks in different industries often have structurally different return profiles, so this adjustment could make return comparisons more meaningful. Whether this actually improves performance is something I aim to find out.</p>

<h2 id="handling-missing-data">Handling Missing Data</h2>

<p>Some models, like decision trees, handle missing data automatically, but others don’t. To keep things simple, I use:</p>

<ul>
  <li><strong>Forward fill:</strong> Use the last known value if past data exists.</li>
  <li><strong>Cross-sectional mean imputation:</strong> If no past data is available, replace the missing value with the sector average for that day.</li>
  <li><strong>Default values:</strong>
    <ul>
      <li>For z-scoring, set missing values to 0.</li>
      <li>For ranking, set missing values to 0.5 (midpoint of the ranking scale).</li>
    </ul>
  </li>
</ul>

<p>This approach is simple, effective, and works well for now.</p>

<h2 id="modeling-the-cross-sectional-normalized-score">Modeling the Cross-Sectional Normalized Score</h2>

<p>At the core of this strategy, I’m building a model to predict a stock’s cross-sectional normalized score, which could be its Sharpe ratio, return, or another performance measure. I think of this as a function mapping available information at time $t$ to an expected score at $t+1$. To ensure comparability across stocks, the score is normalized in the cross-section before modeling.</p>

<p>Mathematically, I assume that there exists a function $g(\cdot)$ such that:</p>

\[s_{i,t+1} = g(\mathbf{z}_{i,t}) + \epsilon_{i,t+1}\]

<p>where:</p>
<ul>
  <li>$s_{i,t+1}$ is the true cross-sectional normalized score for stock $i$ at time $t+1$.</li>
  <li>$z_{i,t}$ is a vector of predictor variables for stock $i$ at time $t$.</li>
  <li>$\epsilon_{i,t+1}$ is the error term, representing what the model cannot predict.</li>
</ul>

<p>The objective is to approximate $g(\cdot)$ using historical data. This function follows two key principles:</p>

<ul>
  <li>it leverage the entire panel of stocks, meaning the same functional form applies universally.</li>
  <li>It depends only on stock-specific features at time $t$. While some features contain past information (such as return over the past 20 days), these are explicitly engineered rather than dynamically learned. In addition, the model does not learn interactions between different stocks.</li>
</ul>

<h3 id="ridge-regression-as-a-baseline">Ridge Regression as a Baseline</h3>

<p>To estimate $g(\cdot)$, I use Ridge Regression, a simple yet effective baseline, particularly when predictors are highly correlated. It solves the following optimization problem:</p>

\[\underset{\boldsymbol{\beta}}{\min} \frac{1}{n} \sum_{i=1}^n (s_{i,t+1} - \mathbf{x}_i^\top \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \beta_j^2\]

<p>where the second term, $\lambda \sum_{j=1}^p \beta_j^2$, regularizes the coefficients to prevent instability.</p>

<p>Ridge is a reasonable choice here because:</p>
<ul>
  <li>Stocks with similar characteristics often exhibit collinearity, and Ridge helps stabilize coefficient estimates.</li>
  <li>The regularization term shrinks extreme values, reducing sensitivity to noise.</li>
  <li>It provides a simple reference point before exploring more complex models.</li>
</ul>

<p>The model is estimated using historical data, and to assess its effectiveness, I apply an expanding walkforward validation which I explain just below in a bit more detail.</p>

<h2 id="expanding-walkforward-validation">Expanding Walkforward Validation</h2>

<p>To see how well the model holds up over time, I use an expanding walkforward validation. The idea is simple:</p>

<ol>
  <li>Start with a 3-year burn-in period – The model isn’t tested yet; it just learns from the data.</li>
  <li>Update the model every 2 years – Each time, I add the latest data and refit the model.</li>
  <li>Keep expanding the dataset – Older data stays in, and new data gets added.</li>
</ol>

<p>With stock data, I’ve always found that the more historical data, the better. The signal-to-noise ratio is low, so keeping as much information as possible helps the model the find the signal in all the noise.</p>

<p>A rolling validation window could be an alternative, but it discards older data that might still be valuable. In my experience, an expanding window works better because it allows the model to pick up long-term relationships, leading to more stable predictions.</p>

<p>For hyperparameter tuning, one option is to split the training set into separate train and validation sets. But honestly, I’ve never found this to be worth the extra time. Optimizing hyperparameters can take a while, and in most cases, default values that make sense are already a very good starting point.</p>

<p>Below is a schematic of the expanding walkforward approach:</p>

<p><img src="/assets/ridge/walk-forward.png" alt="Figure 2" /></p>

<p><strong>Figure 2</strong>: Expanding walkforward validation process.</p>

<h2 id="portfolio-construction">Portfolio Construction</h2>

<p>Once I have stock rankings, I build a long-short portfolio:</p>

<ul>
  <li>I go long on the 75 stocks with the highest scores.</li>
  <li>I short the 75 stocks with the lowest scores.</li>
</ul>

<p>The approach is robust across different portfolio sizes, whether using 50, 100, or 150 stocks.</p>

<p>To keep risk under control, I use volatility targeting:</p>
<ul>
  <li>Higher-volatility stocks get smaller weights.</li>
  <li>Lower-volatility stocks get larger weights.</li>
</ul>

<p>This ensures that the portfolio maintains a stable risk profile instead of being dominated by a few volatile names.</p>

<p>For a deeper dive into my portfolio construction process, check out my <a href="https://piinghel.github.io/quant/2024/12/15/low-volatility-factor.html">previous article</a> where I go into more detail.</p>

<h2 id="results">Results</h2>
<p>To evaluate different modeling choices, I tested 10 model variations, combining:</p>
<ul>
  <li>5 normalization methods: Raw, Z-score (global &amp; sector), Ranking (global &amp; sector).</li>
  <li>2 target labels: Sharpe Ratio (SR 20) and Return (Return 20).</li>
  <li>A combined strategy (“Combo”), which equally weights all strategies.</li>
</ul>

<p>To ensure a fair comparison visually, all strategies are scaled to 10% volatility. The goal is to understand how normalization, sector adjustments, and target labels affect performance. Figure 3 visualizes cumulative returns across all strategies—without labels, adding a bit of suspense.</p>

<p>While all models deliver positive returns, there is a clear differences in performance.</p>

<p><img src="/assets/ridge/all_lines.png" alt="Figure 3" /></p>

<p><strong>Figure 3</strong>: Cumulative returns of all strategies, scaled to 10% volatility.</p>

<p>Figure 4 shows how different normalization methods, sector adjustments, and target label choices affect Sharpe Ratios across the models. It compares Z-scoring, raw features, and ranking, and also looks at the effect of normalizing within sectors versus globally. Plus, it shows how using Sharpe Ratios or raw returns as the target label changes things.</p>

<p>First off, normalization is pretty clear—Z-scoring works best, then ranking, and raw features consistently underperform. I was honestly expecting ranking to be the top performer because it’s supposed to stabilize things, but it seems like it might also take out some useful signals. Z-scoring holds onto more of that valuable information, which is why it does so well. Raw features just add noise, so they end up being the weakest choice.</p>

<p>Then, sector adjustment comes in and has an even bigger impact. Normalizing within sectors really improves Sharpe Ratios compared to global normalization. It makes sense because comparing stocks within the same sector gives us more relevant context. By normalizing within sectors, I’m making sure that sector-wide noise doesn’t interfere with the real signals, so the rankings are more stable.</p>

<p>Lastly, the target label is by far the most important factor. When I use Sharpe Ratios as the target, the models consistently perform better than when I use raw returns. This is no surprise—it’s easier to predict volatility than raw returns, so Sharpe Ratios give more reliable, risk-adjusted performance.</p>

<p>To sum it up, while normalization and sector adjustments matter, the key takeaway is the target label. Sharpe Ratios beat raw returns every time, and sector normalization makes the rankings a lot stronger.</p>

<p><img src="/assets/ridge/summary_barplot.png" alt="Figure 4" /></p>

<p><strong>Figure 4</strong>: Sharpe Ratio performance across key modeling choices.</p>

<h3 id="normalization-effects-depend-on-the-target-label">Normalization Effects Depend on the Target Label</h3>

<p>Digging a little bit deeper, Figure 5 visualizes the effect of normalization conditioned on the target label.</p>

<p>For Return 20 models, normalization had a smaller effect, but ranking and Z-scoring still outperformed raw features. Interestingly, Z-scoring has regained popularity in recent years.</p>

<p>For Sharpe Ratio models, the impact of normalization was stronger. Z-scoring was clearly the best performer, followed by ranking, then raw features.</p>

<p><img src="/assets/ridge/normalization_target.png" alt="Figure 5" /></p>

<p><strong>Figure 5</strong>: Cumulative return of different normalization methods, conditioned on the target label. Volatility is set at 10% for all strategies.</p>

<h3 id="key-takeaways">Key Takeaways</h3>
<ul>
  <li>Normalization improves signal stability, helping models generalize better.</li>
  <li>Sector-based adjustments on the target label refine comparisons, preventing large sector-specific biases.</li>
  <li>Target label choice affects robustness, with Sharpe Ratio-based models performing better.</li>
</ul>

<h2 id="the-combo-strategy-holds-up-well">The “Combo” Strategy Holds Up Well</h2>

<p>I was surprised to see that the “Combo” strategy performed really well, coming in second for Sharpe ratio (you can see it in Table 1). Instead of picking just one top model, it weights all strategies equally—and still ended up second overall.</p>

<p>Even without fine-tuning, blending multiple models helped smooth the performance and made it more stable. It’s pretty clear to me that diversifying across models can outperform just sticking with a single “best” model.</p>

<h2 id="full-performance-breakdown">Full Performance Breakdown</h2>

<p>To quantify these findings, here’s the performance breakdown across models:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Return (ann.)</th>
      <th>Volatility (ann.)</th>
      <th>Sharpe Ratio</th>
      <th>Max Drawdown</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SR Z-Score By Sector</td>
      <td>12.24%</td>
      <td>7.94%</td>
      <td>1.54</td>
      <td>16.09%</td>
    </tr>
    <tr>
      <td>Combo</td>
      <td>8.99%</td>
      <td>6.62%</td>
      <td>1.36</td>
      <td>15.19%</td>
    </tr>
    <tr>
      <td>SR Z-Score Global</td>
      <td>10.68%</td>
      <td>8.30%</td>
      <td>1.29</td>
      <td>18.39%</td>
    </tr>
    <tr>
      <td>SR Ranking By Sector</td>
      <td>9.95%</td>
      <td>7.83%</td>
      <td>1.27</td>
      <td>13.94%</td>
    </tr>
    <tr>
      <td>SR Ranking Global</td>
      <td>10.40%</td>
      <td>8.41%</td>
      <td>1.24</td>
      <td>15.93%</td>
    </tr>
    <tr>
      <td>SR Raw Global</td>
      <td>9.76%</td>
      <td>8.39%</td>
      <td>1.16</td>
      <td>15.89%</td>
    </tr>
    <tr>
      <td>Return Ranking By Sector</td>
      <td>8.14%</td>
      <td>7.50%</td>
      <td>1.09</td>
      <td>15.96%</td>
    </tr>
    <tr>
      <td>Return Z-Score By Sector</td>
      <td>8.07%</td>
      <td>7.43%</td>
      <td>1.09</td>
      <td>19.37%</td>
    </tr>
    <tr>
      <td>Return Raw Global</td>
      <td>6.95%</td>
      <td>7.54%</td>
      <td>0.92</td>
      <td>18.99%</td>
    </tr>
    <tr>
      <td>Return Ranking Global</td>
      <td>6.48%</td>
      <td>7.26%</td>
      <td>0.89</td>
      <td>18.86%</td>
    </tr>
    <tr>
      <td>Return Z-Score Global</td>
      <td>6.44%</td>
      <td>7.67%</td>
      <td>0.84</td>
      <td>25.95%</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1</strong>: Performance metrics across different modeling choices, ranked by Sharpe Ratio in descending order. Results exclude transaction costs and slippage.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>Sector normalization (target label), normalization method, and target label choice all had a meaningful impact on performance:</p>
<ul>
  <li>Sector normalization was a game-changer—comparing stocks within their sector led to major improvements.</li>
  <li>Normalization method mattered more than expected—Z-scoring outperformed ranking, contradicting my initial intuition.</li>
  <li>Sharpe Ratio models consistently outperformed return-based models, reinforcing the importance of risk-adjusted metrics.</li>
</ul>

<p>Instead of searching for a single best model, it may be smarter to combine perspectives. The “Combo” strategy showed that diversification across models stabilizes results—even without fine-tuning.</p>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quants" /><summary type="html"><![CDATA[TODO: update with new data reread + spelling and style.]]></summary></entry><entry><title type="html">The Low Volatility Factor: A Steady Approach</title><link href="http://localhost:4000/quant/2024/12/15/low-volatility-factor.html" rel="alternate" type="text/html" title="The Low Volatility Factor: A Steady Approach" /><published>2024-12-15T00:00:00+01:00</published><updated>2024-12-15T00:00:00+01:00</updated><id>http://localhost:4000/quant/2024/12/15/low-volatility-factor</id><content type="html" xml:base="http://localhost:4000/quant/2024/12/15/low-volatility-factor.html"><![CDATA[<p>The low-volatility factor is based on a simple idea: stocks that move less tend to deliver better risk-adjusted returns than those with more extreme price swings. It’s a pattern that has been observed not only in equities but also in other asset classes.</p>

<p>This post is the first in a series on cross-sectional stock selection. I’ll begin with a single-factor strategy, then gradually build up: combining multiple signals using linear regression, testing more advanced design choices, and later exploring interactions and non-linearities with models like LightGBM. At the end, I’ll compare all approaches to see whether complexity actually improves performance. But first, let’s keep things simple—and see how far a basic low-volatility sort can take us.</p>

<h2 id="tradeable-universe">Tradeable Universe</h2>

<p>The dataset covers the Russell 1000 (RIY), which tracks the largest U.S. stocks. To keep it realistic, I filter out stocks priced under $5. The sample runs from 1995 to 2024, covering around 3,300 stocks as companies enter and exit the index. At any given time, about 1,000 stocks are tradeable. Since it uses point-in-time constituents, there’s no survivorship bias. Figures 1 visualizes the number of tradeable stocks over time.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/nr_stocks.svg" alt="Figure 1" /><br />
<strong>Figure 1</strong>: Number of tradeable stocks over time.</p>

<h2 id="measuring-volatility">Measuring Volatility</h2>

<p>To identify low-volatility stocks, I compute the standard deviation of daily returns—a standard way to quantify how much a stock’s price fluctuates. Specifically, I use three short-term rolling windows:</p>

<ul>
  <li>5 trading days</li>
  <li>10 trading days</li>
  <li>21 trading days</li>
</ul>

<p>Shorter windows react more quickly to changes in market conditions, while longer windows provide more stable estimates. By combining them, I aim for a volatility signal that’s both responsive and robust.</p>

<p>Volatility is computed as:</p>

\[\hat{\sigma}_{i,t} = \sqrt{\frac{1}{N-1} \sum_{j=1}^N \left( r_{i,t-j} - \bar{r}_{i,t} \right)^2}\]

<p>where $r_{i,t}$ is the daily return of stock $i$ at time $t$, and $N$ is the length of the rolling window. I annualize this by multiplying the daily volatility by $\sqrt{252}$, assuming 252 trading days in a year.</p>

<p>Across the dataset, the average annualized volatility is about 33%, with most stocks falling between 18% and 39%. The median is 26%. A few names have extreme swings, so I winsorize the volatility values at 5% and 200% to prevent outliers from distorting the rankings.</p>

<p>As shown in Figure 2, the distribution is right-skewed: most stocks cluster around moderate volatility levels, but a small number show very high fluctuations.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/distribution_volatilities.svg" alt="Figure 2" /><br />
<strong>Figure 2</strong>: Distribution of annualized volatility across all stocks.</p>

<h2 id="does-low-volatility-matter">Does Low Volatility Matter?</h2>

<p>To check whether low-volatility stocks behave differently, I looked at their returns over the next 10 trading days. I focused on two things: raw returns and risk-adjusted returns, using the Sharpe ratio.</p>

<p>The Pearson correlation between volatility and raw return came out slightly positive, around 0.03. So, more volatile stocks seemed to perform a little better—at least at first glance. That wasn’t exactly what I expected, but it’s a small effect. Switching to Spearman correlation, which is more robust to outliers, the result flattened out to zero.</p>

<p>The picture changes when looking at Sharpe ratios. Here, the correlation with volatility was negative: -0.035 with Pearson, and -0.04 with Spearman. So while higher-vol stocks might deliver the occasional bigger return, they tend to do so with more noise. On a risk-adjusted basis, they come out worse.</p>

<p>The signal is weak, but that’s typical. What matters is that it shows up consistently, especially when applied across a large universe.</p>

<h2 id="sorting-stocks-into-portfolios">Sorting Stocks into Portfolios</h2>

<p>To turn this into a tradeable strategy, I rank stocks by volatility at each point in time and sort them into five portfolios. This ensures that portfolio assignments are always relative to the current market.</p>

<p>Here’s how it works:</p>
<ol>
  <li>Compute rolling volatility for each stock.</li>
  <li>Rank stocks by volatility within the universe.</li>
  <li>Normalize ranks to a 0-1 scale.</li>
  <li>Assign stocks to one of five portfolios based on percentile rank.</li>
</ol>

<p>Let $r_{i,t}$ be the cross-sectional rank of stock $i$ at time $t$, and $N$ be the number of stocks. The normalized rank is:</p>

\[\frac{r_{i,t}}{N}\]

<p>Stocks are then grouped into these buckets:</p>

<ul>
  <li>Portfolio 1: Lowest 10% of stocks ($0 \leq \text{Rank Score} &lt; 0.1$) → Low volatility</li>
  <li>Portfolio 2: 10% to 20% of stocks ($0.1 \leq \text{Rank Score} &lt; 0.2$)</li>
  <li>Portfolio 3: 20% to 80% ($0.2 \leq \text{Rank Score} &lt; 0.8$)</li>
  <li>Portfolio 4: 80% to 90% of stocks ($0.8 \leq \text{Rank Score} &lt; 0.9$)</li>
  <li>Portfolio 5: Highest 10% ($0.9 \leq \text{Rank Score} \leq 1.0$) → High volatility</li>
</ul>

<p>This way, every stock’s classification is determined relative to the cross-sectional volatility of the market at that time.</p>

<h2 id="portfolio-construction">Portfolio Construction</h2>

<p>Once the stocks are grouped into buckets, I construct two types of portfolios: one that assigns equal weights to each stock, and another that adjusts weights to target a specific volatility level.</p>

<h3 id="1-equal-weighted-portfolio">1. Equal-Weighted Portfolio</h3>

<p>In the equal-weighted portfolio, each stock receives the same weight:</p>

\[w_{i,t} = \frac{1}{N_t}\]

<p>where \(N_t\) is the number of stocks in the portfolio at time \(t\). The portfolio remains fully invested. To create the long-short strategy, I go long the low-volatility portfolio (P1) and short the high-volatility portfolio (P5), using equal weights on both sides.</p>

<p>This setup introduces a problem: the two legs have different levels of volatility. Low-volatility stocks naturally exhibit less risk, so the short leg tends to dominate in terms of exposure. This imbalance reduces the effectiveness of the strategy on a risk-adjusted basis.</p>

<h3 id="2-volatility-targeted-portfolio">2. Volatility-Targeted Portfolio</h3>

<p>To correct for this imbalance, I use volatility targeting. The idea is to scale the weight of each stock based on its volatility relative to a fixed target. The scaling factor is defined as:</p>

\[\alpha_{i,t} = \frac{\sigma_{\text{target}}}{\hat{\sigma}_{i,t}}\]

<p>where:</p>

<ul>
  <li>\(\sigma_{\text{target}} = 20\%\) is a fixed target volatility level</li>
  <li>\(\hat{\sigma}_{i,t}\) is the estimated future volatility of stock \(i\), computed using a 60-day rolling standard deviation</li>
</ul>

<p>Each stock’s weight becomes:</p>

\[w_{i,t} = \frac{1}{N_t} \cdot \alpha_{i,t}\]

<p>This approach increases the weight of stocks with below-target volatility and reduces the weight of those above it. To avoid excessive concentration, I cap individual weights at 4%. The total portfolio weight is also constrained to remain below or equal to 1.0.</p>

<p>During periods of high volatility, the portfolio may become partially invested to stay within the exposure limit. This isn’t market timing—it’s a way to control portfolio-level risk dynamically.</p>

<p>Portfolios are rebalanced weekly based on updated volatility estimates. Transaction costs are not included in this analysis.</p>

<h2 id="results">Results</h2>

<h3 id="equal-weighted-portfolio">Equal-Weighted Portfolio</h3>

<p>I start by evaluating the equal-weighted version of the strategy. The long side holds the lowest-volatility stocks (P1), and the short side holds the highest-volatility ones (P5). Within each group, stocks are given equal weight.</p>

<p>On their own, the results make sense. The low-volatility portfolio delivers a geometric return of 11.8% with a Sharpe ratio of 1.0. The high-volatility portfolio returns 4.7%, with much higher volatility and little risk-adjusted performance.</p>

<p>But when combining the two into a long-short portfolio, the performance breaks down. The issue is a volatility mismatch: the short side is much more volatile than the long side—38.6% versus 12.2%. Equal weighting doesn’t account for this difference. As a result, the short leg ends up driving most of the portfolio’s risk.</p>

<p>The final long-short portfolio has high volatility (32.6%), a negative Sharpe ratio (-0.2), and a drawdown over 90%. Figure 3 shows the key performance metrics before applying volatility targeting. A full summary is included in Table 1.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/barplot_metrics_ew.png" alt="Figure 3" /></p>

<p><strong>Figure 3</strong>: Geometric Return, Volatility, and Sharpe Ratio for equal-weighted portfolios (before volatility targeting).</p>

<h3 id="volatility-targeted-portfolio">Volatility-Targeted Portfolio</h3>

<p>Volatility targeting adjusts for the imbalance between long and short legs by scaling each stock’s weight relative to a fixed volatility target (set here at 20%). This helps align the total risk of the long and short portfolios and results in more stable performance.</p>

<p>After applying volatility targeting, the improvements are clear. The long-short portfolio volatility drops from 32.6% to 8.3%, and the Sharpe ratio rises from -0.2 to 0.9. The max drawdown is also cut significantly—from over 90% to 33.6%.</p>

<p>The long-only portfolio becomes slightly more stable as well. Its volatility decreases from 12.2% to 9.7%, while the Sharpe ratio improves from 1.0 to 1.1.</p>

<p>Figure 4 shows how volatility, return, and Sharpe ratio change across the five portfolios after targeting is applied. The result is more balanced exposure and better overall risk-adjusted returns.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/barplot_metrics_ew_vt.png" alt="Figure 4" /><br />
<strong>Figure 4</strong>: Geometric return, volatility, and Sharpe ratio after volatility targeting.</p>

<h3 id="performance-over-time">Performance Over Time</h3>

<p>The impact of volatility targeting is also visible over time. Figure 5 shows the net asset value of the long-short portfolio after adjustment. Compared to the equal-weighted version, returns are smoother and drawdowns less severe.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/perf_backtest_ew_vt.png" alt="Figure 5" /><br />
<strong>Figure 5</strong>: Net asset value of the volatility-targeted long-short portfolio.</p>

<h3 id="summary-table">Summary Table</h3>

<p>To summarize the improvements, here are the core metrics before and after applying volatility targeting:</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Long (No VT)</th>
      <th>Long (VT)</th>
      <th>Short (No VT)</th>
      <th>Short (VT)</th>
      <th>L/S (No VT)</th>
      <th>L/S (VT)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Geometric Return</td>
      <td>11.8%</td>
      <td>11.1%</td>
      <td>4.7%</td>
      <td>2.9%</td>
      <td>-5.3%</td>
      <td>7.7%</td>
    </tr>
    <tr>
      <td>Volatility</td>
      <td>12.2%</td>
      <td>9.7%</td>
      <td>38.6%</td>
      <td>9.7%</td>
      <td>32.6%</td>
      <td>8.3%</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>1.00</td>
      <td>1.14</td>
      <td>0.12</td>
      <td>0.30</td>
      <td>-0.19</td>
      <td>0.93</td>
    </tr>
    <tr>
      <td>Max Drawdown</td>
      <td>40.2%</td>
      <td>29.5%</td>
      <td>89.6%</td>
      <td>36.7%</td>
      <td>91.6%</td>
      <td>33.6%</td>
    </tr>
    <tr>
      <td>Max Time Underwater</td>
      <td>863 days</td>
      <td>612 days</td>
      <td>4,802 days</td>
      <td>1,647 days</td>
      <td>5,539 days</td>
      <td>944 days</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Performance metrics before and after volatility targeting. “VT” applies volatility targeting; “No VT” uses equal weighting.</p>

<h3 id="portfolio-exposure-over-time">Portfolio Exposure Over Time</h3>

<p>Figure 6 shows how total portfolio weights evolve over time for both the long (low-volatility) and short (high-volatility) sides after applying volatility targeting.</p>

<p>By total weight, I mean the sum of all individual stock weights within each leg. Since the short side holds more volatile stocks, it naturally receives less capital—typically between 0.2 and 0.6. The long side, made up of more stable names, stays closer to fully invested.</p>

<p>This behaviour is expected. Volatility targeting reduces exposure to riskier assets and increases it for more stable ones. During periods of market stress—like the dot-com crash, the financial crisis, or COVID—both legs reduce their exposure. The mechanism reacts by shrinking position sizes to stay within the target risk level.</p>

<p>The result is a portfolio that adapts to changing market conditions without needing explicit market timing.</p>

<p><img src="/assets/2024-12-15-low-volatility-factor/portfolio_weights_long_short_vol_target.svg" alt="Figure 6" /><br />
<strong>Figure 6:</strong> Total weight of all positions in the long (P1) and short (P5) portfolios after volatility targeting.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p>The low-volatility factor delivers stronger risk-adjusted returns. Lower-vol stocks tend to outperform, which goes against the usual link between higher risk and higher return.</p>
  </li>
  <li>
    <p>Equal weighting in a long-short setup creates a risk imbalance. The short leg ends up dominating volatility, which pulls down performance.</p>
  </li>
  <li>
    <p>Volatility targeting fixes that imbalance. It adjusts weights to align the risk on both sides, making the strategy more stable and improving Sharpe ratios.</p>
  </li>
</ol>

<p>Even this simple adjustment already helps a lot. In the next post, I’ll look at how combining multiple signals can take things a step further.</p>]]></content><author><name>piinghel</name><email>pjinghelbrecht@gmail.com</email></author><category term="Quant" /><summary type="html"><![CDATA[The low-volatility factor is based on a simple idea: stocks that move less tend to deliver better risk-adjusted returns than those with more extreme price swings. It’s a pattern that has been observed not only in equities but also in other asset classes.]]></summary></entry></feed>