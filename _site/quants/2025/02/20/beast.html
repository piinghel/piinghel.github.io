<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>LightGBM</title>

  <!-- MathJax Config -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>

  <!-- MathJax Scripts -->
  <script type="text/javascript" async
    src="https://polyfill.io/v3/polyfill.min.js?features=es6">
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>LightGBM | Pieter-Jan</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="LightGBM" />
<meta name="author" content="piinghel" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Roadmap The idea to show performance of LightGBM first using the same approach as the last article and then also compare it to the low vol and linear model and compare performance and show the impressive added value you get from including non-linearities" />
<meta property="og:description" content="Roadmap The idea to show performance of LightGBM first using the same approach as the last article and then also compare it to the low vol and linear model and compare performance and show the impressive added value you get from including non-linearities" />
<link rel="canonical" href="http://localhost:4000/quants/2025/02/20/beast.html" />
<meta property="og:url" content="http://localhost:4000/quants/2025/02/20/beast.html" />
<meta property="og:site_name" content="Pieter-Jan" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-02-20T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="LightGBM" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"piinghel"},"dateModified":"2025-02-20T00:00:00+01:00","datePublished":"2025-02-20T00:00:00+01:00","description":"Roadmap The idea to show performance of LightGBM first using the same approach as the last article and then also compare it to the low vol and linear model and compare performance and show the impressive added value you get from including non-linearities","headline":"LightGBM","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/quants/2025/02/20/beast.html"},"url":"http://localhost:4000/quants/2025/02/20/beast.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Pieter-Jan" />
</head>
</head>

<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Pieter-Jan</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
    <div class="wrapper">
      <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">LightGBM</h1>
    <p class="post-meta"><time class="dt-published" datetime="2025-02-20T00:00:00+01:00" itemprop="datePublished">
        Feb 20, 2025
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="roadmap">Roadmap</h1>
<ul>
  <li>
    <p>The idea to show performance of LightGBM first using the same approach as the last article and then also compare it to the low vol and linear model and compare performance and show the impressive added value you get from including non-linearities</p>
  </li>
  <li>introduce LightGBM very shortly: check</li>
  <li>explain again a bit the process but a very brief recap. Refer to previous articles: check</li>
  <li>The idea is to show the added value of LightGBM compared to ridge compared to the low vol factor: check</li>
  <li>I’m thinking of also introducing the singal performance and metrics: TODO
      - add also low vol factor here
      - add summary statisitcs here as well; mean, min max, std, sr</li>
  <li>Compare performance returns: low vol, linear, lightgbm: check</li>
  <li>Should be a relatively short article</li>
  <li>introduce transaction costs: ok will say</li>
  <li>show turnover: nope</li>
  <li>Quickly tell about the diminsihing performance over the last 10 years</li>
  <li>Mentioned the neural networks approach to model weigths</li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>In my last two articles, I explored different ways to rank stocks and build long-short portfolios.</p>

<p>In the first article, I used a simple one-factor model based on volatility — ranking stocks by their recent volatility and betting on the fact that low-volatility stocks tend to outperform high-volatility ones.</p>

<p>In the second article, I moved to a more flexible approach by combining multiple features — like momentum, volatility, size, and liquidity — using Ridge regression, a linear model that learns how to weigh these features. I also examined some important design choices, like whether to use ranking or z-scoring for feature normalization, and whether to normalize the target label by sector.</p>

<p>Now I want to take this a step further and see what happens when I switch to a non-linear model, LightGBM. The idea is to keep everything else the same — same dataset, same features, same target label, same allocation model — and only change the model. This way, I can isolate the impact of using a non-linear approach that can capture more complex patterns in the data.</p>

<p>In the end, I’ll compare LightGBM with both Ridge and the low-volatility factor, to clearly see how much is gained (or not) when moving from a simple factor to a linear model and then to a non-linear one.</p>

<h2 id="recap-of-the-process">Recap of the process</h2>

<p>Before diving into LightGBM, let me briefly recap the setup I’ve been using, which stays unchanged for this analysis.</p>

<p>I’m working with the Russell 1000 universe, using point-in-time data and filtering out stocks priced below $5 to avoid illiquid names. The feature set is broad, covering about 150 features related to momentum, volatility, liquidity, size, mean reversion, and correlation with the market.</p>

<p>For the target label, I focus on the Sharpe ratio over the next 20 days. I’ve chosen this because it provides a more stable and risk-adjusted measure of performance compared to raw returns, which are often extremely noisy.</p>

<p>All features are normalized using cross-sectional ranking — turning each feature into a percentile rank between 0 and 1 on each day. Ranking has become my default because it makes features comparable across stocks and over time, and it avoids the impact of extreme outliers. I know z-scoring could sometimes be a better choice, and that’s something I want to revisit in the future, but for now I’m sticking with ranking.</p>

<p>Once I have model scores, I use a volatility-targeted allocation model to build long-short portfolios. Specifically, I go long the top 75 and short the bottom 75 stocks, adjusting each position based on volatility to avoid concentrating risk in a few names. This makes the portfolio more stable over time.</p>

<p>Finally, portfolios are rebalanced every three weeks. I find this strikes a good balance between adapting to new information and keeping turnover under control.</p>

<h2 id="why-move-to-lightgbm">Why move to LightGBM</h2>

<p>So far, the Ridge model combined multiple features in a linear way — it learned fixed weights for each feature, and that was it. But markets are rarely linear, and I suspect there are more complex patterns that Ridge simply can’t capture. For example, a feature like volatility might only matter when combined with a momentum signal, or extreme values of a feature might behave differently than moderate ones.</p>

<p>To address this, I’m moving to gradient boosting, a method that builds strong predictive models by combining many small decision trees. Gradient boosting works iteratively — each new tree tries to fix the mistakes of the previous ones — making it well suited to capture non-linear effects and feature interactions.</p>

<p>There are several well-known implementations of gradient boosting, like XGBoost, CatBoost, and LightGBM. I’m using LightGBM because it’s fast and efficient, especially when working with large datasets like mine. In practice, I don’t expect a big difference between these libraries, but LightGBM tends to be much quicker to train and predict, so it’s the obvious choice for this kind of project.</p>

<p>Another reason to choose gradient boosting is that these models are often among the best-performing approaches for tabular datasets — including many winning solutions in Kaggle competitions. So it makes sense to see whether this kind of model can push performance beyond what Ridge and the low-volatility factor can achieve.</p>

<p>With that in mind, I applied LightGBM to the same setup as before: ranking stocks in the Russell 1000, going long the top 75, short the bottom 75, and using volatility-based position sizing. This keeps everything consistent, so we can isolate the impact of the model itself.</p>

<h2>Performance</h2>

<p>Let’s take a look at how LightGBM performs when applied to the same setup as before — and the results are quite strong.</p>

<div style="display: block; text-align: left;">
  <img src="/assets/lightgbm/perf_lgbm.png" width="600" style="display: block; margin: 0;" />
  <p><strong>Figure 1:</strong> Performance of the LightGBM strategy, before and after transaction costs.</p>
</div>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Short</th>
      <th>Long</th>
      <th>L/S (No Fees)</th>
      <th>L/S (Fees)</th>
      <th>Russell 1000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Return (Ann. %)</td>
      <td>1.46</td>
      <td>12.48</td>
      <td>12.33</td>
      <td>10.69</td>
      <td>7.29</td>
    </tr>
    <tr>
      <td>Volatility (Ann. %)</td>
      <td>9.95</td>
      <td>10.54</td>
      <td>6.67</td>
      <td>6.69</td>
      <td>19.58</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.15</td>
      <td>1.18</td>
      <td>1.85</td>
      <td>1.60</td>
      <td>0.37</td>
    </tr>
    <tr>
      <td>Maximum Drawdown (%)</td>
      <td>30.78</td>
      <td>31.10</td>
      <td>13.79</td>
      <td>13.97</td>
      <td>56.88</td>
    </tr>
    <tr>
      <td>Max Time Under Water (days)</td>
      <td>1619</td>
      <td>599</td>
      <td>234</td>
      <td>265</td>
      <td>1666</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Performance statistics of the LightGBM strategy, with and without transaction costs (5 basis points per trade).</p>

<p>After accounting for costs, the strategy posts a 10.7% annualized return with volatility just under 7%, leading to a Sharpe ratio of 1.60. Drawdowns stay around 14%, and the strategy tends to bounce back relatively quickly after periods of underperformance.</p>

<p>What’s nice to see is that both sides of the book are doing their job. The long leg delivers a solid 12.5% return — well above the Russell 1000’s 7.3%. The short leg returns just 1.5%, meaning those bottom-ranked stocks are underperforming the market, as intended. That’s exactly the kind of clean separation you want in a ranking strategy.</p>

<p>That said, it’s worth noting the Russell 1000 isn’t a proper benchmark here — it’s long-only and far more volatile — but it’s still helpful as a rough point of reference since we’re trading within that universe.</p>

<p>Because both legs are volatility-targeted, the difference in returns translates cleanly into long-short performance, without relying on one side to carry all the weight. The result is a fairly smooth and balanced profile</p>

<h2 id="model-performance">Model Performance</h2>

<h3 id="1-statistical-model-performance">1. Statistical Model Performance</h3>

<p>Before looking at portfolio returns, it’s useful to evaluate how well each model ranks stocks based on expected future performance — independently of any portfolio construction.</p>

<p>Each day, the models generate a prediction for every stock $i$ in the universe. For Ridge and LightGBM, these predictions are trained to estimate the Sharpe ratio over the next 20 trading days, which we compute and rank within each sector to account for industry effects.</p>

<p>This gives us, on each day $t$, a full cross-section of predicted scores and a corresponding cross-section of true future Sharpe ratios — both across all stocks.</p>

<p>To evaluate signal quality, we compute the Spearman rank correlation between the predicted ranking and the true ranking:</p>

\[\rho_t = \text{Spearman} \left( \{ \hat{y}_{i,t} \}, \{ y_{i,t} \} \right)\]

<p>where:</p>

<ul>
  <li>$\hat{y}_{i,t}$ is the model-predicted score for stock $i$ at time $t$</li>
  <li>$y_{i,t}$ is the realized Sharpe ratio over the next 20 days for stock $i$, cross-sectionally rank-normalized by sector</li>
</ul>

<p>This correlation is computed across all stocks on each day $t$, giving us a time series ${ \rho_t }$ of daily values.</p>

<p>From this series, we compute:</p>

<ul>
  <li>Mean correlation — average predictive strength</li>
  <li>Standard deviation — signal stability</li>
  <li>Sharpe ratio — consistency over time (mean divided by standard deviation)</li>
</ul>

<p><img src="/assets/lightgbm/signal.png" width="600" /></p>
<p><strong>Figure 2:</strong> Cumulative Spearman correlation between model signals and future Sharpe ratio rankings, computed daily.</p>
<p><br /></p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Low Volatility</th>
      <th>Ridge Regression</th>
      <th>LightGBM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Mean (%)</td>
      <td>4.96</td>
      <td>5.14</td>
      <td>5.43</td>
    </tr>
    <tr>
      <td>Standard Deviation (%)</td>
      <td>14.39</td>
      <td>8.70</td>
      <td>7.03</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.34</td>
      <td>0.59</td>
      <td>0.77</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 2:</strong> Daily signal quality metrics. Models are evaluated based on how well their predicted rankings align with future 20-day Sharpe ratio rankings (normalized by sector), measured using Spearman correlation across all stocks each day.</p>

<p>Figure 2 plots the cumulative Spearman correlation between each model’s daily predictions and realized outcomes — giving us a running view of how consistently each signal aligns with future Sharpe ratio rankings. A steadily rising line indicates stable predictive power over time, while flatter or more erratic curves suggest a noisier, less reliable signal.</p>

<p>The summary statistics in Table 2 make this more concrete. While the average correlation values are all in the 5% range, LightGBM stands out by delivering its signal more consistently — with the lowest day-to-day variability and the highest signal-level Sharpe ratio.</p>

<p>This might sound modest — after all, Spearman correlation ranges from -1 to 1 (or -100% to 100%), and we’re hovering around just 5%. But this is exactly what’s meant by “low signal-to-noise” in finance: even a small but stable signal can lead to meaningful results. And that’s what we’re seeing here — LightGBM offers a modest edge in correlation that translates into significantly better downstream performance.
The signal-level results are encouraging — LightGBM produces a stronger and more stable ranking than both Ridge regression and the low-volatility factor. But how does this translate into actual financial performance?</p>

<h3 id="2-financial-performance">2. Financial Performance</h3>

<p><img src="/assets/lightgbm/perf_return_comp.png" width="600" /></p>
<p><strong>Figure 3:</strong> Performance over the full sample (1997–2024).</p>

<p><img src="/assets/lightgbm/perf_return_comp_last_10yr.png" width="600" /></p>
<p><strong>Figure 4:</strong> Performance over the last decade (2015–2024).</p>
<p><br /></p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Low Vol (Full)</th>
      <th>LR (Full)</th>
      <th>LGBM (Full)</th>
      <th>Low Vol (10Y)</th>
      <th>LR (10Y)</th>
      <th>LGBM (10Y)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Return (Ann. %)</td>
      <td>5.62</td>
      <td>8.76</td>
      <td>11.68</td>
      <td>7.94</td>
      <td>8.91</td>
      <td>9.33</td>
    </tr>
    <tr>
      <td>Volatility (Ann. %)</td>
      <td>8.67</td>
      <td>7.98</td>
      <td>7.09</td>
      <td>9.76</td>
      <td>8.91</td>
      <td>8.17</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.67</td>
      <td>1.09</td>
      <td>1.59</td>
      <td>0.83</td>
      <td>1.00</td>
      <td>1.13</td>
    </tr>
    <tr>
      <td>Max. Drawdown (%)</td>
      <td>35.27</td>
      <td>20.18</td>
      <td>13.71</td>
      <td>12.11</td>
      <td>17.07</td>
      <td>11.69</td>
    </tr>
    <tr>
      <td>Max. Time Under Water (Days)</td>
      <td>844</td>
      <td>862</td>
      <td>297</td>
      <td>350</td>
      <td>307</td>
      <td>297</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 3:</strong> Strategy performance over the full sample (1997–2024) and the last decade (2015–2024).</p>

<p>Figures 3 and 4, along with Table 3, show a clear pattern: LightGBM outperforms both Ridge and Low Vol across nearly every metric. It delivers higher returns, lower volatility, and stronger Sharpe ratios — with significantly shallower drawdowns and faster recovery times. Over the full sample, the Sharpe ratio reaches 1.59 — a meaningful improvement over Ridge (1.09) and Low Vol (0.67).</p>

<p>That said, it’s worth pointing out that performance in the last decade is lower across the board. LightGBM still leads, but the margin has narrowed. Returns are flatter, volatility is slightly higher, and Sharpe ratios have declined compared to the earlier years.</p>

<p>Why is that? Is the market becoming more efficient? Are features becoming less predictive? Is it overfitting to a regime that no longer holds? These are important questions — ones I won’t tackle here, but that I plan to explore in a follow-up post.</p>

<p>For now, the takeaway is simple: LightGBM shows consistent improvements over simpler models, but like all strategies, its performance evolves — and understanding <em>why</em> is just as important as measuring <em>how much</em>.</p>

<h2 id="whats-next">What’s Next</h2>

<p>There’s clearly room to improve. As we’ve seen, performance over the past decade has declined. It’s tempting to jump to conclusions about why — changing market dynamics, feature decay, increased competition — but the truth is, it’s hard to know. What we do know is that results have weakened, and that’s reason enough to dig deeper.</p>

<p>Here are a few directions I’m thinking about:</p>

<ul>
  <li>
    <p><strong>Understanding model exposures</strong><br />
How much of the return is driven by known risk factors or sector tilts? Are we unknowingly taking on systematic exposures that explain most of the performance? Decomposing the P&amp;L and analyzing factor loadings would help shed light here. There are some great books and papers that go deep into this, and I plan to study those more closely.</p>
  </li>
  <li>
    <p><strong>Expanding the feature set</strong><br />
Right now, the model only sees price, market cap, and volume — a relatively narrow view. There’s likely signal in other types of data, especially when the current ones start to fade. Also, we arbitrarily chose to predict the Sharpe ratio over the next 20 days. But why not 10? Or 30? Or 60? Maybe even combine models trained on different horizons — this could help smooth out noise and pick up different return profiles.</p>
  </li>
  <li>
    <p><strong>Rethinking the modeling objective</strong><br />
One thing that’s been bothering me is the two-step setup: first, we model a score or ranking; then we convert that into positions using a separate allocation rule. But what if we skipped the middle step and directly optimized for portfolio weights? There’s some interesting work out there on end-to-end portfolio optimization that might be worth exploring.</p>
  </li>
  <li>
    <p><strong>Learning the SDF directly with transformers</strong><br />
A direction that’s particularly exciting is inspired by <em>Artificial Asset Pricing</em> by Kelly, Mazarei, and Xiu (2023). Instead of predicting returns or Sharpe ratios and then ranking stocks, they use a transformer architecture to learn the stochastic discount factor directly. This approach sidesteps traditional factor modeling entirely, allowing the network to learn pricing kernels from raw panel data. It’s an ambitious setup, but one that might help close the gap between predictive modeling and true portfolio optimization.</p>
  </li>
</ul>

<p>All of this opens the door to a broader research direction — and I’ll dig into these ideas in future posts.</p>

  </div><a class="u-url" href="/quants/2025/02/20/beast.html" hidden></a>
</article>

    </div>
  </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">piinghel</li>
          <li><a class="u-email" href="mailto:pjinghelbrecht@gmail.com">pjinghelbrecht@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>Systematic trading and data science things.
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
