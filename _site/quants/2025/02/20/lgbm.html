<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Stock Ranking with Boosting</title>

  <!-- MathJax Config -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>

  <!-- MathJax Scripts -->
  <script type="text/javascript" async
    src="https://polyfill.io/v3/polyfill.min.js?features=es6">
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Stock Ranking with Boosting | Quant Notes</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Stock Ranking with Boosting" />
<meta name="author" content="anon" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction" />
<meta property="og:description" content="Introduction" />
<link rel="canonical" href="http://localhost:4008/quants/2025/02/20/lgbm.html" />
<meta property="og:url" content="http://localhost:4008/quants/2025/02/20/lgbm.html" />
<meta property="og:site_name" content="Quant Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-02-20T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Stock Ranking with Boosting" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"anon"},"dateModified":"2025-02-20T00:00:00+01:00","datePublished":"2025-02-20T00:00:00+01:00","description":"Introduction","headline":"Stock Ranking with Boosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4008/quants/2025/02/20/lgbm.html"},"url":"http://localhost:4008/quants/2025/02/20/lgbm.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4008/feed.xml" title="Quant Notes" />
</head>
</head>

<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Quant Notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
    <div class="wrapper">
      <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Stock Ranking with Boosting</h1>
    <p class="post-meta"><time class="dt-published" datetime="2025-02-20T00:00:00+01:00" itemprop="datePublished">
        Feb 20, 2025
      </time><span class="reading-time"> • 10 min read</span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="introduction">Introduction</h2>

<p>In my last two articles, I explored different ways to rank stocks and build long-short portfolios.</p>

<p>The first article explored a basic approach: ranking stocks by recent volatility and taking a position based on the idea that low-volatility stocks outperform higher volatility ones.</p>

<p>In the second article, I moved to a more flexible approach by combining multiple features (like momentum, volatility, size, and liquidity) using Ridge regression, a linear model that learns how to weigh these features. I also examined some important design choices, like whether to use ranking or z-scoring for feature normalization, choise of the target label, and whether to normalize the target label by sector.</p>

<p>In this article, I take it a step further by replacing the linear model with a boosted decision tree model (LightGBM). The setup stays exactly the same (same dataset, features, target, and allocation), so I can isolate the effect of using a non-linear model that captures more complex relationships.</p>

<p>I’ll then compare the results to both Ridge and the original low-volatility factor to see how performance evolves as we move from a simple factor to a linear model, and then to a non-linear one.</p>

<h2 id="recap-of-the-process">Recap of the Process</h2>

<p>In the first two articles (<a href="https://piinghel.github.io/quants/2025/01/14/lowvol.html"><em>The Low Volatility Factor: A Steady Approach</em></a> and <a href="https://piinghel.github.io/quants/2025/02/09/ridge.html"><em>Multi-Factor Stock Selection with Ridge Regression</em></a>), I explored two ways to rank stocks: a simple volatility-based factor, and a linear model (Ridge) combining multiple features. The setup stayed consistent throughout: Russell 1000 universe, point-in-time data, a 20-day forward Sharpe ratio as the target, and a risk-aware allocation model.</p>

<p>In this third article, I’m taking the next step by replacing Ridge with a boosted tree model (LightGBM). Everything else stays the same, so I can isolate the effect of switching to a non-linear model that captures interactions and more complex patterns.</p>

<p>I’ll compare results across all three approaches to see how performance evolves as we move from a simple factor to a linear model, and then to a non-linear one.</p>

<p>All features are normalized using cross-sectional ranking, turning each feature into a percentile rank between 0 and 1 on each day. Ranking has become my default because it makes features comparable across stocks and over time, and it avoids the impact of extreme outliers. I know z-scoring could sometimes be a better choice, and that’s something I want to revisit in the future, but for now I’m sticking with ranking.</p>

<p>Once I have model scores, I use a volatility-targeted allocation model to build long-short portfolios. Specifically, I go long the top 75 and short the bottom 75 stocks, adjusting each position based on volatility to avoid concentrating risk in a few names. This helps keep the portfolio more stable over time.</p>

<p>Portfolios are rebalanced every three weeks, which I’ve found strikes a good balance between adapting to new information and keeping turnover manageable. To reduce sensitivity to rebalance timing, I also stagger the portfolio into three tranches, a technique I explained in more detail in <a href="https://piinghel.github.io/quants/2025/05/10/rebalancing-luck.html"><em>Rebalancing Luck</em></a>.</p>

<h2 id="why-move-to-lightgbm">Why Move to LightGBM</h2>

<p>So far, the Ridge model combined multiple features in a linear way: it learned fixed weights for each feature, and that was it. But markets are rarely linear, and I suspect there are more complex patterns that Ridge simply can’t capture. For example, a feature like volatility might only matter when combined with a momentum signal, or extreme values of a feature might behave differently than moderate ones.</p>

<p>To address this, I’m moving to gradient boosting—a method that builds strong predictive models by combining many small decision trees. Gradient boosting works iteratively: each new tree tries to fix the mistakes of the previous ones, making it well suited to capture non-linear effects and feature interactions.</p>

<p>There are several well-known implementations of gradient boosting, like XGBoost, CatBoost, and LightGBM. I’m using LightGBM because it’s fast and efficient, especially when working with large datasets like mine. In practice, I don’t expect a big difference between these libraries, but LightGBM tends to be much quicker to train and predict, so it’s the obvious choice for this kind of project.</p>

<p>Another reason to choose gradient boosting is that these models are often among the best-performing approaches for tabular datasets, including many winning solutions in Kaggle competitions. So it makes sense to see whether this kind of model can push performance beyond what Ridge and the low-volatility factor can achieve.</p>

<p>With that in mind, I applied LightGBM to the same setup as before: ranking stocks in the Russell 1000, going long the top 75, short the bottom 75, and using volatility-based position sizing. This keeps everything consistent, so I can isolate the impact of the model itself.</p>

<h2 id="performance">Performance</h2>

<p>Let’s take a look at how LightGBM performs when applied to the same setup as before. The results are quite strong.</p>

<p><img src="/assets/lightgbm/perf_lgbm.png" alt="Figure 1" />
<strong>Figure 1:</strong> Performance of the LightGBM strategy, before and after transaction costs.</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Short Sleeve</th>
      <th>Long Sleeve</th>
      <th>L/S (No Fees)</th>
      <th>L/S (Fees)</th>
      <th>Russell 1000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Return (Annualized)</td>
      <td>1.46%</td>
      <td>12.48%</td>
      <td>12.33%</td>
      <td>10.69%</td>
      <td>7.29%</td>
    </tr>
    <tr>
      <td>Volatility (Annualized)</td>
      <td>9.95%</td>
      <td>10.54%</td>
      <td>6.67%</td>
      <td>6.69%</td>
      <td>19.58%</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.15</td>
      <td>1.18</td>
      <td>1.85</td>
      <td>1.60</td>
      <td>0.37</td>
    </tr>
    <tr>
      <td>Max Drawdown</td>
      <td>30.78%</td>
      <td>31.10%</td>
      <td>13.79%</td>
      <td>13.97%</td>
      <td>56.88%</td>
    </tr>
    <tr>
      <td>Max Time Underwater</td>
      <td>1,619 days</td>
      <td>599 days</td>
      <td>234 days</td>
      <td>265 days</td>
      <td>1,666 days</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1:</strong> Performance statistics of the LightGBM strategy, with and without transaction costs (5 basis points per trade).</p>

<p>After accounting for transaction costs, the strategy posts a 10.7% annualized return with volatility just under 7%, leading to a Sharpe ratio of 1.60. Drawdowns stay around 14%, and the strategy tends to bounce back relatively quickly after periods of underperformance.</p>

<p>What’s nice to see is that both sides of the book are doing their job. The long leg delivers a solid 12.5% return, well above the Russell 1000’s 7.3%. The short leg returns just 1.5%, meaning those bottom-ranked stocks are underperforming the market, as intended. That’s exactly the kind of clean separation you want in a ranking strategy.</p>

<p>That said, it’s worth noting the Russell 1000 isn’t a proper benchmark here (it’s long-only and far more volatile), but it’s still helpful as a rough point of reference since we’re trading within that universe.</p>

<p>Because both legs are volatility-targeted, the difference in returns translates cleanly into long-short performance, without relying on one side to carry all the weight. The result is a fairly smooth and balanced profile</p>

<h2 id="signal-quality">Signal Quality</h2>

<p>To get a feel for why LightGBM performs well, I like to zoom in on signal quality: how well each model ranks stocks by expected performance <em>before</em> any portfolio construction happens.</p>

<p>Each day, both Ridge and LightGBM assign a score to every stock $i$ in the universe. These scores are trained to predict a performance metric, specifically the Sharpe ratio over the next 20 trading days. All of this is done completely out-of-sample: the model has no access to future data when making predictions.</p>

<p>So for each day, I end up with a cross-section of predicted scores $\hat{y}_{i,t}$, along with realized outcomes $y_{i,t}$ based on forward-looking Sharpe ratios.</p>

<p>The low-volatility signal works differently. It’s just a simple rule: rank stocks based on their trailing volatility. No learning, no optimization, just a static heuristic.</p>

<p>To measure how well the predicted rankings align with actual performance, I compute the Spearman rank correlation between predicted and realized values:</p>

\[\rho_t = \text{Spearman} \left( \{ \hat{y}_{i,t} \}, \{ y_{i,t} \} \right)\]

<p>This gives a daily time series ${ \rho_t }$, from which I compute:</p>
<ul>
  <li>Mean correlation: how strong the signal is on average</li>
  <li>Standard deviation: how much it varies</li>
  <li>Sharpe ratio: how consistent it is over time (mean divided by standard deviation)</li>
</ul>

<p><img src="/assets/lightgbm/signal.png" alt="Figure 2" />
<strong>Figure 2:</strong> Cumulative Spearman correlation between model signals and future Sharpe ratio rankings, computed daily.</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Low Volatility</th>
      <th>Ridge Regression</th>
      <th>LightGBM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Mean</td>
      <td>4.96%</td>
      <td>5.14%</td>
      <td>5.43%</td>
    </tr>
    <tr>
      <td>Standard Deviation</td>
      <td>14.39%</td>
      <td>8.70%</td>
      <td>7.03%</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.34</td>
      <td>0.59</td>
      <td>0.77</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 2:</strong> Daily signal quality metrics based on Spearman correlation between predicted and realized Sharpe ratio rankings.</p>

<p>The results might seem modest (average correlations around 5%), but that’s expected when working with financial data. Predicting returns is noisy by nature. The key is stability. LightGBM delivers a more consistent signal than the other models, with less variability and a higher signal-level Sharpe ratio.</p>

<p>This kind of stability is valuable. Even weak signals, if reliable, can drive meaningful long-term performance when applied systematically.</p>

<h2 id="financial-performance">Financial Performance</h2>

<p><img src="/assets/lightgbm/perf_return_comp.png" alt="Figure 3" />
<strong>Figure 3:</strong> Performance over the full sample (1997–2024).</p>

<p><img src="/assets/lightgbm/perf_return_comp_last_10yr.png" alt="Figure 4" />
<strong>Figure 4:</strong> Performance over the last decade (2015–2024).</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Low Vol (Full)</th>
      <th>LR (Full)</th>
      <th>LGBM (Full)</th>
      <th>Low Vol (10Y)</th>
      <th>LR (10Y)</th>
      <th>LGBM (10Y)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Return (Ann. %)</td>
      <td>5.62%</td>
      <td>8.76%</td>
      <td>11.68%</td>
      <td>7.94%</td>
      <td>8.91%</td>
      <td>9.33%</td>
    </tr>
    <tr>
      <td>Volatility (Ann. %)</td>
      <td>8.67%</td>
      <td>7.98%</td>
      <td>7.09%</td>
      <td>9.76%</td>
      <td>8.91%</td>
      <td>8.17%</td>
    </tr>
    <tr>
      <td>Sharpe Ratio</td>
      <td>0.67</td>
      <td>1.09</td>
      <td>1.59</td>
      <td>0.83</td>
      <td>1.00</td>
      <td>1.13</td>
    </tr>
    <tr>
      <td>Max. Drawdown (%)</td>
      <td>35.27%</td>
      <td>20.18%</td>
      <td>13.71%</td>
      <td>12.11%</td>
      <td>17.07%</td>
      <td>11.69%</td>
    </tr>
    <tr>
      <td>Max. Time Under Water</td>
      <td>844 days</td>
      <td>862 days</td>
      <td>297 days</td>
      <td>350 days</td>
      <td>307 days</td>
      <td>297 days</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 3:</strong> Strategy performance over the full sample (1997–2024) and the last decade (2015–2024).</p>

<p>Figures 3 and 4, along with <span class="reference">Table 3</span>, show a clear pattern: LightGBM outperforms both Ridge and Low Vol across nearly every metric. It delivers higher returns and lower volatility, resulting in the strongest Sharpe ratios. Drawdowns are smaller, and recovery is faster, with time under water cut by more than half compared to the linear model.</p>

<p>That said, the last decade has been tougher. Performance has declined across the board, and while LightGBM still leads, the gap has narrowed. Its Sharpe ratio drops from 1.59 to 1.13, and Ridge isn’t far behind at 1.00. Returns are flatter, and the edge is less pronounced.</p>

<p>What’s driving this? Has the market become more efficient? Are the features decaying? Or is the model overfitting to an older regime? These are open questions I plan to explore in a future post.</p>

<p>For now, the takeaway is straightforward: LightGBM delivers more stable and stronger results than the simpler models. But like any strategy, its performance isn’t static, and understanding the conditions under which it holds up (or breaks down) is just as important.</p>

<h2 id="conclusion">Conclusion</h2>

<p>There’s clearly still a lot to improve. The model works, but performance has softened in the last decade, and it’s not obvious why. It could be changing regimes, feature decay, more competition… or all of the above.</p>

<p>Here’s what I’m exploring:</p>

<ul>
  <li>
    <p><strong>Portfolio construction</strong><br />
I’ve started digging into <em>Advanced Portfolio Management</em> and <em>The Elements of Quantitative Investing</em> by Paleologo, both really solid so far. I want to better understand what’s actually driving P&amp;L. Am I just picking up factor risk? Can I improve how I size positions or manage constraints?</p>
  </li>
  <li>
    <p><strong>Feature and target design</strong><br />
Right now, everything is based on price, volume, and market cap. There’s probably more signal out there, or better ways to use the existing data. Also thinking about prediction horizon: why stick to 20 days? What if I blend different horizons?</p>
  </li>
  <li>
    <p><strong>Rethinking the objective</strong><br />
The current pipeline scores stocks, then allocates separately. But maybe it makes more sense to predict portfolio weights directly. I’m especially intrigued by the recent paper <em>Artificial Intelligence Asset Pricing Models</em> (Kelly et al., 2025), which uses transformers to learn the stochastic discount factor from raw panel data. Super ambitious, but a really interesting direction.</p>
  </li>
</ul>

<p>Lots of cool stuff to explore, and I’ll share more as I go.</p>


  </div>

  <nav class="post-navigation"><div class="post-nav-prev">
        <span class="post-nav-label">Previous</span>
        <a href="/quants/2025/02/09/ridge.html">Multi-Factor Stock Selection with Ridge Regression</a>
      </div><div class="post-nav-next">
        <span class="post-nav-label">Next</span>
        <a href="/quants/2025/04/20/dope-articles.html">Good Reads</a>
      </div></nav><a class="u-url" href="/quants/2025/02/20/lgbm.html" hidden></a>
</article>

    </div>
  </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4008/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">anon</li>
          
        </ul>
      </div>
      <div class="footer-col">
        <p>Systematic trading and data science things.
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
