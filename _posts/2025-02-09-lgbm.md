---
layout: post
title: "LightGBM"
date: 2025-02-20
categories: [Quants]
---

# TODO
 - update figures and tables with more recent data.
 - reread text and style.

## Introduction  

In my last two articles, I explored different ways to rank stocks and build long-short portfolios.  

In the first article, I used a simple one-factor model based on volatility — ranking stocks by their recent volatility and betting on the fact that low-volatility stocks tend to outperform high-volatility ones.  

In the second article, I moved to a more flexible approach by combining multiple features — like momentum, volatility, size, and liquidity — using Ridge regression, a linear model that learns how to weigh these features. I also examined some important design choices, like whether to use ranking or z-scoring for feature normalization, and whether to normalize the target label by sector.  

Now I want to take this a step further and see what happens when I switch to a non-linear model, LightGBM. The idea is to keep everything else the same — same dataset, same features, same target label, same allocation model — and only change the model. This way, I can isolate the impact of using a non-linear approach that can capture more complex patterns in the data.  

In the end, I’ll compare LightGBM with both Ridge and the low-volatility factor, to clearly see how much is gained (or not) when moving from a simple factor to a linear model and then to a non-linear one.  

## Recap of the process  

Before diving into LightGBM, let me briefly recap the setup I’ve been using, which stays unchanged for this analysis.  

I’m working with the Russell 1000 universe, using point-in-time data and filtering out stocks priced below $5 to avoid illiquid names. The feature set is broad, covering about 150 features related to momentum, volatility, liquidity, size, mean reversion, and correlation with the market.  

For the target label, I focus on the Sharpe ratio over the next 20 days. I’ve chosen this because it provides a more stable and risk-adjusted measure of performance compared to raw returns, which are often extremely noisy.  

All features are normalized using cross-sectional ranking — turning each feature into a percentile rank between 0 and 1 on each day. Ranking has become my default because it makes features comparable across stocks and over time, and it avoids the impact of extreme outliers. I know z-scoring could sometimes be a better choice, and that’s something I want to revisit in the future, but for now I’m sticking with ranking.  

Once I have model scores, I use a volatility-targeted allocation model to build long-short portfolios. Specifically, I go long the top 75 and short the bottom 75 stocks, adjusting each position based on volatility to avoid concentrating risk in a few names. This makes the portfolio more stable over time.  

Finally, portfolios are rebalanced every three weeks. I find this strikes a good balance between adapting to new information and keeping turnover under control.  

## Why move to LightGBM  

So far, the Ridge model combined multiple features in a linear way — it learned fixed weights for each feature, and that was it. But markets are rarely linear, and I suspect there are more complex patterns that Ridge simply can't capture. For example, a feature like volatility might only matter when combined with a momentum signal, or extreme values of a feature might behave differently than moderate ones.

To address this, I’m moving to gradient boosting, a method that builds strong predictive models by combining many small decision trees. Gradient boosting works iteratively — each new tree tries to fix the mistakes of the previous ones — making it well suited to capture non-linear effects and feature interactions.

There are several well-known implementations of gradient boosting, like XGBoost, CatBoost, and LightGBM. I’m using LightGBM because it’s fast and efficient, especially when working with large datasets like mine. In practice, I don’t expect a big difference between these libraries, but LightGBM tends to be much quicker to train and predict, so it’s the obvious choice for this kind of project.

Another reason to choose gradient boosting is that these models are often among the best-performing approaches for tabular datasets — including many winning solutions in Kaggle competitions. So it makes sense to see whether this kind of model can push performance beyond what Ridge and the low-volatility factor can achieve.

With that in mind, I applied LightGBM to the same setup as before: ranking stocks in the Russell 1000, going long the top 75, short the bottom 75, and using volatility-based position sizing. This keeps everything consistent, so we can isolate the impact of the model itself.  

<h2>Performance</h2>

<p>Let’s take a look at how LightGBM performs when applied to the same setup as before — and the results are quite strong.</p>

<div style="display: block; text-align: left;">
  <img src="/assets/lightgbm/perf_lgbm.png" width="600" style="display: block; margin: 0;">
  <p><strong>Figure 1:</strong> Performance of the LightGBM strategy, before and after transaction costs.</p>
</div>




| Metric                          | Short Sleeve | Long Sleeve | L/S (No Fees) | L/S (Fees) | Russell 1000 |
|---------------------------------|--------------|-------------|----------------|-------------|----------------|
| Return (Annualized)             | 1.46%        | 12.48%      | 12.33%         | 10.69%      | 7.29%          |
| Volatility (Annualized)         | 9.95%        | 10.54%      | 6.67%          | 6.69%       | 19.58%         |
| Sharpe Ratio                    | 0.15         | 1.18        | 1.85           | 1.60        | 0.37           |
| Max Drawdown                    | 30.78%       | 31.10%      | 13.79%         | 13.97%      | 56.88%         |
| Max Time Underwater (days)      | 1,619 days   | 599 days    | 234 days       | 265 days    | 1,666 days     |

**Table 1:** Performance statistics of the LightGBM strategy, with and without transaction costs (5 basis points per trade).


After accounting for transaction costs, the strategy posts a 10.7% annualized return with volatility just under 7%, leading to a Sharpe ratio of 1.60. Drawdowns stay around 14%, and the strategy tends to bounce back relatively quickly after periods of underperformance.

What’s nice to see is that both sides of the book are doing their job. The long leg delivers a solid 12.5% return — well above the Russell 1000’s 7.3%. The short leg returns just 1.5%, meaning those bottom-ranked stocks are underperforming the market, as intended. That’s exactly the kind of clean separation you want in a ranking strategy.

That said, it’s worth noting the Russell 1000 isn’t a proper benchmark here — it’s long-only and far more volatile — but it’s still helpful as a rough point of reference since we’re trading within that universe.

Because both legs are volatility-targeted, the difference in returns translates cleanly into long-short performance, without relying on one side to carry all the weight. The result is a fairly smooth and balanced profile

## Signal Quality

To understand *why* LightGBM performs well, it's useful to zoom in on signal quality — how well each model ranks stocks by expected performance before any portfolio construction takes place.

Each day, the Ridge and LightGBM models output a score for every stock $i$ in the universe. These scores are trained to predict the Sharpe ratio over the next 20 trading days, computed out-of-sample and normalized by sector to remove broad industry effects. This gives us a daily cross-section of predicted scores $$\hat{y}_{i,t}$$ and realized outcomes $y_{i,t}$.

The low-volatility signal, in contrast, is a simple heuristic — ranking stocks based on their trailing volatility — and does not involve model training.

To measure alignment between the model’s predictions and actual future performance, we compute the Spearman rank correlation between predicted and realized rankings across all stocks:

$$
\rho_t = \text{Spearman} \left( \{ \hat{y}_{i,t} \}, \{ y_{i,t} \} \right)
$$

This yields a daily time series $\{\rho_t\}$, from which we compute:
- Mean correlation — average predictive strength  
- Standard deviation — signal stability  
- Sharpe ratio — consistency over time (mean divided by standard deviation)

<img src="/assets/lightgbm/signal.png" width="600">  
<p>Figure 2: Cumulative Spearman correlation between model signals and future Sharpe ratio rankings, computed daily.</p>  
<br>

| Metric                   | Low Volatility | Ridge Regression | LightGBM |
|--------------------------|----------------|------------------|----------|
| Mean                     | 4.96%          | 5.14%            | 5.43%    |
| Standard Deviation       | 14.39%         | 8.70%            | 7.03%    |
| Sharpe Ratio             | 0.34           | 0.59             | 0.77     |

**Table 2: Daily signal quality metrics based on Spearman correlation between predicted and realized Sharpe ratio rankings.**

The results might seem modest — average correlations around 5% — but that’s expected when working with financial data. Predicting returns is noisy by nature. The key is stability. LightGBM delivers a more consistent signal than the other models, with less variability and a higher signal-level Sharpe ratio.

This kind of stability is valuable. Even weak signals, if reliable, can drive meaningful long-term performance when applied systematically.


### 2. Financial Performance

<img src="/assets/lightgbm/perf_return_comp.png" width="600">
<p><strong>Figure 3:</strong> Performance over the full sample (1997–2024).</p>

<img src="/assets/lightgbm/perf_return_comp_last_10yr.png" width="600">
<p><strong>Figure 4:</strong> Performance over the last decade (2015–2024).</p>
<br>

| Metric                       | Low Vol (Full) | LR (Full) | LGBM (Full) | Low Vol (10Y) | LR (10Y)  | LGBM (10Y)  |
|------------------------------|----------------|-----------|-------------|----------------|----------|-------------|
| Return (Ann. %)              | 5.62%          | 8.76%     | 11.68%      | 7.94%          | 8.91%    | 9.33%       |
| Volatility (Ann. %)          | 8.67%          | 7.98%     | 7.09%       | 9.76%          | 8.91%    | 8.17%       |
| Sharpe Ratio                 | 0.67           | 1.09      | 1.59        | 0.83           | 1.00     | 1.13        |
| Max. Drawdown (%)            | 35.27%         | 20.18%    | 13.71%      | 12.11%         | 17.07%   | 11.69%      |
| Max. Time Under Water        | 844 days       | 862 days  | 297 days    | 350 days       | 307 days | 297 days    |

**Table 3: Strategy performance over the full sample (1997–2024) and the last decade (2015–2024).**

Figures 3 and 4, along with Table 3, show a clear pattern: LightGBM outperforms both Ridge and Low Vol across nearly every metric. It delivers higher returns and lower volatility, resulting in the strongest Sharpe ratios. Drawdowns are smaller, and recovery is faster — with time under water cut by more than half compared to the linear model.

That said, the last decade has been tougher. Performance has declined across the board, and while LightGBM still leads, the gap has narrowed. Its Sharpe ratio drops from 1.59 to 1.13, and Ridge isn’t far behind at 1.00. Returns are flatter, and the edge is less pronounced.

What’s driving this? Has the market become more efficient? Are the features decaying? Or is the model overfitting to an older regime? These are open questions I plan to explore in a future post.

For now, the takeaway is simple: LightGBM provides a consistent improvement over simpler models. But like any strategy, its performance evolves — and understanding *why* is just as important as measuring *how much*.


## What's Next

There’s clearly still a lot to improve. The model works, but performance has softened in the last decade — and it's not obvious why. It could be changing regimes, feature decay, more competition… or all of the above.

Here’s what I’m exploring:

- **Portfolio construction**  
  I’ve started digging into *Advanced Portfolio Management* and *The Elements of Quantitative Investing* by Paleologo — both really solid so far. I want to better understand what’s actually driving P&L. Am I just picking up factor risk? Can I improve how I size positions or manage constraints?

- **Feature and target design**  
  Right now, everything is based on price, volume, and market cap. There’s probably more signal out there — or better ways to use the existing data. Also thinking about prediction horizon: why stick to 20 days? What if I blend different horizons?

- **Rethinking the objective**  
  The current pipeline scores stocks, then allocates separately. But maybe it makes more sense to predict portfolio weights directly. I’m especially intrigued by the recent paper *Artificial Intelligence Asset Pricing Models* (Kelly et al., 2025), which uses transformers to learn the stochastic discount factor from raw panel data. Super ambitious, but a really interesting direction.

Lots of cool stuff to explore — and I’ll share more as I go.



